<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>機器之心- 知乎專欄</title><link>https://zhuanlan.zhihu.com/jiqizhixin</link><description>提供專業的前沿科技信息</description><lastBuildDate>Thu, 03 Dec 2020 11:46:51 GMT</lastBuildDate><generator>morerssplz 0.4</generator><docs>http://blogs.law.harvard.edu/tech/ rss</docs><item><title>告別渣畫質，視頻會議帶寬降90%，英偉達公開Maxine服務背後重要技術</title><link>https://zhuanlan.zhihu.com/p/327853169 </link><description><p><img src="https://pic1.zhimg.com/v2-8728c13c0d8dc693cb23a469439da6db_b.jpg"></p><div><blockquote>10 月初，英偉達推出了一項AI 視頻會議服務Maxine，使用了AI 來提升分辨率、降低背景噪聲、壓縮視頻、對齊人臉以及執行實時翻譯和轉錄。最近，英偉達團隊發布的新論文揭露了這背後的技術。</blockquote><p& gt;機器之心報導，作者：魔王、蛋醬。</p><p>如果讓打工人用幾個關鍵詞總結2020 年的生活，「視頻會議」應該是其中一個。</p><p>受疫情影響，這一年來，遠程辦公和視頻會議正在成為新的潮流。在忍受會議枯燥的同時，很多人迷上了AI 換臉，期望能夠實現「一邊開會，一邊摸魚」的夢想。此前機器之心也介紹過<a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650785496&amp;idx=3&amp;sn= b6a4c9478e6eb751b976d768d9ae30dd&amp;chksm=871a02a6b06d8bb0dc3f008f3ded2c9698980dcd5a80ee5e14294ddeeeaa7a24541f0e07ebf6&amp;scene=21#wechat_redirect" target="_blank">Avatarify</a>這樣的熱門項目。</p><p>只是…… 效果不一定很理想：</p><figure data-size="normal"><img src="https://pic4.zhimg.com /v2-48bbdd84be80ab886e21357e40b004cf_r.jpg" data-caption="" data-size="normal" data-thumbnail="https://pic4.zhimg.com/v2-48bbdd84be80ab886e21357e40b004cf_b.jpg" width="600" referrerpolicy="no-referrer"></figure><p>給出一個人的源圖像，和一個人的動作視頻（此處稱為驅動視頻(driving video)，動作視頻和源圖像中的人物可以一致或不一致），如何合成逼真的說話者頭部視頻，即將源圖像中的頭像與驅動視頻中的動作合二為一。源圖像編碼目標人物的外觀，驅動視頻決定輸出視頻中的人物動作。</p><p>最近，針對這一任務，<b>英偉達提出了一種純神經式的渲染方法，即不使用人物頭部的3D 圖模型，只使用在one-shot 設置下訓練而成的深度網絡，進行說話者頭部視頻的渲染。</b></p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-9e70f321d9337ac49ebfa129e2ef1233_r.jpg" data-caption="" data-size="normal" width="864" referrerpolicy="no-referrer"></figure><p>論文鏈接：<a href="https://arxiv. 新型3D 關鍵點表徵</b>來表示視頻，3D 關鍵點表徵的特點是將人物特定信息和動作相關信息分解開來，關鍵點及其分解均使用無監督學習方式得到。使用該分解，英偉達能夠對人物特定表徵應用3D 變換，來模擬頭部姿勢的變化，如轉動頭部。下圖2 展示了英偉達提出的新方法：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-b947b768869107037d12201bb4944bc2_r.jpg" data-caption="" data-size="normal" width="988" referrerpolicy="no-referrer"></figure><p>研究者在多個說話者頭部合成任務中進行了大量實驗驗證，包括視頻重建、動作遷移和人臉重定向（face redirection），還將該方法應用於降低視頻會議的帶寬。<b>通過僅發送關鍵點表徵、在接收端重建源視頻，該方法將視頻會議帶寬降至H.264 商用標準所需帶寬的十分之一，且不影響視覺質量。</b></p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-44d6e798751262a3de97fb95a24bc6f0_r.jpg" data-size="normal " data-thumbnail="https://pic1.zhimg.com/v2-44d6e798751262a3de97fb95a24bc6f0_b.jpg" width="600" referrerpolicy="no-referrer"><figcaption>視頻重建效果。</figcaption></figure><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-18cfea2dced482bf635089f5f92316c9_r.jpg" data-size="normal " data-thumbnail="https://pic2.zhimg.com/v2-18cfea2dced482bf635089f5f92316c9_b.jpg" width="600" referrerpolicy="no-referrer"><figcaption>動作遷移。</figcaption></figure><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-f22bb56694e1b0efc311aaca6860d458_r.jpg" data-size="normal " data-thumbnail="https: gt;在沒有3D 圖模型的情況下，實現了對輸出視頻的局部自由視角控制，即在合成過程中允許改變說話者頭部的角度；</li><li>將視頻會議的帶寬，降至H.264 視頻壓縮標準所需帶寬的十分之一。</li></ul><p><b>英偉達新方法</b></p><p>英偉達提出一種純神經合成方法，不使用3D 圖模型。該方法包含三個主要步驟：</p><ul><li>源圖像特徵提取；</li><li>驅動視頻特徵提取；</li><li>視頻合成。</li></ul><p>研究者使用一組網絡並進行聯合訓練，來完成這些步驟。</p><p>其中前兩個步驟參見下圖3：</p><figure data-size="normal"><img src="https://pic4.zhimg. com/v2-a8392cb817f6e4d079c9480874bda667_r.jpg" data-size="normal" width="982" referrerpolicy="no-referrer"><figcaption> 圖3：源圖像和驅動視頻特徵提取。</figcaption></figure><p>具體而言，該研究從源圖像中提取人物外觀特徵和3D 典型關鍵點及其雅克比行列式，同時還估計人物頭部姿勢和表情變化引起的關鍵點擾動，利用它們來計算源關鍵點。</p><p>對於驅動視頻，研究者仍舊估計其頭部姿勢和表情形變。通過重用來自源圖像的3D 典型關鍵點，來計算驅動關鍵點。</p><p>第三個步驟參見圖5：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/ v2-358f616ab1cea93d3e43947966c4fe72_r.jpg" data-size="normal" width="1080" referrerpolicy="no-referrer"><figcaption>圖5：視頻合成。</figcaption></figure><p> 該步驟中，研究人員使用源關鍵點、驅動關鍵點及其雅克比行列式來估計K 個flow（w_1、w_2、w_k），這些flow 用於扭曲源特徵f_s。然後將這些結果結合起來輸入到運動場（motion field）估計網絡M，得到流分解掩碼m。將m 和w_k flow 進行線性組合得到合成流場w（composited flow field），可用於扭曲3D 源特徵。最後，生成器G 將扭曲後的特徵轉換為輸出圖像y。</p><p>而該方法還包括一個主要環節：用無監督方式學習一組3D 關鍵點及其分解。研究人員將這些關鍵點分解成兩部分：一部分建模人臉表情，一部分建模人物的幾何特徵。二者與目標人物頭部姿勢相結合，就可以生成圖像特定的關鍵點，然後利用它們學習兩個圖像之間的映射函數。</p><p>在第一個步驟中，從源圖​​像得到的關鍵點是圖像特定的，且包含人物特徵、姿勢和表情信息。關鍵點計算流程參見下圖4：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-b9942846f254cdeb3f1d9264bfcb4b9a_r.jpg" data- caption="" data-size="normal" width="1080" referrerpolicy="no-referrer"></figure><p><b>訓練細節</b></p> < p>下圖展示了該模型中網絡的實現細節，以及模型構造塊詳情：</p><figure data-size="normal"><img src="https://pic4.zhimg .com/v2-fb0b0f887758786d8a43cceefe300147_r.jpg" data-size="normal" width="1080" referrerpolicy="no-referrer"><figcaption> 圖12：模型中各個組件的具體架構。</figcaption></figure><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-e32769731915e4f75fc6128f7b113451_r.jpg" data-size="normal " width="1038" referrerpolicy="no-referrer"><figcaption>圖13：模型構造塊。</figcaption></figure><p><b>實驗</b></p><p><b>說話者頭部圖像合成</b>& lt;/p><p>這部分涉及兩個任務：相同人物的圖像合成和不同人物的動作遷移。</p><p>首先是源圖像和驅動圖像中人物身份一致的情況。研究者對比了五種人臉合成方法，量化評估結果參見下表1。可以看出，該研究提出的方法在兩個數據集的所有指標上的表現均優於其他方法。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-1154a53d5dcdf5b377323ba388f871f5_r.jpg" data-caption="" data-size=" normal" width="1080" referrerpolicy="no-referrer"></figure><p>在圖6 和圖7 中，研究者分別展示了不同方法的定性比較結果，該研究提出的方法能夠更加真實地再現動作變化。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-072a9c9e1cbdcdc780aaf0dc3860f7c4_r.jpg" data-caption="" data-size=" normal" width="1080" referrerpolicy="no-referrer">< /figure><p>接下來，研究者在源圖像和驅動圖像中人物不同的情況下，進行方法對比，結果如表2 所示。該研究提出的方法取得了最低的FID 分數。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-b18d0ca2f9d15b04398af0bc95ddb70b_r.jpg" data-caption="" data-size=" normal" width="1004" referrerpolicy="no-referrer"></figure><p> 圖8 展示了不同方法間的對比結果，可以看出英偉達方法生成的結果更為真實，且保留了原有的人物特徵。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-c3f37c791266516a57c1c03fad0fef95_r.jpg" data-caption="" data-size=" normal" width="1040" referrerpolicy="no-referrer"></figure><p><b>人臉重定向</b></p> <p>研究人員對pixel2style2pixel (pSp)、Rotate-and-Render (RaR) 和該研究提出方法進行了量化對比，結果參見下表3：</p><figure data-size="normal "><img src="https://pic1.zhimg.com/v2-1f9696e202db91207a019081e601109c_r.jpg" data-caption="" data-size="normal" width="986" referrerpolicy="no-referrer" ></figure><p>三種方法的示例對比結果如圖9 所示。</p><p>可以看出，pSp 模型雖然能夠將人臉前置，但會丟失人物的身份特徵。RaR 採用了3D 人臉模型，因此生成結果的視覺效果更具吸引力，但在人臉區域以外的地方存在問題。此外，這兩種方法都存在時間穩定性問題。對比之下，該研究提出方法實現了不錯的人臉前置效果。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-ddc813fd3ced31e40df3c992c90ca767_r.jpg" data-caption="" data-size=" normal" width="996" referrerpolicy="no-referrer"></figure><p><b>在視頻會議中的應用</b></p><p>該模型能夠利用緊湊表徵對驅動圖像中的動作進行蒸餾，這有助於降低視頻會議應用的帶寬。視頻會議流程可以看做接收者看到發送者面部的動態版本。</p><p>圖10 展示了使用該研究提出的神經說話者頭部模型搭建的視頻會議系統。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-4f754ad8abf2458db3a0a8a5e6905fa2_r.jpg" data-size="normal" width="1030 " referrerpolicy="no-referrer"><figcaption>圖10：視頻壓縮框架。</figcaption></figure><p>在發送端，驅動圖像編碼器提出關鍵點擾動δ_d,k 和頭部姿勢R_d 和t_d，然後使用熵編碼器進行壓縮並傳送至接收端。接收端對信息進行解壓縮，並將其與源圖像s 結合生成輸入d 的重建結果y。</p><p> 論文作者表示，目前該方法在壓縮方面的優勢僅限於說話者頭部視頻，至於一般的視頻壓縮，還未能達到如此理想的效果。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-0720021eeaee43fbf9c0c7c8b364a449_r.jpg" data-caption="" data-size=" normal" width="730" referrerpolicy="no-referrer"></figure><p>目前，英偉達已經開放了在線演示網址：<a href="http://nvidia-research-mingyuliu .com/face_redirection" target="_blank"><span class="invisible">http://</span><span class="visible">nvidia-research-mingyuliu.com</ span><span class="invisible">/face_redirection</span><span class="ellipsis"></span></a>& lt;/p><p></p><p></p></div></description><author>機器之心</author><guid>https://zhuanlan .zhihu.com/p/327853169</guid><pubDate>Thu, 03 Dec 2020 02:46:32 GMT</pubDate></item><item><title>模型聽人講幾句就能學得更好？斯坦福提出用語言解釋輔助學習</title><link>https://zhuanlan.zhihu.com/p/324277492</link><description><div><blockquote>語言是人與人之間最自然的溝通方式，能幫助我們傳遞很多重要的信息。斯坦福大學人工智能實驗室（SAIL）近日發表博客，介紹了其兩篇ACL 2020 論文。這兩項研究立足於近段時間BERT 等神經語言模型的突破性成果，指出了一個頗具潛力的新方向：使用語言解釋來輔助學習NLP 乃至計算機視覺領域的任務。</blockquote><p>選自Stanford AI Lab Blog，作者：Jesse Mu、Shikhar Murty，機器之心編譯，編輯：Panda。</p><p> 想像一下：如果你是一位機器學習從業者並想要解決某個分類問題，比如將彩色方塊群分類為1 或0。你通常會這樣做：收集一個包含大量樣本的數據集，標註數據，然後訓練一個分類器。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-1eb7ce4dad6b21a958f687f8d9d0ae90_r.jpg" data-caption="" data-size=" normal" width="1080" referrerpolicy="no-referrer"></figure><p>但人類的學習方式卻並非如此。對於這個世界，人類有一種非常強大且直觀的信息溝通機制：語言！</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-30a23b100593f626871fbc379a15de21_r.jpg" data-caption="" data-size=" normal" width="1000" referrerpolicy="no-referrer"></figure><p>只需一個短語「at least 2 red squares（至少兩個紅方塊）」，我們就能歸納上面的整個數據集，而且效率要高得多。< /p><p>語言是人類學習的一大關鍵媒介：我們使用語言來傳遞關於這個世界的信念、教育他人以及描述難以直接體驗的事物。因此，對監督式機器學習模型而言，語言理應是一種簡單且有效的方法。但是，過去基於語言的學習方法都難以擴展到現代深度學習系統致力於解決的一般任務，而這些領域使用的語言形式往往很自由。</p><p>今年斯坦福大學AI 實驗室（SAIL）的兩篇ACL 2020 論文在這一研究方向上取得了一些進展：針對自然語言處理（NLP）和計算機視覺領域的多種高難度任務，他們首先用語言解釋這些任務，然後使用深度神經網絡模型來學習這些語言解釋，進而幫助解決這些任務。</p><ul><li>ExpBERT: Representation Engineering with Natural Language Explanations</li><li>Shaping Visual Representations with Language for Few-shot Classification</li></ul>< p><b>難在哪裡？</b></p><p>對人類而言，語言是一種教授他人的直觀媒介，但為何使用語言來執行機器學習會這麼難？</p><p> 主要的難題也是最基本的問題：在其它輸入的語境中理解語言解釋。光是構建能夠理解豐富和模糊語言的模型就已經很難了，而構建能將語言與周圍世界關聯起來的模型還要更難。舉個例子，給定解釋「at least 2 red squares（至少兩個紅方塊）」，模型不僅要理解什麼是「red（紅）」和「squares（方塊）」，還要理解它們如何指代了輸入的特定部分（通常很複雜）。</p><p>過去一些研究依靠語義解析器來將自然語言陳述（比如at least 2 red squares）轉換為形式化的邏輯表徵（比如Count(Square AND Red) &gt; 2) ）。如果我們可以輕鬆地通過執行這些邏輯公式來檢查解釋是否適用於輸入，則可以將解釋用作特徵來訓練模型。但是，語義解析器僅對簡單的領域有效，因為簡單我們才能人工設計可能見到語言解釋的邏輯語法。它們難以處理更豐富和更模糊的語言，也難以擴展用於更複雜的輸入，比如圖像。</p><p>幸運的是，BERT 等現代深度神經語言模型已經顯現出解決多項語言理解任務的潛力。因此，SAIL 在這兩篇論文中提出使用神經語言模型來緩解這些基本問題。這些神經語言模型或以確定相關領域內語言解釋為目標，或使用了可以解讀語言解釋的通用型「知識」來進行預訓練。下面將詳細地介紹這些神經語言模型，看它們如何能在更富挑戰性的任務設置中學習更豐富且更多樣化的語言。</p><p><b>ExpBERT：使用自然語言解釋來設計和創建表徵</b></p><figure data-size="normal"><img src ="https://pic1. 首先來看一個關係提取任務：模型需要根據一小段文本識別其中提到的兩個人是否已經結婚。儘管當前最佳的NLP 模型有可能僅基於數據來解決這一任務，但人類還能通過語言描述來暗示兩人是否已經結婚，比如度蜜月的人通常是已婚的。這樣的語言解釋能用於訓練更好的分類器嗎？</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-2716a78697cd3cbfb9b4fbeb7f156ece_r.jpg" data-caption="" data-size=" normal" width="1080" referrerpolicy="no-referrer"></figure><p>對於語言任務，我們可以提取輸入x 的特徵（比如是否出現了特定詞）來訓練模型，而解釋還能提供額外的特徵。仍以上述任務為例，我們知道「蜜月（honeymoon）」是相關的語言描述，如果我們能創建一個蜜月特徵，並使其在段落描述到兩人將要度蜜月時激活，則這個信號應該可用於訓練更好的模型。</p><p>但創建這樣的特徵需要某種解釋解讀機制（explanation interpretation mechanism），這樣模型才能知道對輸入的解釋是否為真。語義解析器就是這樣一種工具：給定「A 和B 正在度蜜月」，我們可以將這個解釋解析成一種邏輯形式，即當分析一個輸入時，如果在提到A 和B 時還提到了「蜜月」，則返回1。但如果解釋更模糊呢？比如「A 和B 很恩愛」。我們如何解析它？</p>< figure data-size="normal"><img src="https://pic2.zhimg.com/v2-0f5ae1131f57550ecd515821e431722d_r.jpg" data-caption="" data-size="normal" width="1080" referrerpolicy="no-referrer"></figure><p>儘管語義解析在領域較小時高效且準確，但擴展性能很差，因為它只能解讀遵循固定語法規則集和預定義函數（比如contains 和extract_text）的解釋。為了解決這些問題，SAIL 的研究者看中了神經語言模型BERT 的軟推理能力。BERT 在文本蘊涵任務上尤其高效，即確定一個句子是否暗含另一個句子或與另一個句子有矛盾。比如「她吃了披薩」暗含「她吃了食物」。</p><p>SAIL 提出的ExpBERT 模型使用了針對文本蘊涵任務訓練的BERT 模型，但研究者為其設定的訓練目標是識別任務段落裡是否蘊涵一個解釋。BERT 在這一過程中輸出的特徵可替代上述語義解析器提供的指示特徵。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-a95a7444c86d1ea45da8875c3e4cf1f7_r.jpg" data-caption="" data-size=" normal" data-thumbnail="https://pic4.zhimg.com/v2-a95a7444c86d1ea45da8875c3e4cf1f7_b.jpg" width="638" referrerpolicy="no-referrer"></figure><p>BERT 的這種軟推理能力能否提升語義解析效果？在上面的婚姻識別任務中，研究者發現相較於僅使用輸入特徵（無解釋）訓練得到的分類器，ExpBERT 能帶來顯著提升。其中重要的一點是：使用語義解析器來解析解釋的作用不大，因為一般性的解釋（恩愛）難以轉換為邏輯形式。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-dae867d7c280ef429e75fb10f0e3aee4_r.jpg" data-caption="" data-size=" normal" width="777" referrerpolicy="no-referrer"></figure><p>論文還比較了更多基準方法，探索了更大的關係提取任務（如TACRED），執行了控制變量研究，研究了使用解釋相比於添加數據的高效性。此處不再贅述。</p><p><b>使用語言為少次分類任務塑造視覺表徵</b></p>< 上文描述的研究使用自然語言解釋來幫助解決單個任務，比如識別婚姻狀況。但是，認知科學領域的研究表明：語言還能讓我們獲取正確的特徵和抽象概念，進而幫助我們解決未來的任務。例如，能說明A 和B 已婚的語言解釋還能說明其它一些對人類關係而言非常重要的概念：孩子、女兒、蜜月等等。知道這些額外概念不僅有助於識別已婚夫婦，還有助於幫助識別其它關係，比如兄弟姐妹、父母等。</p><p>在機器學習中，我們可能會問：如果我們最終希望解決的新任務沒有提供語言說明，語言如何為高難度且未指明的領域提供恰當的特徵？SAIL 的第二篇論文便探索了這一任務設置，這個任務的難度更大：語言能否提升跨模態（這裡是視覺）的表徵學習？</p><p>具體來說，該研究重點關注的是少次視覺推理任務，比如下面這個來自ShapeWorld 數據集的例子：</p><figure data-size="normal" ><img src="https://pic1.zhimg.com/v2-cba194be22d11708c1b556a0a7de3d7c_r.jpg" data-caption="" data-size="normal" data-thumbnail="https://pic1.zhimg. com/v2-cba194be22d11708c1b556a0a7de3d7c_b.jpg" width="563" referrerpolicy="no-referrer"></figure><p> 針對一個視覺概念給定一個小型訓練樣本集，任務目標是確定留出集的測試圖像是否表達了同樣的概念。現在，如果假設能在訓練時間獲得相關視覺概念的語言解釋，又會如何呢？我們能否使用它們來學習一個更好的模型，即便在測試時沒有語言可用？</p><p>SAIL 的研究者將該任務放到了一個元學習任務框架中：他們沒有在單個任務上訓練和測試模型，而是選擇了在一組任務上訓練模型，其中每個任務都有一個小型訓練集和配套的語言描述（元訓練集/ meta-train set）。然後，他們在一組未見過任務組成的元測試集（meta-test set）上測試模型的泛化能力，並且該測試集沒有可用的語言描述。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-35c62a171a516224985ad0d13af3a01c_r.jpg" data-caption="" data-size=" normal" width="1080" referrerpolicy="no-referrer"></figure><p>首先，如果沒有語言描述，我們會如何解決這一任務？一種典型的方法是原型網絡（Prototype Network），其策略是學習某個能對訓練圖像執行嵌入、求平均並將其與測試圖像的嵌入進行對比的模型f_θ（在這裡是一個深度卷積神經網絡）：</p><figure data-size="normal"><img src="https: //pic1.zhimg.com/v2-cbe3ebe0ff16c7f080ebdfe10f037b28_r.jpg" data-caption="" data-size="normal" data-thumbnail="https://pic1.zhimg.com/v2-cbe3ebe0ff16c7f080ebdfe10f037b28_b.jpg" width= "639" referrerpolicy="no-referrer"></figure><p>在此基礎上，為了使用語言，SAIL 提出一種名為語言塑造型學習（Language Shaped Learning/LSL）的方法：如果能在訓練時使用語言解釋，則可以促使模型學習不僅對分類有用的表徵，而且該表徵還能用於預測語言解釋。SAIL 採用的具體方案是引入一個輔助訓練目標（即與最終的目標任務無關），同時訓練一個循環神經網絡（RNN）解碼器來預測對輸入圖像表徵的語言解釋。有一點至關重要，即這個解碼器的訓練過程取決於圖像模型f_θ 的參數，因此該過程應該能促使f_θ 更好地編碼語言中顯現的特徵和抽象。</p><p>從效果上看，可以說這是訓練模型在訓練期間表徵概念時「把想法大聲說出來」。在測試階段，則可以直接拋棄RNN 解碼器，使用這個「經過語言塑造的」圖像嵌入按常規方式執行分類即可。</p><p>研究者使用真實圖像和人類語言，在上述ShapeWorld 數據集以及更真實的Birds 數據集上進行了測試：</p><figure data-size="normal" > <img src="https://pic2.zhimg.com/v2-ab3dc798b9d9174c1333bdb82d213d6d_r.jpg" data-caption="" data-size="normal" width="1000" referrerpolicy="no-referrer">< ;/figure><p>在這兩種情況下，相對於無語言解釋的基準模型（Meta）、使用隱含語言的學習（L3）方法，這個輔助訓練目標實現了性能提升：</ p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-d22341d788822dc06fc9472d3d49ef8e_r.jpg" data-caption="" data-size="normal" width ="1000" referrerpolicy="no-referrer"></figure><p>此外，該論文還研究了語言的哪些部分最重要（其實差不多都挺重要），以及LSL 需要多少語言才能取得優於無語言模型的表現（其實只需一點點）。詳情請參閱原論文。</p><p><b>展望未來</b></p><p> 正如NLP 系統理解和生成語言的能力在日益增長一樣，機器學習系統基於語言學習解決其它高難度任務的潛力也在增長。SAIL 的這兩篇論文表明，<b>在視覺與NLP 領域的多種不同類型任務上，通過學習語言解釋，深度神經語言模型可成功提升泛化能力。</b></p><p>研究者指出，這是訓練機器學習模型方面一個激動人心的新途徑，而且強化學習等領域已經對一些類似的想法進行了探索。在他們的設想中，未來在解決機器學習任務時，我們無需再收集大量有標註數據集，而是可以通過人與人之間使用了成千上萬年的互動方式——「語言」來與模型進行自然且富有表達力的交互。</p><p>原文鏈接：<a href="https://ai.stanford.edu/blog/learning-from-language/" target="_blank"><span class=" invisible">https://</span><span class="visible">ai.stanford.edu/blog/le</span><span class="invisible">arning-from- language/</span><span class="ellipsis"></span></a></p></div& gt;</description><author>機器之心</author><guid>https://zhuanlan.zhihu.com/p/324277492</guid><pubDate>Tue, 01 Dec 2020 12:44:04 GMT </pubDate></item><item><title>GNN、RL強勢崛起，CNN初現疲態？這是ICLR 2021最全論文主題分析</title><link>https://zhuanlan.zhihu.com/p/323725854</link><description><p><img src="https:// pic3.zhimg.com/v2-2684152974436ab2bd4079bb0c2e9ed8_b.jpg"></p><div><blockquote>ICLR 2021 會議的Rebuttal 環節已經結束，最終接收結果也將在下月正式公佈。日前，有開發者從ICLR 2021 OpenReview 網頁抓取了論文數據，並做了論文高頻詞、論文得分分佈情況等信息的可視化呈現。</blockquote><p>機器之心報導，作者：蛋醬、小舟。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-707f472c5d8d9e68d932cc9fc87b52fe_r. <p>項目地址：<a href="https://github.com/evanzd/ICLR2021-OpenReviewData" target="_blank"><span class="invisible">https://< /span><span class="visible">github.com/evanzd/ICLR2</span><span class="invisible">021-OpenReviewData</span><span class="ellipsis"> ;</span></a></p><p><b>關鍵詞頻率</b></p><p>下圖列舉了提交論文中出現頻率排名前50 的關鍵詞，與往年一樣，深度學習、強化學習、表徵學習、圖神經網絡都是非常熱門的話題。但與<a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650771181&amp;idx=3&amp;sn=168519165c15801aa3acbfa5ddde4d35&amp; chksm=871a4a93b06dc385c83224ab87d8165f16cfd45c18283b62875b7d6125c0da5a4b497f913b12&amp;scene=21#wechat_redirect" target="_blank">去年的統計數據</a>相比，卷積神經網絡（convolutional neural network）的熱度驟降，「元學習」、 「表徵學習」、「圖神經網絡」的熱度均有上升。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-49e4b8386e016c3e82abf54496199b66_r.jpg" data-caption="" data-size=" normal" width="1080" referrerpolicy="no-referrer"></figure><p>由提交論文關鍵詞組成的詞云更加直觀地展示了不同研究方向的熱門程度：</p> ;<figure data-size="normal"><img src="https://pic4.zhimg.com/v2-db0349880ffd9b1cb6f70b75f4f37317_r.jpg" data-caption="" data-size="normal" width= "1080" referrerpolicy="no-referrer"></figure><p><b>論文得分分佈情況</b></p><p>今年ICLR 的論文評審得分集中在5 分左右，平均值為5.169。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-a854303bd1a237c68185d17ea865b3a4_r.jpg" data-caption="" data-size=" normal" width="1080" referrerpolicy="no-referrer"></figure><p><b>關鍵詞vs 得分</b></p><p>項目作者還對論文平均得分和關鍵詞頻率進行了統計。結果顯示，如果論文作者希望盡可能地獲得高評分，或許ta 應該使用「深度生成模型」（deep generative models）、「歸一化流」（normalizing flows）、「神經正切核」（neural tangent kernel） 、「梯度下降法」（gradient descent）等關鍵詞。</p><figure data-size="normal">< img src="https://pic3.zhimg.com/v2-e5f1b6d4e6fac9cdc342bab12890b7ce_r.jpg" data-caption="" data-size="normal" width="1080" referrerpolicy="no-referrer"></ figure><p>有趣的是，在名為「國際學習表徵會議」的ICLR 大會中，「表徵學習」這一關鍵詞的頻率僅位於第三位，前兩名的位置被「深度學習」和「強化學習」牢牢佔據。</p><p>看完統計結果以後，斯坦福大學教授Christopher Manning 表示：「強化學習驚人地崛起了！但卷積神經網絡已現疲態。」</p><figure data -size="normal"><img src="https://pic2.zhimg.com/v2-4d920d344da99cef70ffe07b6d8d11b9_r.jpg" data-caption="" data-size="normal" width="743" referrerpolicy= "no-referrer"></figure><p>觀察近年來的統計結果，「圖神經網絡（GNN）」的熱度正在逐年攀升。DeepMind 高級研究科學家PetarVeličković直言：「不可否認的是，圖神經網絡已經位於深度學習工具箱中的第一階梯。」< 總共有10305 條評審意見，有1797 條評分發生了變化；</li><li>在Rebuttal 環節中，有16 篇論文得到了額外的評審意見。</li></ul><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-8962bed63dd66b9e71d50cd4006962f8_r.jpg" data-size="normal " width="680" referrerpolicy="no-referrer"><figcaption>ICLR 2021 Rebuttal 環節後論文得分變化情況。</figcaption></figure><p>從目前OpenReview 的結果來看，這一屆的ICLR 沒有滿分論文，8 分以上的論文總共有12 篇：</p><figure data -size="normal"><img src="https://pic4.zhimg.com/v2-6ed2d114144f7947a368a5eea48cc573_r.jpg" data-caption="" data-size="normal" width="978" referrerpolicy= "no-referrer">< /figure><p>ICLR 2021 的最終接收結果預計在下月公佈，但你可以在上述評審得分統計文檔中查看自己感興趣的論文。</p><p>論文查看地址：<a href="https://docs.google.com/spreadsheets/d/1fsXrKwKtoghQTPhgnCveRfn9GTsSVvoVRvEOaK9Eqp8/edit#gid=1229227255" target="_blank">< span class="invisible">https://</span><span class="visible">docs.google.com/spreads</span><span class="invisible">heets/ d/1fsXrKwKtoghQTPhgnCveRfn9GTsSVvoVRvEOaK9Eqp8/edit#gid=1229227255</span><span class="ellipsis"></span></a></p></div></description><author>機器之心</author><guid>https://zhuanlan.zhihu.com/p/323725854</guid><pubDate>Tue, 01 Dec 2020 06:25:16 GMT</pubDate></item><item><title>「它將改變一切」，DeepMind AI解決生物學50年來重大挑戰，破解蛋白質分子折疊問題</title>< link>https://zhuanlan.zhihu.com/p/323297132</link><description><p><img src="https://pic4.zhimg.com/v2-f2dbdc100e042552273675fb5ba79327_b.jpg"> </p><div><blockquote>生物學界最大的謎團之一，蛋白質折疊問題被AI 破解了。</blockquote><p>機器之心報導，機器之心編輯部。</p><p>CASP14 組織者、年近七旬的UC Davis 科學家Andriy Kryshtafovych 在大會上感嘆道，I wasn't sure that I would live long enough to see this（我活久見了） [1]。</p><p>11 月30 日，一條重磅消息引發了科技界所有人的關注：谷歌旗下人工智能技術公司DeepMind 提出的深度學習算法「Alphafold」破解了出現五十年之久的蛋白質分子折疊問題。</p><p> 最新一代算法Alphafold 2，現在已經擁有了預測蛋白質3D 折疊形狀的能力，這一複雜的過程對於人們理解生命形成的機制至關重要。</p><p>DeepMind 重大科研突破的消息一出即被《Nature》、《Science》等科學雜誌爭相報導，新成果也立刻獲得了桑達爾· 皮查伊、伊隆· 馬斯克等人的祝賀。</p><p>科學家們表示，Alphafold 的突破性研究成果將幫助科研人員弄清引發某些疾病的機制，並為設計藥物、農作物增產，以及可降解塑料的「超級酶」研發鋪平道路。</p><p>「這是該研究領域激動人心的一刻，」DeepMind 創始人、首席執行官德米斯· 哈薩比斯說道。「這些算法今天已經足夠成熟強大，足以被應用於真正具有挑戰性的科學問題上了。」</p><figure data-size="normal"><img src="https:/ /pic3.zhimg.com/v2-bbe271dbacfb5283994919c9079d815e_r.jpg" data-caption="" data-size="normal" width="1080" referrerpolicy="no-referrer"></figure><p>蛋白質對於生命至關重要，它們是由氨基酸鏈組成的大型複雜分子，其作用取決於自身獨特的3D 結構。弄清蛋白質折疊成何種形狀被稱為「蛋白質折疊問題」。在過去50 年裡，蛋白質折疊一直是生物學領域的重大挑戰。< /p><p>DeepMind 的AlphaFold 讓人類在這一問題上取得了重要突破。在今年的國際蛋白質結構預測競賽CASP 中，DeepMind 開發的AlphaFold 最新版本擊敗了其他選手，在準確性方面比肩人類實驗結果，被認為是蛋白質折疊問題的解決方案。這一突破證明了AI 對於科學發現，尤其是基礎科學研究的影響。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-efada656ae90c05ab5576cbe3a7c3f2f_r.jpg" data-size="normal" width="756 " referrerpolicy="no-referrer"><figcaption>在兩年一次的CASP 競賽中，各組爭先預測蛋白質的3D 結構。今年，AlphaFold 擊敗了所有其他小組，並在準確性方面與實驗結果相匹配。</figcaption></figure><p>對於不熟悉生物領域的人來說，CASP 的大名可能有些陌生——CASP 全稱The Critical Assessment of protein Structure Prediction，旨在對蛋白質結構預測進行評估，被譽為蛋白質結構預測的奧林匹克競賽。CASP 從1994 年開始舉辦，每兩年一屆，目前正在進行的一屆是11 月30 日開始的CASP14。</p><p>而DeepMind 這一突破有什麼影響？< /p><p>用哥倫比亞大學計算生物學家Mohammed AlQuraishi 在Nature 文章中的話來說，「可以說這將對蛋白質結構預測領域造成極大影響。我懷疑許多人會離開該領域，因為核心問題已經解決。這是一流的科學突破，是我一生中最重要的科學成果之一。」</p><p><b>蛋白質折疊問題</b></p> <p>蛋白質的形狀與它的功能密切相關，而預測蛋白質結構對於理解其功能和工作原理至關重要。很多困擾全人類的重大問題（如尋找分解工業廢料的酶）基本上都與蛋白質及其扮演的角色有關。</p><p>多年以來，蛋白質結構一直是熱門的研究話題，研究者使用核磁共振、X 射線、冷凍電鏡等一系列實驗技術來檢測和確定蛋白質結構。但這些方法往往依賴大量試錯和昂貴的設備，每種結構的研究都要花數年時間。</p><p>1972 年，美國科學家克Christian Anfinsen 因「對核糖核酸酶的研究，特別是對其氨基酸序列與生物活性構象之間聯繫的研究」獲得諾貝爾化學獎。在頒獎禮上，他提出了一個著名的假設：從理論上來說，蛋白質的氨基酸序列應該可以完全決定其結構。這一假設引發了長達五十年的探索，即僅僅基於蛋白質的一維氨基酸序列計算出其三維結構。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-bcf6bffc0a752fc967393046b15e75b2_r.jpg" data-caption=" ></span></a><p><b>CASP 14 比賽最新結果：AlphaFold 中位GDT 高達92.4</b></p><p>CASP 競賽由John Moult 和Krzysztof Fidelis 兩位教授於1994 年創立，每兩年進行一次盲審，以促進蛋白質結構預測方面的新SOTA 研究。</p><p>一直以來，CASP 選擇近期才經過實驗確定的蛋白質結構，作為參賽團隊測試其蛋白質結構預測方法的目標（有些結構即使在評估時仍然處於待確定狀態）。這些蛋白質結構不會事先公佈，參賽者也必須對其結構進行盲測，最後將預測結果與實驗數據進行對比。正是基於這種嚴苛的評估原則，CASP 一直被稱為預測技術評估方面的「黃金標準」。</p><p>CASP 衡量預測準確率的主要指標是GDT（Global Distance Test），範圍從0 到100，可以理解為預測的氨基酸殘基在正確位置閾值距離內的百分比。John Moult 教授表示，GDT 分數在90 分左右，即可視為對人類實驗方法具備競爭力。</p><p>在剛剛公佈的第14 屆CASP 評估結果中，DeepMind 的最新AlphaFold 系統在所有預測目標中的中位GDT 達到92.4，意味其平均誤差大概為1.6 埃（Angstrom），相當於一個原子的寬度（或0.1 納米）。即使在難度最高的自由建模類別中，AlphaFold 的中位GDT 也達到了87.0。</p>< figure data-size="normal"><img src="https://pic2.zhimg.com/v2-6cef7f795a81c3a24755d31a8139c2b5_r.jpg" data-size="normal" width="1080" referrerpolicy="no-referrer "><figcaption>歷屆CASP 競賽自由建模類別中預測準確率中位數的提升情況，度量指標為BEST-OF-5 GDT。</figcaption></figure><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-bb7118770db49c06d20a413981329f13_r.jpg" data-size="normal " data-thumbnail="https://pic4.zhimg.com/v2-bb7118770db49c06d20a413981329f13_b.jpg" width="1079" referrerpolicy="no-referrer"><figcaption>CASP 競賽自由建模類別中的兩個目標蛋白質示例。AlphaFold 能夠預測出高度準確的蛋白質結構。</figcaption></figure><p> 這些令人振奮的結果開啟了生物學家使用計算結構預測作為科研主要工具的時代。DeepMind 提出的方法對於某些重要的蛋白質類別尤其有用，例如膜蛋白（membrane protein）。膜蛋白很難結晶，因此很難通過實驗方法來確定其結構。</p><blockquote>該計算工作代表了在蛋白質折疊這一具備50 年曆史的生物學問題上的驚人進展，比該領域人士成功預測蛋白質折疊結構早了幾十年。我們將很興奮，它能從多個方面對生物學研究帶來基礎性改變。——Venki Ramakrishnan 教授（諾貝爾獎得主，英國皇家學會會長）</blockquote><p><b>DeepMind 這樣解決蛋白質折疊問題</b></p><p> 2018 年，DeepMind 團隊<a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650753062&amp;idx=1&amp;sn=5c23afbc08a5e9fad2bb65416813452c& amp;chksm=871a8c58b06d054e5e96a556de6916dc040f3fd7f5377def8c88fbc1136e3a75095fe2730636&amp;scene=21#wechat_redirect" target="_blank">使用初始版AlphaFold 參加CASP13 比賽</a> ，取得了最高的準確率。之後，DeepMind 將CASP13 方法和相關代碼一併發表在Nature 上。而現在，DeepMind 團隊開發出新的深度學習架構，並使用該架構參加CASP14 比賽，達到了空前的準確率水平。這些方法從生物學、物理學、機器學習，以及過去半個世紀眾多科學家在蛋白質折疊領域的工作中汲取靈感。</p><p>我們可以把蛋白質折疊看作一個「空間圖」，節點表示殘基（residue），邊則將殘基緊密連接起來。這個空間圖對於理解蛋白質內部的物理交互及其演化史至關重要。對於在CASP14 比賽中使用的最新版AlphaFold，<b>DeepMind 團隊創建了一個基於注意力的神經網絡系統，並用端到端的方式進行訓練，以理解圖結構，同時基於其構建的隱式圖執行推理。該方法使用進化相關序列、多序列比對（MSA）和氨基酸殘基對的表示來細化該圖</b>。</p><p>通過迭代這一過程，該系統能夠較強地預測蛋白質的底層物理結構，並在幾天內確定高度準確的結構。此外，AlphaFold 還能使用內部置信度度量指標判斷預測的每個蛋白質結構中哪一部分比較可靠。</p><p>DeepMind 團隊在公開數據上訓練這一系統，這些數據來自蛋白質結構數據庫（PDB）和包含未知結構蛋白質序列的大型數據庫，共包括約170,000 個蛋白質結構。該系統使用約128 個TPUv3 內核（相當於100-200 個GPU）運行數週，與現今機器學習領域出現的大型SOTA 模型相比，該系統所用算力相對較少。</p><p> 此外，DeepMind 團隊透露，他們準備在適當的時候將這一AlphaFold 新系統相關論文提交至同行評審期刊。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-fbae0d03d1b9502048a534523df1d36c_r.jpg" data-caption="" data-size=" normal" width="1080" referrerpolicy="no-referrer"></figure><p><i>AlphaFold 主要神經網絡模型架構概覽。該模型基於進化相關的蛋白質序列和氨基酸殘基對運行，迭代地在二者的表示之間傳遞信息，從而生成蛋白質結構。</i></p><p><b>對現實世界的潛在影響</b></p><p>「讓AI 突破幫助人們進一步理解基礎科學問題」 ，經過4 年的研究攻關，現在AlphaFold 正在逐步實現DeepMind 初創時的願景，在藥物設計和環境可持續性等領域都產生了重要的影響。</p><p>馬克斯· 普朗克演化生物學研究所所長，CASP 評估員Andrei Lupas 教授表示：「AlphaFold 的精確模型讓我們解決了近十年來被困擾的蛋白質結構，重新啟動關於信號如何跨細胞膜傳輸的研究。」</p& gt;<p>DeepMind 表示願與其他研究者合作，以進一步了解AlphaFold 在未來幾年的潛力。除了作用於經過同行評審的論文以外，DeepMind 還在探索如何以最佳的可擴展方式為系統提供更廣泛的訪問可能。</p><p>同時，DeepMind 的研究者還研究了蛋白質結構預測如何幫助人們理解一些特殊的疾病。例如，通過幫助識別存在故障的蛋白質，並推斷其相互作用的方式，來理解一些疾病的原理。這些信息能夠讓藥物開發更加精確，從而補充現有的實驗方法，並更快找到更有希望的治療方法。</p><blockquote>AlphaFold 是十分卓越的，它在預測結構蛋白質的速度和精度上有著驚人的表現。這一飛躍證明了計算方法對於生物學中的轉換研究，加速藥物研發過程都具有廣闊的前景。</blockquote><p>同時許多證據也表明，蛋白質結構預測在未來的大流行應對上是有用的。今年早些時候，DeepMind 使用AlphaFold 預測了包括ORF3a 在內的幾種未知新冠病毒蛋白質結構。在CASP14 中，AlphaFold 預測了另一種冠狀病毒蛋白質ORF8 的結構。目前，實驗人員已經證實了ORF3a 和ORF8 的結構。儘管具有挑戰性，並且相關序列很少，但與實驗確定的結構相比，AlphaFold 在兩種預測上都獲得了較高的準確率。</p><p>除了加速對已知疾病的了解，AlphaFold 還具備很多令人興奮的技術潛力：探索數億個目前還沒有模型的數億蛋白質，以及未知生物的廣闊領域。由於DNA 指定了構成蛋白質結構的氨基酸序列，基因組學革命使大規模閱讀自然界的蛋白質序列成為可能——在通用蛋白質數據庫（UniProt）中有1. 8 億個蛋白質序列。相比之下，考慮到從序列到結構所需的實驗工作，蛋白質數據庫（PDB）中只有大約170000 個蛋白質結構。在未確定的蛋白質中可能有一些新的和未確定的功能——就像望遠鏡幫助人類更深入的觀察未知宇宙一樣，像AlphaFold 這樣的技術可以幫助找到未確定的蛋白質結構。</p><p><b>開創新的可能</b></p><p>AlphaFold 是DeepMind 迄今為止取得的最重要進展之一，但隨著後續科學研究的開展，依然有很多問題尚待解決。DeepMind 預測的結構並非全部都是完美的。還有很多要學習的地方，包括多蛋白如何形成複合體，如何與DNA、RNA 或者小分子交互，以及如何確定所有氨基酸側鏈的精確位置。此外，在與他方合作的過程中，還需要學習如何以最好的方式將這些科學發現應用在新藥開發以及環境管理方式等諸多方面。</p><p>對於所有致力於科學領域中計算和機器學習方法的人而言，像AlphaFold 這樣的系統彰顯了AI 作為基礎探索輔助工具的驚人潛力。正如50 年前Anfinsen 提出的遠超當時科研能力所及的挑戰一樣，這個世界依然有諸多未知的方面。</p><p>DeepMind 取得的這一進展令人們更加堅信，AI 將成為人類擴展科學知識邊界的最有用工具之一，同時也期待未來多年的艱苦工作能夠帶來更偉大的發現。</p><p>AlphaFold 科研突破相關視頻請戳：</p><a href="https://v.qq.com/x/page/d3208wl42dz.html" data-draft-node="block" data-draft-type="link-card" data-image="https://pic3.zhimg.com/v2-8ed92c32ccb584c11cd8e53b5beabb66_180x120.jpg" data-image-width="320" data-image-height="180" target="_blank">「Alphafold」破解了出現五十年之久的蛋白質分子折疊問題_騰訊視頻</a><p></p>< ;/div></description><author>機器之心</author><guid>https://zhuanlan.zhihu.com/p/323297132</guid><pubDate>Tue, 01 Dec 2020 02:40: 32 GMT</pubDate></item><item><title>官宣！邢波出任全球首個AI大學校長，MBZUAI明年1月迎來首批新生</title><link>https://zhuanlan.zhihu.com/p/321761749</link><description><p> <img src="https://pic2.zhimg.com/v2-47ac0c5cc99dfe6a5dc9f94c79e1b583_b.jpg"></p>& lt;div><blockquote>剛剛，全球第一所人工智能大學——默罕默德· 本· 扎耶德人工智能大學（MBZUAI）宣布：任命人工智能領域著名學者邢波教授擔任校長一職。</blockquote><p>機器之心報導，機器之心編輯部。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-891a483fb9dc2af5eb01fb26c67fd62e_r.jpg" data-caption="" data-size=" normal" width="1080" referrerpolicy="no-referrer"></figure><p>在人工智能蓬勃發展的今天，越來越多的機構開始投入到人工智能人才培養之中。去年10 月，阿聯酋阿布扎比酋長國宣布成立世界第一所人工智能大學默罕默德· 本· 扎耶德人工智能大學（Mohammad Bin Zayed University of Artificial Intelligence，MBZUAI），這是全球第一所專注於研究生培養的研究型人工智能大學。該校將於2021 年1 月迎來首批學生。</p><p>日前，MBZUAI 官方宣布任命人工智能領域著名學者、教授邢波擔任校長一職。前臨時校長、英國牛津大學腫瘤成像學教授邁克爾· 布雷迪（Michael Brady）卸任，目前仍為MBZUAI 校董會成員。</p>& gt;</p><p>MBZUAI 以阿布扎比王儲、阿拉伯聯合酋長國聯邦武裝部隊副總司令謝赫· 穆罕默德· 本· 扎耶德· 阿爾· 納赫揚（Sheikh Mohamed bin Zayed Al Nahyan ）的名字命名，坐落於阿布扎比著名的環保城馬斯達爾市（Masdar City）。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-18f37da7e9b1ef642aa8d9bff5f511b2_r.jpg" data-caption="" data-size=" normal" width="720" referrerpolicy="no-referrer"></figure><p>阿聯酋是第一批在人工智能領域看到商機的國家之一。根據PWC 中東2018 年關於人工智能的一份報告，到2030 年，AI 將為阿聯酋經濟貢獻96 億美元，佔其國內生產總值的近13.6%。為了發揮AI 技術的潛能，阿聯酋在國家層面採取了一系列促進AI 技術發展的措施。2017 年，阿聯酋任命時年27 歲的奧馬爾· 本· 蘇丹· 奧拉瑪（Omar Bin Sultan Al Olama）為「人工智能國家部長」，這也是世界上首位「AI 部長」。奧拉瑪曾說過一句名言：「數據就是新的石油」。</p><p> MBZUAI 的官網介紹頁面顯示，這所學校將向本地和國際留學生提供三個專業領域（機器學習、計算機視覺和自然語言處理）的碩士（兩年）和博士（四年）課程，並提供全額獎學金及校內住宿、阿聯酋居留簽證等津貼福利。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-e3981724f9629f0ff6f8a27add93667f_r.jpg" data-caption="" data-size=" normal" width="1080" referrerpolicy="no-referrer"></figure><p>作為全球第一所人工智能大學，MBZUAI 有著強大的董事會陣容。在成立之初，校方即宣布創始董事會由阿聯酋國務部長蘇丹艾· 阿爾· 賈比爾出任校園董事會主席，英國牛津大學腫瘤成像學邁克爾· 布雷迪出任臨時校長。此外，董事會成員還包括美國密歇根州立大學特聘教授阿尼爾· 傑恩（Anil K. Jain）、創新工場董事長兼CEO 李開復、MIT Daniela Rus 等全球範圍內的人工智能專家。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-30b98cb0ae092ddaea2fc33a4d6847da_r.jpg" data-caption="" data-size=" normal" width=" p>機器之心報導，編輯：魔王。</p><p>讀博還是不讀博，這是個問題。</p><p>是否讀博、讀博有多難是個經久不衰的話題。最近，一個reddit 熱帖再次點燃了大家的討（tu）論（cao）熱情。</p><p>一位機器學習方向博五學生談論了他的讀博經歷，而主旨竟然是「<b>為什麼你不應該讀博？</b>」。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-80151a9d0bf15163511a6883299105d7_r.jpg" data-caption="" data-size=" normal" width="1080" referrerpolicy="no-referrer"></figure><p><b>為什麼不應該讀博？</b></p><p>這位博士生分享了他在「博士之旅」中的一些觀察，並表示自己的讀博經歷和體驗並非個例。</p><p>以下是他的觀察結果：</p><p>首先，<b>讀博耗時長，機會成本高，而最終的回饋卻並不豐厚< /b>。這有點像是一個騙局。一些朋友還分享過教授不讓學生畢業的「恐怖故事」……</p><p>但這只是發帖人認為不應讀博的表層理由。</p><p>主要原因是<b>讀博傷害創造力和創新性</b> 。博士項目吸納了很多視野廣闊、有創造力和創新性、有抱負、積極進取的學生，略微天真但有夢想。這些學生在開始讀博時擁有獨特的想法和視角，以及解決問題空間的新方法，並且期待自己能產生影響力。</p><p>然而博士項目把這些都毀掉了。在博士項目結束時，學生被變成了機器，用和他人同樣的方式來解決問題。他們被這樣教導：<b>這是SOTA 方法，你只要對這些算法做出哪怕微小的改進就已經很幸運了</b>。</p><p><b>問題在於SOTA 可能只是局部最優解呢</b>。也就是說，這些學生被灌輸的想法是用次優方法解決問題空間。這就難怪他們無法做出有影響力的東西呢，方法本身就處於平台期了。</p><p>那麼如何使機器學習模型跳出局部最優解呢？對探索/ 隨機化給予獎勵。</p><p>發帖人認為我們需要反省教學方式。顯然，為了高效，博士生需要具備一定程度的特定領域專業知識，但<b>這不能以想像力作為代價，更不能是尋求新方法的勇氣</b>。99% 的新方法可能結果不如SOTA 方法，但也許正是一個獨特的、瘋狂的idea 會使領域變得更加開闊。</p><p>當你成為「專家」的時候，你獲得了很多，同時也失去了很多。發帖人表示：「在開始讀博前，我能夠很興奮地發動自己的想像力，思考一些天馬行空的方法來解決問題。其中大部分想法存在致命缺陷，但我對此並不設限。」< ;/p><p> 科研應當是一場富有創造性的瘋狂冒險。而博士項目吸引了有潛力帶來巨大影響力的學生，然後又澆滅了他們的激情和創造性。這就像明星大學生運動員進入了一個執教糟糕的隊伍，最後變得越來越差。</p><p>這篇帖子發出後，引發了大家對「創造性」、「一味追求SOTA」等的激烈討論。今天，reddit 上出現了一個回應帖，其標題是「<b>為什麼應該讀博</b>」。</p><p><b>為什麼應該讀博？</b></p><p>這位發帖人是一位強化學習方向的博士，ta 表示很享受自己的博士生涯，並闡述了從讀博經歷中學到的東西，給出了關於讀博的一些建議。</p><p>ta 認為以下這些事情使得讀博經歷令人滿意：</p><ol><li>與導師建立富有成效的關係。如果你足夠幸運，你的導師可能是世界級專家，還能即時回應你的問題，對你的idea 感興趣並提出有益的改進建議。</li><li>在不要求具體產出的前提下，了解自己感興趣的主題。</li><li> 日常工作能夠匹配你想要建立的技能組合。</li><li> 基於自己的idea 自主創建項目。</li><li> 擁有實驗室專家資源，並鍛煉與其合作、社交、接受反饋的能力。</li>< li> 獲得去工業界實習的機會。</li><li>在頂級會議和期刊上發表工作。</li></ol><p>如果你能從讀博生涯中獲得這些，那這次經歷一定是有趣且值得的。如果你足夠幸運，這還將為你之後的職業生涯奠定基礎。</p><p>那麼如何評估以上7 點呢？發帖人提供了一些建議：</p><ol><li>仔細閱讀潛在導師的最佳出版物和近期有影響力的工作，確認其此前是否指導過優秀的學生。與潛在導師現在或之前的學生聯繫，詢問他們與導師合作時的工作狀態。如果可以的話，你還可以參與實驗室輪轉項目。</li><li>了解實驗室同事是否有很大的論文發表壓力。如果是，那麼你可能很難了解其他領域。你所在的實驗室/ 大學是否歡迎來自不同角度的創造性想法，是否有參加有趣講座、和有才能的人進行交流互動的機會？</li><li>你將成為PhD 所學方向的領域專家。思考這會帶給你什麼技能組合，讀博結束後你又能憑藉它們獲得什麼。同樣地，你還需要思考獲取這些技能的過程，以及你是否享受這一過程。</li><li>導師給你的是涉及狹窄主題的項目還是一幅更廣闊的圖景？（推薦後者，儘管風險性更大。）導師的發表文章主題局限於狹窄的主題還是多個相關領域？導師的工作是否具備較高質量？</li><li>與現在實驗室的成員見面，嘗試了解他們的興趣、專業方向和合作意願。如果他們近期發表過文章，閱讀並與他們進行討論。< /li><li> 博士期間的實習對學習和未來職業生涯很有幫助。機器學習領域能夠提供很好的機會，請盡量利用好這些機會。</li><li>實驗室同事是否經常在頂級會議和期刊上發表文章？他們的工作是否被廣泛引用，或者更具體地，是否對領域研究產生直接影響？</li></ol><p>最後，請記住一點，在現實中，你不太可能有機會滿足所有這些標準，所以你的<b>期望要合理</b> ;，將讀博可能獲得的機會與非博士的機會進行仔細權衡，認真評估所有證據，然後跟著自己的直覺做出是否讀博的選擇。</p><p>此外，這位發帖人還強調：</p><blockquote>沉沒成本謬誤是真實的。在考慮現有項目和未來項目時，如果你在一個想法上下了很大功夫卻沒有成功，不要害怕改變方向。同樣地，如果你盡力了，但事情並沒有解決，也不要怯於更換導師或合作夥伴。在止步不前時要及時發現這一點，並儘己所能（當然是在合理的範圍內）擺脫它。如果事情變得很糟糕，不要害怕輟學。讀博生涯應該充滿興奮和機會，而不是對失敗的恐懼。</blockquote><p>沒有人能隨隨便便讀完博士。去年，Nature 進行的博士生調查揭示了博士學位攻讀中那些艱難的真相：科研壓力、與導師的交流問題、就業壓力等等。然而，依然有很多令人艷羨的「別人的博士生涯」。</p><p>當我們羨慕「別人的博士生涯」時，真正羨慕的是什麼？當我們面臨讀博挫折時，是否要撐下去，能否撐下去？</p>& lt;p>以及最根本的，讀博還是不讀博？這個問題，你怎麼答？</p><p>參考閱讀：</p><ul><li><a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw== &amp;mid=2650774333&amp;idx=1&amp;sn=fde99e9869b07fe073c49a4a020d6758&amp;chksm=871a5f43b06dd655e71a83650fe55c76fd474bb868c98df5a368690def3a2778928eae9bdab6&amp;scene=21#wechat_redirect" target="_blank">吃虧受苦、前途未卜，Nature 調查顯示博士生三分之​​一可能抑鬱</a></li><li><a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp ;mid=2650774598&amp;idx=2&amp;sn=5c6b93f9e8faf3e85103740062b0ee1d&amp;chksm=871a5838b06dd12e289c1c8d62ca81a1acb2702c623e81e4c3a98d0d5df412602ff387a3e36a&amp;scene=21#wechat_redirect" /span></a></li></ul><p>Transformer 在多個模態（語言、圖像、蛋白質序列）中獲得了SOTA 結果，但它存在一個缺點：自註意力機制的平方級複雜度限制了其在長序列領域中的應用。目前，研究人員提出大量高效Transformer 模型（「xformer」），試圖解決該問題。其中很多展示出了媲美原版Transformer 的性能，同時還能有效降低自註意力機制的內存複雜度。</p><p>谷歌和DeepMind 的研究人員對比了這些論文的評估和實驗設置，得到了以下幾點發現：</p><ul><li>首先，高效Transformer 缺少統一的基準測試，使用的任務類型也多種多樣：每個模型在不同的任務和數據集上進行評估。</li><li>其次，評估所用基准通常是隨意選擇的，未充分考慮該任務是否適用於長程建模評估。</li><li>第三，很多論文將歸納偏置的效果和預訓練的優點混為一談，這會模糊模型的真正價值：預訓練本身是計算密集型的，將歸納偏置和預訓練分離開來可降低xformer 研究的門檻。</li></ul><p> 於是，谷歌和DeepMind 的研究人員提出了一個新基準Long-Range Arena (LRA)，用來對長語境場景下的序列模型進行基準測試。該基準包括合成任務和現實任務，研究人員在此基准上對比了十個近期提出的高效Transformer 模型，包括Sparse Transformers、<a href="http://mp.weixin.qq.com/s? __biz=MzA3MzI4MjgzMw==&amp;mid=2650800984&amp;idx=4&amp;sn=b8ba889031d2f6810689cdb7bebf1342&amp;chksm=84e5c726​​b3924e30d07c23d6e0501238ca198ce987915bbcb85457864ebc44a2287138be2724&amp;scene=21#wechat_redirect" target="_blank">Reformer</a> ;、Linformer、Longformer、Sinkhorn Transformer、<a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650800984&amp;idx=4&amp; sn=b8ba889031d2f6810689cdb7bebf1342&amp;chksm=84e5c726​​b3924e30d07c23d6e0501238ca198ce987915bbcb85457864ebc44a2287138be2724&amp;scene=21#wechat_redirect" target="_blank">Performer</a>、Synthesizer、Linear Transformer 和BigBird 模型。</p><p>該基準主要關注模型在長語境場景下的能力，不過研究人員對xformer 架構在不同數據類型和條件下的能力也很感興趣。因此，該基準選擇了具備特定先驗結構的數據集和任務。例如，這些架構可以建模層級結構長序列或包含某種空間結構形式的長序列嗎？這些任務的序列長度從1K 到16K token 不等，還包括大量數據類型和模態，如文本、自然圖像、合成圖像，以及需要類似度、結構和視覺- 空間推理的數學表達式。該基準主要面向高效Transformer，但也可作為長程序列建模的基準。</p><p>除了對比模型質量以外，該研究還進行了大量效率和內存使用分析。研究者認為，並行性能基準測試對於社區是有益且珍貴的，能夠幫助大家深入了解這些方法的實際效率。總之，該研究提出了一個統一框架，既能對高效Transformer 模型進行簡單的並行對比分析，還能對長程序列模型進行基準測試。該框架使用JAX/FLAX1 編寫。</p><p><b>高效Transformer 評估新基準：Long-Range Arena (LRA)</b></p><p><b>基準需求</b> ;</p><p>在創建LRA 基準之前，研究者先列舉了一些需求：</p><ul> <li>1. 通用性：適用於所有高效Transformer 模型。例如，並非所有xformer 模型都能執行自回歸解碼，因此該基準中的任務僅需要編碼。</li><li>2. 簡潔性：任務設置應簡單，移除所有令模型對比複雜化的因素，這可以鼓勵簡單模型而不是笨重的pipeline 方法。</li><li>3. 挑戰性：任務應該對目前模型有一定難度，以確保未來該方向的研究有足夠的進步空間。</li><li>4. 長輸入：輸入序列長度應該足夠長，因為評估不同模型如何捕獲長程依賴是LRA 基準的核心關注點。</li><li>5. 探索不同方面的能力：任務集合應當評估模型的不同能力，如建模關係和層級/ 空間結構、泛化能力等。</li><li>6. 非資源密集、方便使用：基準應該是輕量級的，方便不具備工業級計算資源的研究者使用。</li></ul><p><b>任務</b></p><p>LRA 基準包含多項任務，旨在評估高效Transformer 模型的不同能力。具體而言，這些任務包括：Long ListOps、比特級文本分類、比特級文檔檢索、基於像素序列的圖像分類、Pathfinder（長程空間依賴性）、Pathfinder-X（極端長度下的長程空間依賴性）。</p><p><b>LRA 任務所需的注意力範圍& lt;/b></p><p>LRA 基準的主要目標之一是評估高效Transformer 模型捕獲長程依賴的能力。為了對注意力機制在編碼輸入時需要考慮的空間範圍進行量化估計，該研究提出了「所需注意力範圍」(required attention span)。給出一個注意力模型和輸入token 序列，注意力模塊的所需注意力範圍是query token 和attended token 間的平均距離。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-28c29dd4c54c8da2ae851b0b7561754b_r.jpg" data-caption="" data-size=" normal" width="702" referrerpolicy="no-referrer"></figure><p>圖2 總結了LRA 基準中每項任務的所需注意力範圍，從圖中可以看出每項任務的所需注意力範圍都很高。這表明，Transformer 模型不僅僅涉及局部信息，在很多任務和數據集中，注意力機制通常需要結合鄰近位置的信息。</p><p><b>實驗</b></p><p><b>量化結果</b></p><p> 實驗結果表明，LRA 中的所有任務都具備一定的挑戰性，不同xformer 模型的性能存在一定程度的差異。具體結果參見下表1：</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-168ddf8834a85925a618a5d6bab995b9_r.jpg" data-caption= "" data-size="normal" width="1080" referrerpolicy="no-referrer"></figure><p><b>效率基準</b></p>< p>表2 展示了xformer 模型的效率基準測試結果：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-cfed08dfc42f97ccf807eca068881457_r .jpg" data-caption="" data-size="normal" width="1080" referrerpolicy="no-referrer"></figure><p> 從中可以看出，低秩模型和基於核的模型通常速度最快。整體最快的模型是Performer，在4k 序列長度上的速度是Transformer 的5.7 倍，Linformer 和Linear Transformer 緊隨其後。最慢的模型是Reformer，在4k 序列長度上的速度是Transformer 的80%，在1k 序列長度上的速度是Transformer 的一半。</p><p>此外，研究者還評估了這些模型的內存消耗情況。結果顯示，內存佔用最少的模型是Linformer，在4k 序列長度上只使用了0.99GB per TPU，而原版Transformer 使用了9.48GB per TPU，內存佔用減少了約90%。</p><p><b>整體結果：不存在萬能模型</b></p><p>根據研究人員的分析，在LRA 所有任務中整體性能最好（ LRA 分數最高）的模型是BigBird。但是，BigBird 在每項任務中的性能均不是最好，它只是在所有任務上都能取得不錯的性能。Performer 和Linear Transformer 在一些任務中表現搶眼，但其平均分被ListOps 任務拖累。</p><p>下圖3 展示了模型性能、速度和內存佔用之間的權衡情況。BigBird 性能最好，但速度幾乎與原版Transformer 相同。而Local Attention 模型速度很快，但性能較低。在這些模型中，基於核的模型（如Performer、Linformer 和Linear Transformer）能夠在速度和性能之間獲得更好的折中效果，同時內存佔用也較為合理。< /p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-4e535482d76df85aa127ca747d7d21ce_r.jpg" data-caption="" data-size="normal" width="1080" referrerpolicy="no-referrer"></figure><p></p></div></description><author>機器之心</author><guid> https://zhuanlan.zhihu.com/p/319458006</guid><pubDate>Sun, 29 Nov 2020 12:10:08 GMT</pubDate></item><item><title>重提大一統？微軟被曝開發原生運行安卓App的Win10系統，最早明年見</title><link>https://zhuanlan.zhihu.com/p/319452979</link><description><p><img src= "https://pic3.zhimg.com/v2-8d0b8af74b194cd48a2d4161f1af0db2_b.jpg"></p><div>& lt;blockquote>據消息人士透露，微軟正在著手一個代號為「拿鐵」的項目。如果一切順利，開發者有望在不更改代碼或更改少量代碼的情況下將安卓app 引入Win10。</blockquote><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-9a41bfb59f4f12914f44f7a0c8657696_r.jpg" data-caption="" data-size=" normal" width="830" referrerpolicy="no-referrer"></figure><p>近日，外媒Windows Central 報導稱，據消息人士透露，微軟正在開發原生支持安卓應用的系統，有望允許開發者將安卓app 打包成MSIX 提交到Microsoft Store，從而無需更改代碼（或更改少量代碼）即可將安卓app 引入Windows 10 系統。消息人士還透露，該項目名為「Latte（字面含義：拿鐵）」，最早將於明年發布。</p><p>此前，微軟曾計劃通過一個名為Astoria 的項目將安卓app 引入Windows 10 系統，但該項目無疾而終。「Latte」項目的開發目的與之類似，並且可能由Windows 面向Linux 的子系統WSL 提供支持。但要真正運行起來，微軟需要提供能夠運行安卓app 的安卓子系統。</p><p> 微軟此前已經宣布：WSL 即將獲得對GUI Linux 應用的支持，同時GPU 也將提速，以提升app 在WSL 上運行的性能。</p><p>「Latte」項目不太可能包含對Play Services 的支持，因為谷歌不允許Play Services 安裝在原生安卓設備和Chrome OS 以外的任何設備上。這意味著依賴Play Services API 的app 需要進行更新，以移除這些依賴關係來適配Windows 10 系統。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-dd992320cb149dc52351c8282d97dc6f_r.jpg" data-caption="" data-size=" normal" width="830" referrerpolicy="no-referrer"></figure><p>目前，用戶可以使用Windows 10 內置的「Your Phone」app 在PC 上運行安卓app。不過該功能僅限於部分三星設備，且目前還不夠穩定可靠。如果能在PC 本地安裝和運行安卓app，用戶就可以不受手機設備的影響，在PC 上使用缺乏Windows 版本的app。</p><p> 有消息稱，「Latte」項目產品將在明年正式上市，並且可能會在2021 年秋季發布的Windows 10 中發布。但有一點尚需注意：許多安卓app 主要是為手機設計的，因此在比手機大的屏幕上效果可能都不太理想。</p><p>過去幾年，微軟明確表示，在平台上進行app 開發時，不再只重視原生Windows app，而是歡迎引入更多app 開發平台，比如PWA、UWP、Win32、 Linux (via WSL) 等，現在，安卓平台也將包括在內。</p><p><b>對標蘋果？</b></p><p>手機上的APP 在電腦上用不了，這是很多人在日常生活、工作中遇到的尷尬。在移動端優先的開發環境中，很多開發者都會優先進行移動端的開發，這就導致桌面端操作系統陷入了尷尬的處境。因此，打通整個生態成了操作系統能否存活下來的關鍵。</p><p>前段時間，蘋果M1 芯片的發布掀起了一場生態巨變。由於M1 採用了和蘋果A 系列一樣的Arm 架構，所以搭載M1 芯片的Mac 產品可以運行之前為iOS、iPadOS 開發的應用。這意味著macOS 變成了目前應用生態最豐富的桌面操作系統。在移動互聯網時代，這一改變意義重大，也給Windows 和Intel 組成的Wintel 聯盟帶來了不小的挑戰。</p><p> 長期以來，這個聯盟憑藉英特爾的摩爾定律和微軟Windows 系統的升級換代壟斷了桌面端，在硬件、系統兩大核心技術領域高度捆綁住了眾多的硬件廠商和軟件廠商，形成了全球最大的一個生態。但隨著個人電腦業務下滑和平板、手機等移動端的崛起，蘋果和谷歌后來居上，Wintel 聯盟逐漸出現了裂痕。</p><p>近年來，為了在移動優先的互聯網時代佔有一席之地，微軟也開始發力對Arm 架構的支持，很早就開始佈局專門為ARM 架構處理器編譯的Windows on ARM，只是進展一直非常緩慢。</p><p>如果「Latte」項目的計劃一切順利，將安卓app 引入進來，那麼Windows 10 將成為一個在app 支持方面幾乎「通用」的操作系統。</p><p><i>參考鏈接：</i></p><p><a href="https://www.windowscentral.com/windows-10- project-latte-android-apps" target="_blank"><span class="invisible">https://www.</span><span class="visible">windowscentral.com/ wind</span><span class="invisible"> pubDate>Sun, 29 Nov 2020 07:07:02 GMT</pubDate></item><item><title>王者榮耀AI絕悟完全體對戰開啟：英雄隨便選，論文已被NeurIPS收錄</title> <link>https://zhuanlan.zhihu.com/p/319448286</link><description><p><img src="https://pic1.zhimg.com/v2-0635e35a91b04bd6143948efd8839e02_b.jpg"> ;</p><div><blockquote>人工智能2 級就來越塔來殺我，這遊戲怎麼玩？</blockquote><p>機器之心報導，作者：小舟、陳萍、澤南。</p><p>還記得今年五一假期時，騰訊在王者榮耀遊戲中上線的<a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw== &amp;mid=2650786169&amp;idx=1&amp;sn=ac431cc3beb15b61255e367568c1d214&amp;chksm=871a0d07b06d8411b0b1fdf1650dcdcbab40fa83b48d946275b4674b0a73ffc69d121e0100a4&amp;scene=21#wechat_redirect" target="_blank">絕悟AI 挑戰</a>嗎？大多數玩家只能將將挑戰前幾個難度，AI 擊敗你之後還會在聊天裡嘲諷。在AlphaGo 的陰影籠罩圍棋之後，人工智能也把觸角延伸到了最流行的遊戲中。</p><p>最近，騰訊AI Lab 在王者榮耀中的研究也獲得了學界的認可，研究人員提交的論文也<b>被全球頂尖人工智能會議NeurIPS 2020 收錄。</b></p><p>而在11 月中旬，升級版的絕悟AI 挑戰也在王者榮耀中上線，20 個難度關卡的設置為玩家們留下了充足的挑戰空間。在遊戲中，人工智能由5 個智能體互相協作，並不擁有全局視野，反應速度也被調整為和人類類似。在高級關卡中，AI 也會和你在開戰前進行挑選英雄的BP 博弈。</p><p>在11 月28-30 日限時開放的絕悟升級版最終挑戰裡，玩家和AI 可以選擇目前版本遊戲裡幾乎所有的英雄，也接受5 人組隊挑戰。看起來自我博弈的AI 已經達到了無限接近成熟的水平。</p><p>根據已經進行的一些比賽來看，絕悟通過自我博弈學習，早已學會了蹲草叢這樣的操作：</p><figure data-size="normal"> ;<img src="https://pic4.zhimg.com/v2-930d95bca03310d32285fef91142dfdf_b.gif" data-caption="" data-size=" com/v2-110f92fce86d60fa02e174a415e80d8e_r.jpg" data-caption="" data-size="normal" data-thumbnail="https://pic3.zhimg.com/v2-110f92fce86d60fa02e174a415e80d8e_b.jpg" width="535" referrerpolicy=" no-referrer"></figure><p>據AI Lab 的研究人員透露：「有些冷門英雄AI 玩的非常強勢，到時候可能會顛覆一部分大家對於遊戲的認知。」</ p><p>在騰訊AI Lab 及騰訊天美工作室發表的論文《Towards Playing Full MOBA Games with Deep Reinforcement Learning》中，研究人員們提出了MOBA AI 學習範式，支持使用深度強化學習來玩完整的MOBA 遊戲。</p><p>具體而言，該研究將新的和已有的學習技術結合起來，包括課程自我博弈學習、策略蒸餾（policy distillation）, 離策略自適應（off-policy adaption） 、多頭值估計（multi-head value estimation）、蒙特卡洛樹搜索等，在訓練和用大量英雄玩遊戲，同時巧妙地解決了可擴展性的問題。</p><p> 多人在線競技類游戲MOBA 長久以來一直吸引著眾多玩家，其中的王者榮耀、英雄聯盟、Dota 2 等最近也常被AI 研究者當做人工智能的實驗場，其中的多智能體、巨大的狀態動作空間、複雜的環境等元素向AI 系統提出了極大的挑戰。開髮用於MOBA 遊戲的AI 引起了廣泛的關注。</p><p>然而，當OpenAI 的Dota AI 將游戲限制在只能選擇17 名英雄的情況下，若想擴展英雄庫，現有的工作在處理由智能體組合（即陣容）爆炸性增長所導致的遊戲複雜性方面的問題存在難度。因此，現有的AI 系統並不能掌握完全沒有限制​​的MOBA 遊戲。</p><p>在日均活躍玩家數量超1 億的國民手游王者榮耀上進行測試，展示了人類可以打造出能夠擊敗頂級電子競技玩家的超級AI 智能體。通過文獻中首次對MOBA AI 智能體進行大規模性能測試，證明了該AI 的優越性。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-9ec3d25efa0b57ad2f5957673d467488_r.jpg" data-caption="" data-size=" normal" width="1027" referrerpolicy="no-referrer"></figure><p>論文地址：<a href="https://arxiv.org/abs/2011.12692" target="_blank " ><span class="invisible">https://</span><span class="visible">arxiv.org/abs/2011.1269</span><span class="invisible ">2</span><span class="ellipsis"></span></a></p><p><b>學習系統</b>< /p><p>為了解決MOBA 遊戲的複雜性，該研究結合了新的和現有的學習技術，用於神經網絡架構、分佈式系統、強化學習、多智能體訓練、課程學習和蒙特卡羅樹搜索。儘管該研究使用王者榮耀進行研究，但這些技術同樣適用於其他MOBA 遊戲，因為MOBA 遊戲的遊戲機制是類似的。</p><p>研究人員表示，這套強化學習系統運行在一個物理集群上。後續進行的控制變量、時間和性能比較實驗都使用了相同的資源量進行訓練：320 張GPU 卡以及35000 個CPU 核。</p><p><b>模型架構</b></p><p>在模型架構中，絕悟使用了actor-critic結構。<br>< br><b>策略更新</b><br><br>研究人員使用了雙邊截斷的PPO方法：</p><figure data-size="normal">< img src="https://pic4.zhimg.com/v2-b3fe716fbdee613b620dfa514ea241fb_r.jpg" data-caption="" data-size="normal" width="680" referrerpolicy="no-referrer"></ figure><p><b>值函數更新</b><br><br>模型使用了遊戲狀態的完整信息，包括隱藏的觀測信息作為值函數的輸入來降低值估計的方差。值得注意的是，這種方式僅在訓練階段使用，因為測試時只需要策略網絡。為了更準確地估計值函數，研究人員引入了multi-head value(MHV)方法。從下圖可以看出，reward被分成了5大類。損失函數定義如下：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-e40183cb021feca7c0aab341d4e0ac56_r.jpg" data-caption="" data-size=" ></figure><p><b>多智能體訓練</b></p><p>大型英雄池會導致大量的陣容。當使用自我博弈強化學習時，10 個智能體玩一個MOBA 遊戲面臨著運動目標不穩定的問題。此外，不同的self-play 遊戲陣容各不相同，這使得策略學習更加困難。在訓練中出現紊亂的智能體組合導致性能下降。這需要一個範式來指導MOBA 中的智能體學習。</p><ul><li>階段1，從簡單的任務開始，訓練固定的陣容。</li><li>階段2，專注於如何繼承固定陣容的self-plays 遊戲所掌握的知識。</li><li>階段3，通過從第2 階段中提取的模型進行模型初始化，從英雄池中隨機挑選陣容進行持續訓練。</li></ul><p><b>學習徵召模式</b></p><p>王者榮耀的AI 不僅需要會操作英雄，還需要會選英雄，更加需要會玩套路，且限制玩家的套路。</p><p> 擴大英雄池所帶來的一個新問題是BAN 選機制（Ban Pick）。在MOBA 比賽開始之前，兩支隊伍經過挑選英雄的過程，這將直接影響未來的策略和匹配結果。給定一個龐大的英雄池，例如40 個英雄（超過1011 個組合），一個完整的樹搜索方法，如OpenAI Five[2]中使用的Minimax 算法，在計算上是困難的。</p><p>為了解決這個問題，研究者利用蒙特卡洛樹搜索（MCTS）和神經網絡開發了一種BAN 選智能體。MCTS 會估算每次選擇的長期價值，價值最大的英雄將被選中。該研究使用的特定MCTS 版本是應用於樹（UCT）的上置信界。在徵召模式Ban 選的過程中迭代地構建一棵搜索樹，其中每個節點代表一個狀態（兩個團隊都已經選擇了英雄），每條邊代表一種動作（選擇尚未選擇的英雄），然後會產生下一個狀態。</p><p>研究人員使用了3000 萬樣本(自對弈產生) 訓練勝率預測器，使用了1 億樣本(樣本基於MCTS 方法的挑選策略產生) 對值網絡進行了訓練。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-b1b8d532375eacac42ae67315465f146_r.jpg" data-size="normal" width="798 " referrerpolicy="no-referrer"><figcaption>絕悟和人類對決的BP 測試。</figcaption></figure>& lt;p>除了常見的單輪BP ，AI 教練還學會了王者榮耀KPL 賽場上常見的多輪BP 賽制，該模式下不能選重複英雄，對選人策略要求更高。團隊因此引入多輪長周期判定機制，在BO3/BO5 賽制中可以全局統籌、綜合判斷，做出最優BP 選擇。訓練後的BP 模型在對陣基於貪心策略的基準方法時，能達到近70% 勝率，對陣按位置隨機陣容的勝率更接近90%。</p><p><b>AI 的水平大致在什麼階段</b></p><p>騰訊AI Lab 訓練了一個英雄池大小為40 的AI 版本，覆蓋了所有英雄角色，包括坦克、法師、輔助、刺客、射手和戰士。英雄池的規模是OpenAI 的2.4 倍，英雄組合複雜度提升了2.1×10^11 倍。在挑選英雄階段，人類玩家可以從40 個英雄中隨機挑選。在對局中，研究人員沒有對遊戲規則進行任何限制，玩家可以任意購買物品或者使用自己喜歡的召喚師技能。</p><p>為了測試AI 的真實水平，騰訊邀請了王者榮耀職業玩家來和AI 進行對抗。從2020 年2 月13 日到4 月30 日，AI 每週都會和職業玩家進行多次對弈。職業玩家被鼓勵使用他們擅長的英雄以及嘗試不同的遊戲策略。在最初的10 週時間內，人機對決共進行了42 場，AI 贏了40 場（95.2% 的勝率，置信區間[0.838,0.994]）。</p><p> 2020 年5 月1 日到5 月5 號，騰訊AI Lab 將AI 部署到王者榮耀正式服務器上和玩家公開對抗，對參與玩家設置了段位門檻。為了鼓勵玩家參與，玩家如果擊敗了AI 就能夠拿到一枚榮譽徽章。最終，絕悟AI 與頂尖玩家對抗了642,047 局，AI 贏得了其中的627,280 局(勝率97.7，置信區間[0.9766, 0.9774])。對比其它的公開遊戲AI 測試：AphaStar 和OpenAI 各打了90 和7,257 場，而且對參與者沒有遊戲水平的要求。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-78933fdd8501a1ca4fe3715488b242c9_r.jpg" data-caption="" data-size=" normal" width="1080" referrerpolicy="no-referrer"></figure><p><i>圖3：訓練過程：a)教師模型的訓練，即CSPL 的第一階段。b)蒸餾（distillation）過程中的Elo 變化，即CSPL 的第二階段，學生模型的收斂Elo 略低於教師模型。c)和d)分別針對20 個英雄和40 個英雄的情況，將CSPL 的Elo 變化與基線方法進行比較。注意基線方法中沒有第一階段和第二階段。CSPL 在擴展英雄池時具有比基線方法更好的擴展性。</i></p><p> 人們對於研究者們使用的ELO 評分機制可能會留有印象：ELO 等級分在棋類游戲和AI 研究領域經常被用做評價標準，在AlphaGo 的論文中，ELO 也被用於評判AI 的下棋水平。</p><p>具體來說，Top10% 水平的人類玩家ELO 分數大約為1050，Top 1% (王者榮耀中的王者段位)大約為1500，Top 0.01% 大約為1700，職業玩家的水平應該在1730 以上。</p><p>在圖3 中，研究者說明了CSPL 的整個訓練過程和基線方法。表1 對比了兩種方法的具體訓練時間。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-dfb9e35643668107868632028a2dde39_r.jpg" data-caption="" data-size=" normal" width="1080" referrerpolicy="no-referrer"></figure><p>為了進一步分析該方法中的組件，研究者進行了幾項內部控制變量實驗，結果如圖4 所示。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-f883e185413c3885b00d285a50a39eda_r.jpg" 可見裡面不僅有瑤、孫臏等操作相對簡單的英雄，也有露娜、公孫離這種複雜的——騰訊選擇的還都是人類玩家在對戰時經常用的那些。不過提交給NeurIPS 2020 的論文是在幾個月前完成的，從目前遊戲中開放的第二十關挑戰來看，在人機對決中絕大多數的英雄（除了兩三個新英雄）都可以選了。</p><p>騰訊AI Lab 在王者榮耀上的研究可謂成果豐厚，除了這篇NeurIPS 2020 論文之外，騰訊AI Lab 還有一篇監督學習方面的研究被IEEE 期刊TNNLS 收錄《Supervised Learning Achieves Human-Level Performance in MOBA Games: A Case Study of Honor of Kings》：<a href="https://arxiv.org/abs/2011.12582" target="_blank"><span class="invisible ">https://</span><span class="visible">arxiv.org/abs/2011.1258</span><span class="invisible">2</span>< ;span class="ellipsis"></span></a></p><figure data-size="normal">< img src="https://pic2.zhimg.com/v2-512c560d30faa90d50c5aeb62649ef55_r.jpg" data-caption="" data-size="normal" width="553" referrerpolicy="no-referrer"></ figure><p>在第二篇論文中，研究人員提出了一套適用於MOBA 遊戲的特徵和標籤。在此基礎上，AI 建模任務被定義為一個層級的多分類問題，通過神經網絡模型來解決。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-1f90333142958e77ff5e3607a8456103_r.jpg" data-caption="" data-size=" normal" width="553" referrerpolicy="no-referrer"></figure><p>可以看到，AI 在遊戲中使用的是和人類玩家一樣的視角。</p><p>在人類玩家打MOBA 遊戲時，通常會分為三路開局：戰士走上路，射手輔助走下路，法師在中路。然而通過自我博弈訓練出的絕悟幾乎不採用這樣的套路，這或許為人類玩家未來提高戰術水平提供了新的思路。</p><p> 在短短兩年時間裡，絕悟從業餘玩家水平提升到了精通100 個英雄，甚至還能教人類打王者，這樣的速度可謂驚人。最新的版本因而得名「絕悟完全體」。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-f9a33c5e7204414c5329fd170b93f05f_r.jpg" data-caption="" data-size=" normal" width="605" referrerpolicy="no-referrer"></figure><p>最後，如果有人對戰AI 有了很大的挫敗感，實踐證明人工智能打王者榮耀的能力還不是完美的。有玩家在對戰時看到了AI 的迷惑之語：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-d2c764c9540a5035d118bdf1a097bca2_r. jpg" data-caption="" data-size="normal" width="762" referrerpolicy="no-referrer"></figure><p> 另外，基於強化學習的絕悟還需要耗費大量的服務器資源，所以人機對戰的活動是限時的。希望在不久的將來，我們能在隊友斷線重連的時候有絕悟AI 來接手，完成五殺。</p></div></description><author>機器之心</author><guid>https://zhuanlan.zhihu.com/p/319448286</guid><pubDate>Sun, 29 Nov 2020 03:35:20 GMT</pubDate></item><item><title>124頁哈佛數學系本科論文，帶你了解流形學習的數學基礎</title><link>https:// zhuanlan.zhihu.com/p/315778127</link><description><p><img src="https://pic1.zhimg.com/v2-f3eb32629486ace3a1406afdbfff6c8b_b.jpg"></p>< ;div><blockquote>近日，哈佛大學數學系畢業生、現牛津大學博士Luke Melas-Kyriazi 發布其本科畢業論文，結合統計學習、譜圖理論和微分幾何三個數學領域介紹流形學習。</blockquote><p>機器之心報導，編輯：魔王。</p><p> 流形學習（manifold learning）是機器學習、模式識別中的一種方法，在維數約簡方面具有廣泛的應用。它的主要思想是將高維的數據映射到低維，使該低維的數據能夠反映原高維數據的某些本質結構特徵。流形學習的前提是有一種假設，即某些高維數據，實際是一種低維的流形結構嵌入在高維空間中。流形學習的目的是將其映射回低維空間中，揭示其本質。流形學習可以作為一種數據降維的方式。此外，流形能夠刻畫數據的本質，主要代表方法有等距映射、局部線性嵌入等。</p><p>自2000 年在著名的科學雜誌《Science》首次提出以來，流形學習成為機器學習領域中的一個熱點。近日，一篇來自哈佛大學數學系的本科畢業論文引起了大家關注。它結合三個看似不太相關的數學領域來介紹流形學習的數學基礎，這三個領域分別是：<b>統計學習、譜圖理論和微分幾何</b>。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-fb76346eac44c4f04409e1075f5a7783_r.jpg" data-caption="" data-size=" normal" width="938" referrerpolicy="no-referrer"></figure><p>論文鏈接：<a href="https://arxiv.org/pdf/2011.01307.pdf" target= "_blank">< span class="invisible">https://</span><span class="visible">arxiv.org/pdf/2011.0130</span><span class="invisible">7 .pdf</span><span class="ellipsis"></span></a></p><p><b>什麼是流形學習？</b></p><p>要想從數據中學習，我們首先要假設數據具備某種內在結構。在一些機器學習方法中，該假設是隱式的。而流形學習領域中該假設是顯式的，它假設觀察到的數據是嵌入在高維空間中的低維流形。直觀來看，這一假設（又叫流形假設）認為數據的形態是相對簡單的。</p><p>以自然圖像的空間為例。圖像是以像素形式存儲的，因此圖像空間在像素空間R^H×W×3 內。但是，我們希望自然圖像空間的維度比像素空間低一些，像素空間某種程度上幾乎被看起來像「噪聲」的圖像塞滿了。此外，我們可以看到自然圖像空間是非線性的，因為兩個自然圖像的（像素級）平均並非自然圖像。流形假設認為，自然圖像空間具備低維流形嵌入在高維像素空間中的微分幾何結構。</p><p> 應當強調的是，流形學習不是監督學習、無監督學習那樣的學習類型，這些學習類型指的是學習任務（是否具備標註數據），而流形學習指的是一組基於流形假設的方法。流形學習方法多在半監督和無監督學習設置下使用，不過也可以用在監督學習環境中。</p><p><b>論文內容概覽</b></p><p>該論文結合三個數學領域來介紹流形學習：統計學習、譜圖理論和微分幾何，並在最後一章中介紹了<b>流形正則化</b>的思想。流形正則化可以學習與數據流形相關的函數，而不是數據所在的外圍空間。</p><p>要想了解流形學習和流形正則化，我們首先需要了解<b>核學習</b>（kernel learning），以及流形與圖之間的關係。</p><p>論文第二、三章重點介紹核學習。第二章介紹了監督和半監督學習的基礎知識，第三章介紹再生核希爾伯特空間中的監督核學習理論，該理論為大量正則化技術奠定了嚴謹的數學基礎。</p><p>第四章通過<b>拉普拉斯算子</b>來探索流形與圖之間的關係。乍一看，流形與圖似乎區別很大，但拉普拉斯算子揭示了二者之間的對應性。</p><p> 第五章介紹了流形正則化。該研究發現，使用基於數據所生成圖的拉普拉斯算子，可以很容易地將流形正則化添加至多種學習算法。本章證明了這一圖方法的理論有效性：在無限數據情況下，數據圖的拉普拉斯算子能夠收斂至數據流形的拉普拉斯算子。</p><p>論文目錄如下：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-c752fac90a28c267cdc8ed8d836911ef_r. jpg" data-caption="" data-size="normal" width="1080" referrerpolicy="no-referrer"></figure><figure data-size="normal"><img src ="https://pic3.zhimg.com/v2-91e45e534c08da29ec64015a8a816c0e_r.jpg" data-caption="" data-size="normal" width="1080" referrerpolicy="no-referrer"></figure> <p><b>作者簡介</b></p><figure data-size=" a href="https://github.com/lukemelas" target="_blank"><span class="invisible">https://</span><span class="visible"> github.com/lukemelas</span><span class="invisible"></span></a></p></div></description><author>機器之心< /author><guid>https://zhuanlan.zhihu.com/p/315778127</guid><pubDate>Fri, 27 Nov 2020 13:53:12 GMT</pubDate></item><item><title >3D視覺：一張圖像如何看出3D效果？</title><link>https://zhuanlan.zhihu.com/p/315772436</link><description><div><blockquote>不同於人類，計算機「看待」世界有自己的方式。為了達到類似人類的視覺水平，各種算法層出不窮，本篇就來窺探其冰山一角。</blockquote><p><b& gt;機器之心原創，作者：陳萍。</b></p><p>我們生活的世界是一個三維物理空間。直觀而言，三維視覺系統有助於機器更好地感知和理解真實的三維場景。三維視覺作為計算機視覺的一個比較重要的研究方向，在過去幾十年間得到了紮實和系統地發展，形成了一套完整的理論體系。近年來，隨著三維成像技術如激光雷達、TOF 相機及結構光等的快速發展，三維視覺研究再次成為研究熱點。</p><p>在<a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650803376&amp;idx=3&amp ;sn=7d7cca1f447aaee307c1b8aa2e2f6e9f&amp;chksm=84e5c8ceb39241d8b5a7f4e76f1fbc9a7d5284fcce4a6963f82fef6590baff39ab5c6bced5db&amp;scene=21#wechat_redirect" target="_blank">上一篇文章</a>中，我們對3D 視覺基礎相關內容進行了概括性總結，本文我們將進行比較深層次的介紹，主要涉及3D 視覺算法及其應用領域。</p><p><b>3D 目標檢測多模態融合算法</b></p><p> 基於視覺的目標檢測是環境感知系統的重要組成，也是計算機視覺、機器人研究等相關領域的研究熱點。三維目標檢測是在二維目標檢測的基礎上，增加目標尺寸、深度、姿態等信息的估計。相比於二維目標檢測，三維目標檢測在準確性、實時性等方面仍有較大的提升空間。</p><p>在目標檢測領域，2D 目標檢測方面發展迅速，出現了以R-CNN、Fast RCNN、Mask RCNN 為代表的two-stage 網絡架構，以及以YOLO、SSD 為代表的one-stage 網絡架構。然而由於2D 圖像缺乏深度、尺寸等物理世界參數信息，在實際應用中存在一定局限性，往往需要結合激光雷達、毫米波等傳感器實現多模態融合算法，以增強系統的可靠性。</p><p>因此，研究者們提出了許多3D 目標檢測方法，<b>根據傳感器的不同大致可分為視覺、激光點雲以及多模態融合三大類</ b>。其中視覺又包括單目視覺和雙目視覺（深度視覺）兩類；激光點雲包括三維點雲投影和三維空間體素特徵；而多模態融合實現了激光點雲與視覺的融合。下面將對現階段比較流行的3D 目標檢測多模態融合算法研究進行介紹。</p><p>論文1<b>《3D-CVF: Generating Joint Camera and LiDAR Features Using Cross-View Spatial Feature Fusion for 3D Object Detection》提出了voxel-based 的多模態特徵融合< /b>。</p><figure data-size="normal">< 該研究提出的網絡整體結構如下所示。可以看出上下兩層分別是對激光雷達點雲信息的特徵提取(voxel-backbone) 和對多張圖像信息的特徵提取與模態轉換。這裡需要提及的是由於圖像信息僅僅只有一個方向的視野，但是多個攝像頭的圖像存在視野重疊，所以多張圖像的信息融合是為了保證整個環視點雲場景的特徵都被涉及到。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-79beed74752cb06827975cf26ed8dcdf_r.jpg" data-caption="" data-size=" normal" width="930" referrerpolicy="no-referrer"></figure><p>論文2<b>《PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based Attentive Cont- conv Fusion Module》提出了point-based 的多模態融合方法</b>。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-d2a4db427a949a98c28e9f9ef237b8ce_r.jpg" data-caption="" data-size=" 此外，基於PACF 模塊，研究人員提出了一個叫做Pointcloud-Image RCNN（PI-RCNN）的3D 多傳感器多任務網絡，該網絡負責圖像分割和3D 目標檢測任務。PI-RCNN 使用分段子網從圖像中提取全分辨率語義特徵圖，然後通過功能強大的PACF 模塊融合多傳感器特徵。受益於PACF 模塊的效果和分段模塊的有表達力的語義特徵，PI-RCNN 使3D 目標檢測的性能大大改善。在KITTI 3D 檢測基準測試中的實驗揭示了PACF 模塊和PI-RCNN 的有效性，並且該方法可以在3D AP 的度量標准上達到最新水平。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-bfdda5c701d21a147c38ae9a47572a79_r.jpg" data-caption="" data-size=" normal" width="941" referrerpolicy="no-referrer"></figure><p>網絡框架如上圖所示，實現過程可分為以下四步：</p><ul> <li>1. 使用圖像語義分割網絡，獲得圖像的語義特徵；</li><li>2. 檢測子網絡- 1 從原始點雲中得到目標的三維候選框；</li> ;<li>3. PACF 模塊融合點雲特徵和圖像語義特徵；</li>< li>4. 檢測子網絡- 2 得到最終的三維檢測結果。</li></ul><p>論文3<b>《EPNet: Enhancing Point Features with Image Semantics for 3D Object Detection》提出了一種新的融合模塊，在不需要任何圖像註釋的情況下，對具有語義特徵的點特徵進行逐點增強</b>。該研究設計了一個端到端的可學習框架EPNet 來集成兩個組件。在KITTI 和SUN-RGBD 數據集上進行的大量實驗表明，EPNet 優於當前最優方法。其網絡結構點雲分支是point encoder-decoder 結構，圖像分支則是一個逐步encoder 的網絡，並且逐層做特徵融合。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-06e5d27b3f2d72ff557e063648a2d597_r.jpg" data-caption="" data-size=" normal" width="692" referrerpolicy="no-referrer"></figure><p>論文地址：<a href="https://arxiv.org/pdf/2007.08856.pdf" target= "_blank"><span class=" data-caption="" data-size="normal" width="916" referrerpolicy="no-referrer"></figure><p>融合過程由三部分組成：grid generator、image sampler 和LI -Fusion layer。</p><ul><li>1. 根據三維激光與圖像的外參，grid generator 將三維激光的每一個點投影到原始圖像上；</li><li>2. image sampler 利用圖像特徵圖與原始圖像的比例關係以及雙線性插值，得到對應的圖像特徵圖；</li><li>3. 為了減少圖像的遮擋以及深度不確定性對融合造成的影響，LI-Fusion layer 利用點雲特徵估計對應圖像特徵的重要程度並篩選，具體是將點雲特徵與圖像特徵經過若干操作學習得到權重值，權重值與圖像特徵相乘再與點雲特徵串聯作為最後的融合特徵。</li></ul><p>論文4<b>《CLOCs: Camera-LiDAR Object Candidates Fusion for 3D Object Detection》提出了一種新穎的Camera-LiDAR 目標候選（CLOC）融合網絡< ;/b> 。CLOC 融合提供了一種低複雜度的多模態融合架構，顯著提高了單模態檢測器的性能。CLOC 在非最大抑制(NMS) 之前對任意2D 和任意3D 的組合輸出候選項進行操作，並被訓練利用它們的幾何和語義一致性，以產生更準確的最終3D 和2D 檢測結果，最後採用maxpooling的方式選擇最終的融合結果。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-67e6e091e74880d6d8952d6eb83dc5de_r.jpg" data-caption="" data-size=" normal" width="765" referrerpolicy="no-referrer"></figure><p>論文地址：<a href="https://arxiv.org/pdf/2009.00784.pdf" target= "_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/pdf/2009.0078</span><span class ="invisible">4.pdf</span><span class=" 隨著結構光和立體視覺等三維成像技術的日益成熟，越來越多的人臉識別研究人員將目光投向了三維人臉識別技術領域。</p><p>目前3D 人臉識別技術的主要技術流程如下：</p><ul><li>(1) 3D 人臉數據獲取；</li>< li>(2) 3D 人臉數據的預處理，包括人臉的檢測、切割、去噪等；</li><li>(3) 3D 人臉數據的特徵提取；</li> <li>(4) 構建適合的分類器對人臉數據進行判別。</li></ul><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-b30d8c262f9f18006186268a8079a883_r.jpg" data-caption="" data-size="normal" width="748" referrerpolicy="no-referrer"></figure><p> 目前3D 人臉識別算法分為如下幾個類別： </p>< ul><li>1. 基於空域匹配的識別算法</li><li>2. 基於局部特徵匹配的識別算法< /li><li>3. 基於整體特徵匹配的識別算法</li><li>4. 基於模型擬合的識別算法</li><li>5. 基於3D+2D 雙模態的識別算法</li></ul><p><b>3D 數據集簡介</b></p><p>目前3D 公開數據少，遠少於2D 圖片；3D 高精度數據集只能靠昂貴的設備採集，過程繁瑣。這裡我們來了解一下現有的3D 數據集。</p><p>1. BU-3DFE (Binghamton University 3D Facial Expression) 數據集：該數據庫目前包含100 位受試者（女性56％，男性44％），年齡從18 歲到70 歲不等，包含各種種族，包括白人、黑人、東亞人、中東人等。</p><p>下載地址：<a href="http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html" target="_blank">< span class="invisible">http://www.</span><span class="visible">cs.binghamton.edu/~liju< /span><span class="invisible">n/Research/3DFE/3DFE_Analysis.html</span><span class="ellipsis"></span></a></p> ;<p>2. KITTI 數據集：由德國卡爾斯魯厄理工學院和豐田美國技術研究院聯合創辦，是目前國際上最大的自動駕駛場景下的計算機視覺算法評測數據集。該數據集用於評估3D 目標檢測和3D 跟踪等計算機視覺技術在車載環境下的性能。</p><p>下載地址：<a href="http://www.cvlibs.net/datasets/kitti/raw_data.php" target="_blank"><span class="invisible ">http://www.</span><span class="visible">cvlibs.net/datasets/kit</span><span class="invisible">ti/raw_data.php< ;/span><span class="ellipsis"></span></a></p><p>3. Cityscapes 數據集：這是一個較新的大規模數據集，它包含50 個不同城市的街道場景中所記錄的各種立體視頻序列，除了一組較大的20000 個弱註釋幀外，還具有5000 幀的高質量像素級註釋。</p><p>下載地址：<a href="https://www.cityscapes-dataset.com/" target="_blank"><span class="invisible">https: //www.</span> <span class="visible">cityscapes-dataset.com/</span><span class="invisible"></span></a></p><p> 4. Matterport 3D 重建數據集：該數據集包含10800 個對齊的三維全景視圖（RGB + 每個像素的深度），來自90 個建築規模場景的194400 個RGB + 深度圖像。</p><p>下載地址：<a href="https://matterport.com/" target="_blank"><span class="invisible">https://< /span><span class="visible">matterport.com/</span><span class="invisible"></span></a></p><p> ;5. 3D 人臉重建相關數據集：該數據集包含用iPhone X 拍攝的100 名受試者的2054 張2D 圖像，以及每個受試者的單獨3D 頭部掃描。</p><p>下載地址：<a href="https: lt;/a></p><p>人臉數據庫匯總官網指路：<a href="http://www.face-rec.org/databases/" target="_blank"> <span class="invisible">http://www.</span><span class="visible">face-rec.org/databases/</span><span class=" invisible"></span></a></p><p><b>面部3D 重建</b></p><p>人臉重建是計算機視覺領域中一個比較熱門的方向，3D 人臉相關應用也是近年來短視頻領域的新玩法。不管是Facebook 收購的MSQRD，還是Apple 研發的Animoji，底層技術都與三維人臉重建有關。</p><p>面部3D 重建，可以理解為從一張或多張2D 圖像中重建出人臉的3D 模型。對於面部3D 重建，我們先來直觀地感受一下效果。</p><p>如下動圖所示，最右邊的重建人臉除了沒有皺紋以外，身份特徵和麵部表情都和原圖相當一致，陰影效果也高度還原。只是眼睛部分似乎不太對，顯得渾濁無神。</p>& lt;figure data-size="normal"><img src="https://pic3.zhimg.com/v2-fbfd4a689ee5cb4c0c18c4501c468aa2_r.jpg" data-size="normal" data-thumbnail="https:// pic3.zhimg.com/v2-fbfd4a689ee5cb4c0c18c4501c468aa2_b.jpg" width="633" referrerpolicy="no-referrer"><figcaption>論文《FML: Face Model Learning from Videos》效果展示</figcaption></ figure><p>下圖中的合成效果也很不錯，表情動態很到位。只是可能實驗者的眼神實在太有戲，AI 表示無力模仿。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-4fe919c0c6e0250ead6c857eeb9f8ad6_r.jpg" data-size="normal" data-thumbnail= "https://pic3.zhimg.com/v2-4fe919c0c6e0250ead6c857eeb9f8ad6_b.jpg" width="550" referrerpolicy="no-referrer"& gt;<figcaption>論文《Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network 》效果展示</figcaption></figure><p>直觀感受完面部3D 重建效果後，我們再來探究模型背後的算法。</p><p>傳統3D 人臉重建方法，大多立足於圖像信息，如基於圖像亮度、邊緣信息、線性透視、顏色、相對高度、視差等一種或多種信息建模技術進行3D 人臉重建。</p><p><b>三維變形模型（3DMM）</b></p><p>隨著技術的發展，研究者們又提出了基於模型的3D 人臉重建方法，這是目前較為流行的3D 人臉重建方法。3D 模型主要用三角網格或點雲來表示，現下流行的模型有通用人臉模型（CANDIDE-3）和三維變形模型（3DMM）及其變種模型，基於它們的3D 人臉重建算法既有傳統算法也有深度學習算法。</p><p>三維變形模型（3DMM）是一個通用的三維人臉模型，用固定的點數來表示人臉。其核心思想是人臉可以在三維空間中一一匹配，並且可以由其他許多幅人臉正交基加權線性相加而來。三維空間中的每一點(x, y, z) 實際上都是由三維空間三個方向的基量(1, 0, 0)，(0, 1, 0)，(0, 0, 1) 加權相加所得，只是權重分別為x，y，z。</p><p> 每一個三維人臉都可以在一個數據庫中的所有人臉組成的基向量空間中進行表示，而求解任意三維人臉的模型，實際上等價於求解各個基向量的係數問題。每一張人臉可以表示為形狀向量和紋理向量的線性疊加。</p><p>任意人臉模型均可以由數據集中的m 個人臉模型進行加權組合，如下：</p><figure data-size="normal"><img src ="https://pic2.zhimg.com/v2-d9e38cbecd7d4b714d67530f7fce787d_r.jpg" data-caption="" data-size="normal" width="803" referrerpolicy="no-referrer"></figure> <p>其中Si、Ti 表示數據庫中第i 張人臉的形狀向量和紋理向量。但是我們實際在構建模型的時候不能使用這裡的Si、Ti 作為基向量，因為它們之間並非正交相關，所以接下來需要使用PCA 進行降維分解。</p><ul><li>(1) 首先計算形狀和紋理向量的平均值；</li><li>(2) 中心化人臉數據；</li>< ;li>(3) 分別計算協方差矩陣；</li><li>(4) 求得形狀和紋理協方差矩陣的特徵值α、β和特徵向量si、ti。</li>< /ul><p>上式可以轉換為下式：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2- 48b0b55b839fe52df89d07e8d7396f66_r.jpg" data-caption="" data-size="normal" width="769" referrerpolicy="no-referrer"></figure><p>其中第一項是形狀和紋理的平均值，而si、ti 則是Si、Ti 減去各自平均值後的協方差矩陣的特徵向量，它們對應的特徵值按照大小進行降序排列。</p><p>等式右邊仍然是m 項，但是累加項降了一維，減少了一項。si、ti 都是線性無關的，取其前幾個分量可以對原始樣本做很好地近似，因此能夠大大減少需要估計的參數數目，並不損失準確率。</p><p>基於3DMM 的方法都是在求解這幾個係數，隨後的很多模型在這個基礎上添加了表情、光照等係數，但是原理與之類似。</p></div></description><author>機器之心</author><guid>https://zhuanlan.zhihu.com/p/315772436</guid><pubDate>Fri, 27 Nov 2020 09:20:54 GMT< /pubDate></item><item><title>魁北克大學教授痛批雙盲評審制度：這不是個好主意</title><link>https://zhuanlan.zhihu.com/p/313722528</ link><description><p><img src="https://pic3.zhimg.com/v2-064e8a7c3b5cf3886305fa1a140498cc_b.jpg"></p><div><blockquote>近日，魁北克大學計算機科學教授Daniel Lemire 在ACM 出版委員會會議上發表言論，批評學術界廣泛使用的雙盲評審制度。</blockquote><p>選自blog，作者：Daniel Lemire，機器之心編譯，編輯：魔王。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-91d078649cbef65cbf380f952b216400_r.jpg" data-caption="" data-size=" normal" width="1080" referrerpolicy="no-referrer"></figure><p>Daniel Lemire 的觀點如下：</p> <p>當你向期刊或會議提交論文手稿時，你不知道誰是評審人。由於對偏見和同質性的擔憂，越來越多的期刊和會議轉向雙盲評審，即投稿人不能透露自己的身份。另一種方式則更加開放——每個人都不匿名。</p><p>雙盲評審背後的想法是，在你不知道作者的姓名和機構時，你就不容易產生歧視。當然，編輯和主席仍然知道作者的身份。開放式同行評審的思路是，如果你的評審文章發表了，你需要繼續跟進。而如果你帶有太多偏見，則可能會遭受懲罰。</p><p>在專業化的環境中存在許多偏見。當然，有些偏見針對少數群體和女性，還有一些針對其他群體。有跡象表明在判斷一項工作的價值時，作者的聲譽可能成為決定性因素。人們通常會給類似自己的人高度評價。此外，還存在一些傳統偏見：不常見的idea 更難得到高評分，傳統機構收到高評價的可能性高於非傳統機構。</p><p>然而，我們不應那麼快就接受「把隱藏作者身份信息作為解決方案」。</p><p>很明顯，我們認為自己可以在招聘過程中有效解決偏見或歧視，因為大部分雇主不基於雙盲評審進行招聘。博士生提交論文進行評審時也不會隱藏自己的姓名。</p><p>那麼，為什麼在有如此明顯優勢的情況下，我們不想隱藏研究者的身份呢？</p><p>首先，雙盲評審的好處只存在於一些趣聞軼事中。事實是，人們有偏見、同質性，你可以一定程度上匿名化內容。但是，這些優勢的證據是混雜的，比如我們不知道這對女性是否有幫助。</p><p> 告訴來自不那麼有名的組織、貧窮或非英語國家、非傳統性別的人，他們需要隱藏這類身份信息才能得到平等對待，這本身並非積極信號。我當然想要居住在一個女性可以像男性一樣發表自己工作的世界。只強調偏見卻不恰當解決問題會讓這些領域對於那些受偏見影響的人失去吸引力。</p><p>另一個擔憂是，雙盲評審會使開放學術（open scholarship）變得困難。我的大部分論文是在網上發布的，我用開放的方式編寫軟件，我嘗試所有我認為屬於開放學術的事情。很顯然，這意味著我不能參與雙盲會議。使開放學術變得困難是一種倒退。</p><p>工作被接收之後呢？當評審有偏見時，讀者不會有偏見嗎？更重要的問題是，讀者重要還是評審人重要？我們寫論文是為了發表還是為了供人閱讀？我毫不猶豫地選擇後者。雙盲同行評審至多可以幫助論文被接收，但對於論文發表後的評價毫無用處。但看起來我們認為這個遊戲的終極目標就是在頂會發表論文，我們是不是把影響因子看得太重，或者我們真的關心能否產出有影響力的研究嗎？</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-29e9a8c6af17bd0dc9ea54d4cfc94aed_r.jpg" data-caption="" data-size=" normal" width="676" referrerpolicy="no-referrer"></figure><p> 《舊金山科研評估宣言》認為：「委員會對資金、招聘、終身教職或晉升進行決策時，需要基於科學內容而非出版指標進行評估。」僅關注如何讓論文被接收就會失去我們真正想珍視的東西。</p><p>沒有免費的午餐。雙盲同行評審是有代價的。</p><p>Rebecca M. Blank 在《The Effects of Double-Blind versus Single-Blind Reviewing: E​​xperimental Evidence from The American Economic Review》中表示，來自學術界以外的作者在雙盲評審階段的論文接收率較低，在評審知道作者身份時，他們傾向於給非學術界人士一個機會，儘管非學術界人士不像學術界人士那樣遵循領域規範。此外，Blank 表示雙盲評審整體上變得更加嚴苛了。</p><p>這一「嚴苛」本質得到了複製和量化，雙盲評審中論文的成功率要比單盲評審低。</p><p>因此，雙盲評審會有一些預想不到的後果。更嚴苛的評審和更低的論文接收率可能並非積極信號。學生可能會想：「當你可以離開學界做一些能夠受到賞識的事的時候，為什麼還要繼續追求論文通過呢？」</p><p>例如，如果我們想要提升女性的代表性，存在一些不那麼侵略性也更積極的方法，如在同行評審過程中納入更多女性作為評審人、編輯等。對於其他偏見也是如此。例如，你應該確保來自非著名學校或貧窮、非英語國家的人得到足夠的代表。那麼如何對待想法不那麼正統的人呢？如何納入更多學術界以外的人呢？</p><p>還存在另一個問題：我們需要奇低的接收率嗎？</p> <p>在計算機科學中，論文接收率低於15% 是很常見的事。我們是否意識到這一結果不可避免地是由被選出的少數人所控制的權利層級，由他們來選擇贏家。而通過接收更多論文，我們可以降低同行評審中偏見的影響力，可以削弱少數人的權力。開源期刊（如PLOS One）證明，你可以將同行評審從「選拔優秀者」轉換成「篩選掉糟糕的研究」。這就相當於你可以在房間數量有限的酒店舉辦會議，但Zoom 和YouTube 有數百萬個房間。當然，這樣做的壞處在於招聘和晉升委員會無法僅憑頂會和期刊的發表論文數量來做決定了，他們必須閱讀這些論文並進行討論。這是艱苦的工作。候選人不再只需要提供論文清單，他們還必須用我們能理解的方式解釋其工作的重要性。</p><p>我的結論是，儘管雙盲評審試圖解決的問題是切實且重要的，但它本身是一個相當粗魯、悲觀的解決方案，會造成一些意料之外的後果。而我們可以做得更好。</p><p>原文鏈接：</p><p><a href="https://lemire.me/blog/2020/11/19/double-blind-peer-review -is-a-bad-idea/?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A+daniel-lemire%2Fatom+%28Daniel+Lemire%27s+blog%29" target="_blank"> ;<span class="invisible">https://</span>< span class="visible">lemire.me/blog/2020/11/</span><span class="invisible">19/double-blind-peer-review-is-a-bad-idea /?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaign=Feed%3A+daniel-lemire%2Fatom+%28Daniel+Lemire%27s+blog%29</span><span class="ellipsis">< ;/span></a></p><p></p></div></description><author>機器之心</author><guid>https://zhuanlan .zhihu.com/p/313722528</guid><pubDate>Thu, 26 Nov 2020 14:07:02 GMT</pubDate></item><item><title>Python3.10第二個alpha版本來了！最新特性值得關注</title><link>https://zhuanlan.zhihu.com/p/313717169</link><description><div>< blockquote>Python3.10 的第二個alpha 版本已在11 月初發布，相比於不久前發布的3.9 版本，新版本對類型註釋擴展、zip、位計數、字典映射又有了新的改進。</blockquote><p><b>選自towardsdatascience，作者：James Briggs，機器之心編譯，編輯：陳萍。</b></p><p>Python3.9 剛剛發布不久，Python3.10 的第二個alpha 版本也已於11 月初發布。透過這個版本，我們或許可以一窺Python 的未來改變。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-6485e83506aa6fa016f27e9e3b962a98_r.jpg" data-caption="" data-size=" normal" width="1000" referrerpolicy="no-referrer"></figure><p>Python3.10 第二個alpha 版本的新功能包括以下三大部分：</p><ul> ;<li>類型註釋擴展</li><li>為什麼類型註釋很重要</li><li> 新方法和行為</li></ul><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-bc68a09aabef117529f8d7cdab626d9c_r.jpg" data- caption="" data-size="normal" width="1080" referrerpolicy="no-referrer"></figure><p><b>類型註釋擴展</b></p> ;<p>Python3.9 版本對類型提示與註釋進行了徹底的修改和清理。Python3.10 版本似乎延續了這一趨勢，Python3.10 alpha 2 版本將類型註釋功能進行了擴展。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-1caf6b351eace329591197b6d70eb85d_r.jpg" data-size="normal" width="1000 " referrerpolicy="no-referrer"><figcaption>從Python 3.0 到Python 3.10 類型註釋的變化。< /figcaption></figure><p><b>類型註釋的延遲評估</b></p><p>類型註釋的評估始終在函數定義時執行，這意味著類型註釋以自上而下的方式逐行進行評估。這看似合乎邏輯，但存在兩個問題：</p><p>引用尚未定義的類型（前向引用）的類型提示無效，必須以字符串形式表示。例如應該是「“int”」而不是「int」（儘管這僅適用於自定義類型，而不是內置/ 預定義類型）。</p><p>由於需要執行類型提示，模塊導入的速度減慢。</p><p>因此，註釋將被存儲在__annotations__，然後進行集中評估，即允許前向引用並首先執行模塊導入（以減少初始化時間）。</p><p><b>Union 操作符類型</b></p><p>Python 3.10 引入了| 操作符。在註釋數據類型時，可以使用| 作為OR。例如，存在一個預計為int 或float 的變量，我們可以將其寫作int | float：</p><div class="highlight"><pre><code class="language-text" >def f(x: *int | float*) -&gt; float: gt;</figure><p><b>類型註釋的延遲評估</b></p><p>類型註釋的評估始終在函數定義時執行，這意味著類型註釋以自上而下的方式逐行進行評估。這看似合乎邏輯，但存在兩個問題：</p><p>引用尚未定義的類型（前向引用）的類型提示無效，必須以字符串形式表示。例如應該是「“int”」而不是「int」（儘管這僅適用於自定義類型，而不是內置/ 預定義類型）。</p><p>由於需要執行類型提示，模塊導入的速度減慢。</p><p>因此，註釋將被存儲在__annotations__，然後進行集中評估，即允許前向引用並首先執行模塊導入（以減少初始化時間）。</p><p><b>Union 操作符類型</b></p><p>Python 3.10 引入了| 操作符。在註釋數據類型時，可以使用| 作為OR。例如，存在一個預計為int 或float 的變量，我們可以將其寫作int | float：</p><div class="highlight"><pre><code class="language-text" >def f(x: *int | float*) -&gt; float: gt;</figure><p><b>類型註釋的延遲評估</b></p><p>類型註釋的評估始終在函數定義時執行，這意味著類型註釋以自上而下的方式逐行進行評估。這看似合乎邏輯，但存在兩個問題：</p><p>引用尚未定義的類型（前向引用）的類型提示無效，必須以字符串形式表示。例如應該是「“int”」而不是「int」（儘管這僅適用於自定義類型，而不是內置/ 預定義類型）。</p><p>由於需要執行類型提示，模塊導入的速度減慢。</p><p>因此，註釋將被存儲在__annotations__，然後進行集中評估，即允許前向引用並首先執行模塊導入（以減少初始化時間）。</p><p><b>Union 操作符類型</b></p><p>Python 3.10 引入了| 操作符。在註釋數據類型時，可以使用| 作為OR。例如，存在一個預計為int 或float 的變量，我們可以將其寫作int | float：</p><div class="highlight"><pre><code class="language-text" >def f(x: *int | float*) -&gt; float: /figure><p><b>類型註釋的延遲評估</b></p><p>類型註釋的評估始終在函數定義時執行，這意味著類型註釋以自上而下的方式逐行進行評估。這看似合乎邏輯，但存在兩個問題：</p><p>引用尚未定義的類型（前向引用）的類型提示無效，必須以字符串形式表示。例如應該是「“int”」而不是「int」（儘管這僅適用於自定義類型，而不是內置/ 預定義類型）。</p><p>由於需要執行類型提示，模塊導入的速度減慢。</p><p>因此，註釋將被存儲在__annotations__，然後進行集中評估，即允許前向引用並首先執行模塊導入（以減少初始化時間）。</p><p><b>Union 操作符類型</b></p><p>Python 3.10 引入了| 操作符。在註釋數據類型時，可以使用| 作為OR。例如，存在一個預計為int 或float 的變量，我們可以將其寫作int | float：</p><div class="highlight"><pre><code class="language-text" >def f(x: *int | float*) -&gt; float: /figure><p><b>類型註釋的延遲評估</b></p><p>類型註釋的評估始終在函數定義時執行，這意味著類型註釋以自上而下的方式逐行進行評估。這看似合乎邏輯，但存在兩個問題：</p><p>引用尚未定義的類型（前向引用）的類型提示無效，必須以字符串形式表示。例如應該是「“int”」而不是「int」（儘管這僅適用於自定義類型，而不是內置/ 預定義類型）。</p><p>由於需要執行類型提示，模塊導入的速度減慢。</p><p>因此，註釋將被存儲在__annotations__，然後進行集中評估，即允許前向引用並首先執行模塊導入（以減少初始化時間）。</p><p><b>Union 操作符類型</b></p><p>Python 3.10 引入了| 操作符。在註釋數據類型時，可以使用| 作為OR。例如，存在一個預計為int 或float 的變量，我們可以將其寫作int | float：</p><div class="highlight"><pre><code class="language-text" >def f(x: *int | float*) -&gt; float: b>類型註釋的延遲評估</b></p><p>類型註釋的評估始終在函數定義時執行，這意味著類型註釋以自上而下的方式逐行進行評估。這看似合乎邏輯，但存在兩個問題：</p><p>引用尚未定義的類型（前向引用）的類型提示無效，必須以字符串形式表示。例如應該是「“int”」而不是「int」（儘管這僅適用於自定義類型，而不是內置/ 預定義類型）。</p><p>由於需要執行類型提示，模塊導入的速度減慢。</p><p>因此，註釋將被存儲在__annotations__，然後進行集中評估，即允許前向引用並首先執行模塊導入（以減少初始化時間）。</p><p><b>Union 操作符類型</b></p><p>Python 3.10 引入了| 操作符。在註釋數據類型時，可以使用| 作為OR。例如，存在一個預計為int 或float 的變量，我們可以將其寫作int | float：</p><div class="highlight"><pre><code class="language-text" >def f(x: *int | float*) -&gt; float: b>類型註釋的延遲評估</b></p><p>類型註釋的評估始終在函數定義時執行，這意味著類型註釋以自上而下的方式逐行進行評估。這看似合乎邏輯，但存在兩個問題：</p><p>引用尚未定義的類型（前向引用）的類型提示無效，必須以字符串形式表示。例如應該是「“int”」而不是「int」（儘管這僅適用於自定義類型，而不是內置/ 預定義類型）。</p><p>由於需要執行類型提示，模塊導入的速度減慢。</p><p>因此，註釋將被存儲在__annotations__，然後進行集中評估，即允許前向引用並首先執行模塊導入（以減少初始化時間）。</p><p><b>Union 操作符類型</b></p><p>Python 3.10 引入了| 操作符。在註釋數據類型時，可以使用| 作為OR。例如，存在一個預計為int 或float 的變量，我們可以將其寫作int | float：</p><div class="highlight"><pre><code class="language-text" >def f(x: *int | float*) -&gt; float: 類型註釋的評估始終在函數定義時執行，這意味著類型註釋以自上而下的方式逐行進行評估。這看似合乎邏輯，但存在兩個問題：</p><p>引用尚未定義的類型（前向引用）的類型提示無效，必須以字符串形式表示。例如應該是「“int”」而不是「int」（儘管這僅適用於自定義類型，而不是內置/ 預定義類型）。</p><p>由於需要執行類型提示，模塊導入的速度減慢。</p><p>因此，註釋將被存儲在__annotations__，然後進行集中評估，即允許前向引用並首先執行模塊導入（以減少初始化時間）。</p><p><b>Union 操作符類型</b></p><p>Python 3.10 引入了| 操作符。在註釋數據類型時，可以使用| 作為OR。例如，存在一個預計為int 或float 的變量，我們可以將其寫作int | float：</p><div class="highlight"><pre><code class="language-text" >def f(x: *int | float*) -&gt; float: 類型註釋的評估始終在函數定義時執行，這意味著類型註釋以自上而下的方式逐行進行評估。這看似合乎邏輯，但存在兩個問題：</p><p>引用尚未定義的類型（前向引用）的類型提示無效，必須以字符串形式表示。例如應該是「“int”」而不是「int」（儘管這僅適用於自定義類型，而不是內置/ 預定義類型）。</p><p>由於需要執行類型提示，模塊導入的速度減慢。</p><p>因此，註釋將被存儲在__annotations__，然後進行集中評估，即允許前向引用並首先執行模塊導入（以減少初始化時間）。</p><p><b>Union 操作符類型</b></p><p>Python 3.10 引入了| 操作符。在註釋數據類型時，可以使用| 作為OR。例如，存在一個預計為int 或float 的變量，我們可以將其寫作int | float：</p><div class="highlight"><pre><code class="language-text" >def f(x: *int | float*) -&gt; float: 引用尚未定義的類型（前向引用）的類型提示無效，必須以字符串形式表示。例如應該是「“int”」而不是「int」（儘管這僅適用於自定義類型，而不是內置/ 預定義類型）。</p><p>由於需要執行類型提示，模塊導入的速度減慢。</p><p>因此，註釋將被存儲在__annotations__，然後進行集中評估，即允許前向引用並首先執行模塊導入（以減少初始化時間）。</p><p><b>Union 操作符類型</b></p><p>Python 3.10 引入了| 操作符。在註釋數據類型時，可以使用| 作為OR。例如，存在一個預計為int 或float 的變量，我們可以將其寫作int | float：</p><div class="highlight"><pre><code class="language-text" >def f(x: *int | float*) -&gt; float: 引用尚未定義的類型（前向引用）的類型提示無效，必須以字符串形式表示。例如應該是「“int”」而不是「int」（儘管這僅適用於自定義類型，而不是內置/ 預定義類型）。</p><p>由於需要執行類型提示，模塊導入的速度減慢。</p><p>因此，註釋將被存儲在__annotations__，然後進行集中評估，即允許前向引用並首先執行模塊導入（以減少初始化時間）。</p><p><b>Union 操作符類型</b></p><p>Python 3.10 引入了| 操作符。在註釋數據類型時，可以使用| 作為OR。例如，存在一個預計為int 或float 的變量，我們可以將其寫作int | float：</p><div class="highlight"><pre><code class="language-text" >def f(x: *int | float*) -&gt; float: gt;Union 操作符類型</b></p><p>Python 3.10 引入了| 操作符。在註釋數據類型時，可以使用| 作為OR。例如，存在一個預計為int 或float 的變量，我們可以將其寫作int | float：</p><div class="highlight"><pre><code class="language-text" >def f(x: *int | float*) -&gt; float: gt;Union 操作符類型</b></p><p>Python 3.10 引入了| 操作符。在註釋數據類型時，可以使用| 作為OR。例如，存在一個預計為int 或float 的變量，我們可以將其寫作int | float：</p><div class="highlight"><pre><code class="language-text" >def f(x: *int | float*) -&gt; float:
    return x * 3.142
    
f(1) # pass
f(1.5) # pass
f('str') # linter will show annotation error</code></pre></div><p>在3.10 之前的版本中，等效運算符使用type.Union 方法進行編寫，例如Union[int, float]。</p><p><b>TypeAlias 註釋</b></p><p>回到前向引用問題，避免前向引用的常見解決方案是將它們作為字符串寫入。</p><p>但是，將類型作為字符串編寫，會在將這些類型分配給變量時出現問題，因為Python 假設字符串文本類型註釋只是一個字符串。</p><p>在使用類型註釋的地方使用該類型註釋變量將返回錯誤。例如：</p><div class="highlight"><pre><code class="language-text">MyType = "ClassName" # ClassName is our type annotation
def foo() -&gt; MyType:
 ...</code></pre></div><p>我們正在嘗試使用MyType 作為類型的別名（alias），但是MyType 將被讀取為字符串值，而不是類型別名。</p><p>只要在後面的代碼中定義了ClassName，這就是有效的。目前，這將引發註釋錯誤。</p><p>為了解決這個問題，該版本添加了一個顯式地將MyType 識別為類型別名的方法：</p><div class="highlight"><pre> <code class="language-text">from typing_extensions import TypeAlias
MyType: TypeAlias = "ClassName"
def foo() -&gt; MyType:
    ...
OR
MyType: TypeAlias = ClassName # if we have defined ClassName already
def foo() -&gt; MyType:
    ...</code></pre></div><p><b>為什麼類型註釋很重要</b></p><p>Python 的強大之處在於它易於使用和掌握，原因之一就是我們不需要在整個代碼中顯式地定義類型。</p><p>這看似違背常理，但允許開發人員定義類型可以極大地增強代碼庫的可讀性和可維護性。例如從transformers 庫的源代碼中提取如下內容：</p><div class="highlight"><pre><code class="language-text">def get_default_model(targeted_task: Dict, framework: Optional[str], task_options: Optional[Any]) -&gt; str:
    ...

class DefaultArgumentHandler(ArgumentHandler):
    ...
    @staticmethod
    def handle_kwargs(kwargs: Dict) -&gt; List:
        ...

    @staticmethod
    def handle_args(args: Sequence[Any]) -&gt; List[str]:</code></pre></div><p>即使沒有上下文，我們也可以讀取這些代碼，並了解應該向這些函數、類和方法提供哪些數據，以及應該返回哪些數據類型。</p><p>在復雜的代碼庫（甚至是簡單的代碼庫）中，類型註釋可以極大地提高可讀性。同時，並不是每個開發者都想（或需要）使用類型註釋，因此可選的、無異常的功能可以達到完美的平衡。</p><p><b>新方法和實現</b></p><p>除了類型註釋方面的更改之外，3.10 alpha 2 版本對其他核心功能也進行了一些更新。</p><p><b>為Zip 添加等長標記</b></p><p>第一個是PEP 618，它為zip() 函數添加了一個可選的strict 標記。設置strict = True，如果zip 的兩個輸入長度不等，則會引發錯誤。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-7524e17e5bc073eee90aacce5950298a_r.jpg" data-caption="" data-size="normal" width="1000" referrerpolicy="no-referrer"></figure><p><i>左側無strict=True 標記，沒有引發錯誤，並且較長的列表被截斷用於創建壓縮生成器。如果設置strict = True，就會引發錯誤。</i></p><p><b>整數的位計數</b></p><p>也叫做「總體計數」（population count）。這一新方法允許計算整數二進製表示中1 的個數，只需寫int.bit_count() 即可：</p><figure data-size="normal"><img src="https ://pic3.zhimg.com/v2-2c1d15ea6690e505738db1d7efb5e666_r.jpg" data-caption="" data-size="normal" width="700" referrerpolicy="no-referrer"></figure><p> ;<b>字典視圖映射</b></p><p>三種字典方法dict.keys()、dict. values() 和dict.items() 返回字典的不同視圖。現在，將mapping 屬性添加到每個視圖對象。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-57cf9246907fd17b63086be2c7a3850f_r.jpg" data-caption="" data-size=" normal" width="700" referrerpolicy="no-referrer"></figure><p>這一新屬性是types.MappingProxyType 對象，用來包裝原始字典。如果在視圖上調用它，則返回原始字典。</p><p>原文鏈接：</p><p><a href="https://towardsdatascience.com/new-features-in-python-3-10-66ac05e62fc7" target ="_blank"><span class="invisible">https://</span><span class="visible">towardsdatascience.com/</span><span class=" invisible" 最近，天才黑客George Hotz 開源了一個小型深度學習框架tinygrad，兼具PyTorch 和micrograd 的功能。tinygrad 的代碼數量不到1000 行，目前該項目獲得了GitHub 1400 星。</blockquote><p>機器之心報導，機器之心編輯部。</p><p>在深度學習時代，谷歌、Facebook、百度等科技巨頭開源了多款框架來幫助開發者更輕鬆地學習、構建和訓練不同類型的神經網絡。而這些大公司也花費了很大的精力來維護TensorFlow、PyTorch 這樣龐大的深度學習框架。</p><p>除了這類主流框架之外，開發者們也會開源一些小而精的框架或者庫。比如今年4 月份，特斯拉人工智能部門主管Andrej Karpathy 開源了其編寫的微型autograd 引擎micrograd，該引擎還用50 行代碼實現了一個類PyTorch api 的神經網絡庫。目前，micrograd 項目的GitHub star 量達到1200 星。</p><p>不久前，天才黑客George Hotz（喬治· 霍茲）開源了一個小型Autograd Tensor 庫tinygrad，它介於PyTorch 和micrograd 之間，能夠滿足做深度學習的大部分要求。上線不到一個月，該項目在GitHub 上已經獲得1400 星。</p><figure data-size="normal"><img src="https://pic3.zhimg. /p><p>George 在項目中保證，tinygrad 代碼量會永遠小於1000 行。</p><p><b>安裝</b></p><p>tinygrad 的安裝過程非常簡單，只需使用以下命令：</p><div class ="highlight"><pre><code class="language-text">pip3 install tinygrad --upgrade</code></pre></div><p><b>示例</b></p><p>安裝好tinygrad 之後，就可以進行示例運行，代碼如下：</p><div class="highlight"><pre>< code class="language-text">from tinygrad.tensor import Tensor
x = Tensor.eye(3)
y = Tensor([[2.0,0,-2.0]])
z = y.matmul(x).sum()
z.backward()
print(x.grad) # dz/dx
print(y.grad) # dz/dy</code></pre></div><p>使用torch 的代碼如下：</p><div class="highlight">< ;pre><code class="language-text">import torch
x = torch.eye(3, requires_grad=True)
y = torch.tensor([[2.0,0,-2.0]], requires_grad=True)
z = y.matmul(x).sum()
z.backward()
print(x.grad) # dz/dx
print(y.grad) # dz/dy</code></pre></div><p><b>滿足對神經網絡的需求</b></p>< p>一個不錯的autograd 張量庫可以滿足你對神經網絡90％的需求。從tinygrad.optim 添加優化器（SGD、RMSprop、Adam），再編寫一些minibatching 樣板代碼，就可以實現你的需求。</p><p>示例如下：</p><div class="highlight"><pre><code class="language-text">from tinygrad.tensor import Tensor
import tinygrad.optim as optim
from tinygrad.utils import layer_init_uniform

class TinyBobNet:
  def __init__(self):
    self.l1 = Tensor(layer_init_uniform(784, 128))
    self.l2 = Tensor(layer_init_uniform(128, 10))

  def forward(self, x):
    return x.dot(self.l1).relu().dot(self.l2).logsoftmax()

model = TinyBobNet()
optim = optim.SGD([model.l1, model.l2], lr=0.001)

# ... and complete like pytorch, with (x,y) data

out = model.forward(x)
loss = out.mul(y).mean()
loss.backward()
optim.step()</code></pre></div><p><b>支持GPU</b></p><p>tinygrad 通過PyOpenCL 支持GPU。但後向傳播暫時無法支持所有ops。</p><div class="highlight"><pre><code class="language-text">from tinygrad.tensor import Tensor
(Tensor.ones(4,4).cuda() + Tensor.ones(4,4).cuda()).cpu()</code></pre></div><p> <b>ImageNet 推斷</b></p><p>「麻雀雖小，五臟俱全。」tinygrad 還能夠支持full EfficientNet，輸入一張圖像，即可得到其類別。</p><div class="highlight"><pre><code class="language-text">ipython3 examples/efficientnet.py https://upload.wikimedia.org/wikipedia/commons /4/41/Chicken.jpg</code></pre></div><p>如果你安裝了webcam 和cv2，則可以使用以下代碼：</p><div class= "highlight"><pre><code class="language-text">ipython3 examples/efficientnet.py webcam </code></pre></div> <p>注意：如果你想加速運行，設置GPU=1。</p><p><b>測試</b></p><p>運行以下代碼可執行測試：</p><div class="highlight"> <pre><code class="language-text">python -m pytest</code></pre></div><p>此外，喬治· 霍茲還計劃添加語言模型、檢測模型，進一步減少代碼量、提升速度等。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-5b15ab1be53bd4a9c551d56728c37f35_r.jpg" data-caption="" data-size=" normal" width="1068" referrerpolicy="no-referrer"></figure><p><b>天才黑客喬治· 霍茲</b></p><figure data- size="normal" ><img src="https://pic1.zhimg.com/v2-937d8ec23e4f8cc89be5b157328c58c0_r.jpg" data-caption="" data-size="normal" width="800" referrerpolicy="no-referrer"> ;</figure><p>該項目的創建者是著名黑客喬治· 霍茲，別號Geohot。</p><p>他於1989 年出生在美國新澤西州，曾就讀於羅切斯特理工學院生物工程專業和卡內基梅隆大學計算機科學系。</p><p>然而，喬治· 霍茲在自己的LinkedIn 主頁上教育經歷描述裡是這麼寫的：</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-66bf418f3034ba392e211b259089a0f3_r.jpg" data-caption="" data-size="normal" width="1080" referrerpolicy="no-referrer"></figure> ;<p>在羅切斯特理工學院就讀期間，他的社團活動是「在宿舍黑iPhone」…… </p><p> 這是他著名的黑客經歷之一。2007 年，17 歲的喬治· 霍茲成功破解iPhone 手機，使手機不再局限於AT&amp;T 網絡，而是支持其他GSM 網絡。2009 年，他開發出一款針對iOS 3.1.2 的越獄軟件blackra1n。2010 年，喬治· 霍茲宣布不再進行越獄軟件的開發。</p><p>然而，他的黑客行動並未停止。</p><p>2009 年起，喬治· 霍茲開始破解PlayStation 3（PS3）。2010 年初，他宣布得到了PS3 系統內存的讀寫權限和處理器的高級控制權。2011 年3 月，喬治· 霍茲被索尼起訴，後和解。</p><p>此外，喬治· 霍茲還破解過三星手機等產品。</p><p>就工作經歷而言，他曾在谷歌、Facebook、SpaceX 工作過。目前，他在自己創立的自動駕駛公司comma.ai 任職。</p><p>2015 年，喬治· 霍茲創立了人工智能創業公司comma.ai，旨在基於機器學習算法構建自動駕駛技術。喬治· 霍茲開發出自動駕駛套件Comma One，只需1000 美元，用戶就能將傳統汽車升級成自動駕駛版本。不過，後來這一計劃被取消。</p><p>2020 年，在CES 大會上，comma.ai 展出了其最新產品——輔助駕駛設備Comma Two，售價999 美元。</p><p>comma.ai 公司還開源了輔助駕駛系統openpilot，參見：<a href="https: //zhuanlan.zhihu.com/p/311886981</link><description><p><img src="https://pic2.zhimg.com/v2-8d2c03e0699e23e6fbc4e14adaa0bd7d_b.jpg"></p> ;<div><blockquote>微分方程與機器學習作為AI 領域建模的兩種方法，各自有什麼優勢？</blockquote><p><b>選自Medium，作者：Col Jung，機器之心編譯，編輯：小舟。</b></p><p>微分方程（DE）與機器學習（ML）類數據驅動方法都足以驅動AI 領域的發展。二者有何異同呢？本文進行了對比。</p><h2><b>微分方程模型示例</b></h2><p><b>納維-斯托克斯方程（氣象學）</ b></p><p>這一模型被用於天氣預測。它是一個混沌模型，當輸入存在一點點不准確，預測結果就會大相徑庭。這就是為什麼天氣預報經常是錯誤的，天氣模擬使用超級計算機完成。</p><figure data-size="normal"><img src="https://pic1. img src="https://pic4.zhimg.com/v2-df3dfd80f98175355f231ec376809883_r.jpg" data-caption="" data-size="normal" width="875" referrerpolicy="no-referrer"></ figure><p><b>SIR 模型（流行病學）</b></p><p>SIR 是基礎的房室模型，可以描述傳染病的傳播情況。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-ee2551f87910921dab8ae25be4ce2db6_r.jpg" data-caption="" data-size=" normal" width="875" referrerpolicy="no-referrer"></figure><figure data-size="normal"><img src="https://pic3.zhimg.com/v2 -1a6a4499ed60372a3317fc936a20b8e6_r.jpg" data-caption="" data-size=" com/v2-dbd9813edc02bfc7cfb770f3b4039c73_r.jpg" data-caption="" data-size="normal" width="875" referrerpolicy="no-referrer"></figure><p>請注意Murray-Gottman「愛情模型」實際上是一個差分方程（微分方程的一種姊妹模型）。差分方程輸出離散的數字序列（例如，每5 年的人口普查結果），而微分方程則建模連續數值（即持續發生的事件）。</p><p>上述5 個模型（微分和差分方程）都是機械模型，我們可以在其中自行選擇系統的邏輯、規則、結構或機制。當然，並不是每次試驗都會成功，反複試驗在數學建模中非常重要。</p><p>納維- 斯托克斯方程假定大氣是流動的流體，上述方程式就是來自流體動力學。廣義相對論假設在一種特殊的幾何形態下，時空會發生扭曲。愛因斯坦提出關於時空扭曲的一些重要想法，數學家Emmy Noether 和David Hilbert 將這些想法整合到愛因斯坦場方程中。SIR 模型假設病毒是通過感染者與未感染者之間的直接接觸傳播的，並且感染者會以固定的速率自動恢復。</p><p>使用機械模型時，觀察和直覺會指導模型的設計，而數據則用於後續驗證假設。</p><p> 所有這些都與經驗模型或數據驅動模型形成鮮明對比，經驗或數據驅動模型首先從數據出發。這其中就包括機器學習模型，其算法通過輸入足夠的高質量樣本來學習系統的基礎邏輯或規則。當人類很難分析或定義系統的機制時，這樣的方法是很明智的。</p><h2><b>數學模型的分類</b></h2><figure data-size="normal"><img src="https://pic3 .zhimg.com/v2-014d6633c9264b559ba1dedcf9bc1422_r.jpg" data-caption="" data-size="normal" width="875" referrerpolicy="no-referrer"></figure><p>機械模型對驅動系統的底層機制進行了假設，在物理學中很常用。實際上，數學建模是從17 世紀人們試圖解開行星運動規律時才開始發展的。</p><p>經驗或數據驅動型建模，特別是機器學習，能夠讓數據來學習系統的結構，這個過程就叫做「擬合」。機器學習對於人類不確定如何將信號從噪聲中分離出來的複雜系統格外有效，只需要訓練一種聰明的算法，讓它來代替你做繁瑣的事情。</p><p>機器學習任務廣義上可以分為：</p><ul><li>監督學習（即回歸與分類）</li>& lt;li>無監督學習（即聚類和降維）</li><li>強化學習</li></ul><figure data-size="normal">< img src="https://pic3.zhimg.com/v2-03032087ea16d885f486d28374db4306_r.jpg" data-caption="" data-size="normal" width="875" referrerpolicy="no-referrer"></ figure><p>如今機器學習和人工智能係統在日常生活中隨處可見。從亞馬遜、蘋果和谷歌的語音助手到Instagram、Netflix 和Spotify 的推薦引擎，再到Facebook 和Sony 的人臉識別技術，甚至特斯拉的自動駕駛技術，所有這些都是由嵌入在大量代碼下的數學與統計模型驅動的。</p><p>我們可以進一步將機械模型和經驗模型分為確定性模型（預測是固定的）和隨機性模型（預測包含隨機性）。</p><ul><li>確定性模型忽略隨機變化，在相同的初始條件下，總會預測出相同的結果。</li><li>隨機模型則考慮了隨機變化，如係統中單個主體的異質性，比如人、動物、細胞之間就存在細微的差別。</li></ul><p& gt;隨機性通常會在模型中引入一些現實性，但同時也存在一定的代價。在數學建模中，我們需要考慮模型的複雜性：簡單的模型易於分析，但可能缺乏預測能力；複雜的模型具有現實性，但嘗試弄清楚模型背後的原理也很重要。因此，我們需要在簡單性和可分析性之間進行權衡，正如統計學家George Box 所說：</p><blockquote>所有的模型都是錯誤的，但其中一些是有用的。</blockquote><p>在機器學習和統計學中，模型複雜度被稱為「偏差- 方差權衡」。高偏差模型過於簡單，導致欠擬合，高方差模型存儲的是噪聲而不是信號（即係統的實際結構），會導致過擬合。</p><h2><b>微分方程與機器學習示例對比</b></h2><p><b>logistic 微分方程</b></p> ;<p>該方程涉及農業、生物學、經濟學、生態學、流行病學等領域。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-ecab34fd29e0aa1b1022b710ecf119a3_r.jpg" data-caption="" data-size=" normal" width="868" referrerpolicy="no-referrer">< /figure><p>繪製dP/dt 對t 的曲線：</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2 -dca038070912a8bdbf10c386bed45b59_r.jpg" data-caption="" data-size="normal" width="875" referrerpolicy="no-referrer"></figure><figure data-size="normal">< ;img src="https://pic4.zhimg.com/v2-9ad3b5097e3e97a00cd86e1491e2b417_r.jpg" data-caption="" data-size="normal" width="875" referrerpolicy="no-referrer">< /figure><p>logistic 模型的一個例子是哈伯特峰值石油模型。1956 年，石油地質學家Marion Hubbert 為德克薩斯州的石油生產量創建了一個預測數學模型。</p><p>令P 表示德克薩斯州的產油量。</p><p> 如果右邊是rP，則石油生產量將會成倍增長。但是Hubbert 知道油量一共只有K=200 gigabarrels。隨著時間的流逝，開採石油變得越來越困難，因此生產率dP/dt 有所下降。(1-P/K) 項說明了資源有限的觀察結果。注意，在考慮實際數據之前，我們就已經推斷出石油開采的機制。</p><ul><li>代表生產率的參數r=0.079 是從50 年的數據中推斷出來的。</li><li>代表石油總量的參數K=200，這是系統的穩定狀態。</li></ul><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-d086bb81ccc068e25b947b9bace435e8_r.jpg" data-caption="" data-size="normal" width="875" referrerpolicy="no-referrer"></figure><p>機器學習模型很難學習嵌入到微分方程中的邏輯所捕獲的潛在機制。從本質上講，任何算法都需要僅基於1956 年之前存在的數據（綠色）預測能夠出現的最大值：</p><figure data-size="normal"><img src=" https://pic4.zhimg. 首先將微分方程編程到Python 或Matlab 中，在將dP/dt 繪製為t 的函數之前，使用數值求解器獲得P(t)。此處使用了Python。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-53aa845325ecaa07078e6b7de87b9ae5_r.jpg" data-caption="" data-size=" normal" width="875" referrerpolicy="no-referrer"></figure><p><b>方法2：獲取解析解</b></p><p>該系統可以使用分離變量法求得解析解。請注意：大多數微分方程無法求得解析解。對此，數學家一直在尋找求解析解的方法。以新西蘭科學家Roy Kerr 為例，他發現了愛因斯坦場方程的一組精確解，進而使人類發現了黑洞。但還好，logistic 微分方程中有一些是具有確切解的。</p><p>首先把所有含有P 的項移到等式左邊，含有t 的項移到等式右邊：</p><figure data-size="normal">< ;img src="https://pic1.zhimg.com/v2-99f3364c86d34c3602076c488e45e020_r.jpg" data-caption="" data-size="normal" width="875" referrerpolicy="no-referrer"></figure><p>將二者整合到一起可得到通解，即滿足微分方程的一組無窮多個函數。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-e2a2f61f72b587beb06e8eda06963911_r.jpg" data-caption="" data-size=" normal" width="875" referrerpolicy="no-referrer"></figure><p>微分方程總是有無窮多個解，由一系列曲線以圖像的方式給出。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-2954a8d09c6f917d34fafb0ddfb04825_r.jpg" data-caption="" data-size=" normal" width="568" referrerpolicy="no-referrer"></figure> <p>將P 重新排列，得到：</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-2816e1aba2bd05175ba613eb134ad762_r.jpg" data-caption="" data-size="normal" width="875" referrerpolicy="no-referrer"></figure><p>微分得到：</p><figure data-size ="normal"><img src="https://pic3.zhimg.com/v2-412add0a16b345f0d6a6d6c93a38f7ae_r.jpg" data-caption="" data-size="normal" width="875" referrerpolicy="no -referrer"></figure><p>這兩個公式對應上述logistic 曲線和類高斯曲線。</p><h2><b>總結</b></h2><p> 在機械建模中，對驅動系統的基本機制進行假設之前，研究者會仔細觀察並研究現象，然後用數據驗證模型，驗證假設是否正確。如果假設正確，皆大歡喜；如果錯誤，也沒關係，建模本身就是要反複試驗的，你可以選擇修改假設或者從頭開始。</p><p>在數據驅動的建模中，我們讓數據來構建系統的藍圖。人類要做的是為機器提供高質量、有代表性並且數量足夠多的數據。這就是機器學習。在人類難以觀察到現象本質時，機器學習算法可以從噪聲中提取信號。神經網絡和強化學習是當下熱門的研究領域，它們能夠創建具有驚人復雜性的模型。而AI 革命尚在繼續。</p></div></description><author>機器之心</author><guid>https://zhuanlan.zhihu.com/p/311886981</guid><pubDate>Thu, 26 Nov 2020 00:20:13 GMT</pubDate></item><item><title>IEEE Fellow、AAAS Fellow 同日公佈，清華唐杰、京東鄭宇等數十位華人入選</title><link> https://zhuanlan.zhihu.com/p/311876834</link><description><div><blockquote> 今日，IEEE 2021 Fellow 和AAAS 2020 Fellow 同時公佈入選名單。據統計，入選IEEE 2021 Fellow 的華人學者近80 人，其中包括清華大學教授唐杰、京東集團副總裁鄭宇等。此外，入選IEEE 2020 Fellow 的百度商業智能實驗室主任熊輝教授當選了AAAS 2020 Fellow。</blockquote><p><b>機器之心報導，機器之心編輯部。</b></p><p>IEEE 全稱是美國電子電氣工程師學會（Institute of Electrical and Electronic Engineers），是國際性電子技術與信息科學工程師學會，在160 多個國家擁有超過40萬會員。IEEE Fellow 為學會最高等級會員，是IEEE 授予成員的最高榮譽，在學術科技界被認定為權威的榮譽和重要的職業成就。當選人需要對工程科學與技術的進步或應用做出重大貢獻，為社會帶來重大價值。當選人數不超過IEEE 當年會員總數的0.1%。</p><p>去年，機器之心對<a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650774969&amp; idx=1&amp;sn=d730b4c46182ef4c89b17d44b4a6bfe3&amp;chksm=871a59c7b06dd0d1d72597bce3224da41e46c100b9f2a36869df51e9242ba881ed6750cdbbc4&amp; 入選理由：因對數據和社交網絡挖掘的知識發現的貢獻而入選</p><p class="ztext-empty-paragraph"><br></p><p>姓名：Xiaorui Wang，俄亥俄州立大學</p><p>入選理由：因對數據中心服務器和嵌入式系統電源管理的貢獻而入選</p><p class="ztext-empty-paragraph "><br></p><p>姓名：Jingyi Yu 虞晶怡，上海科技大學</p><p>入選理由：因對計算機攝影學和計算機視覺的理論分析、算法和系統方面的貢獻而入選</p><p class="ztext-empty-paragraph"><br></p><p>姓名：Lintao Zhang 張霖濤，微軟亞研< /p><p>入選理由：因使用有效的布爾可滿足性方法做計算輔助驗證方面的貢獻而入選。</p><p class="ztext-empty-paragraph"><br></p><p>姓名：Yu Zheng 鄭宇，京東</p>&br></p><p><b>AAAS 2020 Fellow</b></p><figure data-size="normal"><img src="https://pic3 .zhimg.com/v2-e485b335ff9e091fbb01bd028d238d66_r.jpg" data-caption="" data-size="normal" width="1080" referrerpolicy="no-referrer"></figure><p>AAAS （美國科學促進會）近500 名成員獲得了AAAS Fellow 終身榮譽。</p><p>AAAS Fellow 每年由任職於AAAS 理事會的同行選舉產生，AAAS 是該組織的成員管理機構。這個榮譽是為了表彰對STEM 學科的重要貢獻，包括開創性研究、在特定領域的領導、教學和指導、促進合作以及增進公眾對科學的理解。</p><p>2021 年2 月13 日將為489 名新當選的Fellows 舉行在線入職典禮，獲獎者將通過郵寄獲得官方證書和象徵科學與工程的金色和藍色玫瑰別針。</p><p> AAAS Fellow 按照具體學科進行評選，包括農業、糧食與可再生資源、人類學、天文學、大氣與水圈科學、生物科學、化學、教育、工程、地質與地理學、歷史與哲學科學、產業科技、信息計算與通信、語言學與語言科學、醫藥科學、神經科學、藥物科學、物理學、心理學、社會經濟與政治科學、統計學等學科。</p><p>機器之心這裡只統計了<b>信息計算與通信、數學、物理學</b>這三個學科的入選AAAS 2020 Fellow，其中華人學者有以下幾位（如有遺漏請指正）：</p><p>Hui Xiong（熊輝）：羅格斯大學</p><p>入選理由：因對數據挖掘和移動計算領域的貢獻而入選。</p><p class="ztext-empty-paragraph"><br></p><p>Jack Xin：加州大學爾灣分校</p><p>入選理由：因對應用數學和數據科學的貢獻而入選</p><p class="ztext-empty-paragraph"><br></p><p>Ying-Cheng Lai（賴英成）：亞利桑那州立大學</p><p>入選理由：因對非線性動力學和混沌理論，尤其是相對論量子混沌和暫態混沌領域的貢獻而入選</p>< p class="ztext-empty-paragraph"><br></p><p>Yuhai Tu：IBM 托馬斯·J· 沃森研究中心</p><p>入選理由：因對生物物理學，尤其是使用統計物理學方法來研究生命系統中信息處理的動力學和能量學的貢獻而入選。</p><p>參考鏈接：<a href="https://www.aaas.org/news/aaas-announces-leading-scientists-elected-2020-fellows" target="_blank"> ;<span class="invisible">https://www.</span><span class="visible">aaas.org/news/aaas-anno</span><span class= "invisible">unces-leading-scientists-elected-2020-fellows</span><span class="ellipsis"></span></a></p></div> </description><author>機器之心</author> <guid>https://zhuanlan.zhihu.com/p/311876834</guid><pubDate>Wed, 25 Nov 2020 14:14:03 GMT</pubDate></item><item><title>突破AI和機器理解的界限，牛津CS博士143頁畢業論文學習重建和分割3D物體</title><link>https://zhuanlan.zhihu.com/p/311883082</link><description><div> <blockquote>讓機器擁有像人類一樣感知3D 物體和環境的能力，是人工智能領域的一項重要課題。牛津大學計算機科學系博士生Bo Yang 在其畢業論文中詳細解讀瞭如何重建和分割3D 物體，進而賦予機器感知3D 環境的能力，突破了人工智能和機器理解的界限。</blockquote><p>選自arXiv，作者：Bo Yang，機器之心編譯。</p><p>賦予機器像人類一樣感知三維真實世界的能力，這是人工智能領域的一個根本且長期存在的主題。考慮到視覺輸入具有不同類型，如二維或三維傳感器獲取的圖像或點雲，該領域研究中一個重要的目標是理解三維環境的幾何結構和語義。</p><p>傳統方法通常利用手工構建的特徵來估計物體或場景的形狀和語義。但是，這些方法難以泛化至新物體和新場景，也很難克服視覺遮擋的關鍵問題。</p><p> 今年九月畢業於牛津大學計算機科學系的博士生Bo Yang 在其畢業論文《Learning to Reconstruct and Segment 3D Objects》中對這一主題展開了研究。與傳統方法不同，作者通過在大規模真實世界的三維數據上訓練的深度神經網絡來學習通用和魯棒表示，進而理解場景以及場景中的物體。</p><p>總體而言，本文開發了一系列新型數據驅動算法，以實現機器感知到真實世界三維環境的目的。作者表示：「本文可以說是突破了人工智能和機器理解的界限。」</p><p>這篇博士論文有143 頁，共六章。機器之心對該論文的核心內容進行了簡要介紹，感興趣的讀者可以閱讀論文原文。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-a174509f02daeaf2e8a340deb6b88ddf_r.jpg" data-caption="" data-size=" normal" width="853" referrerpolicy="no-referrer"></figure><p>論文地址：<a href="https://arxiv.org/pdf/2010.09582.pdf" target= "_blank"><span class="invisible">https://</span> <span class="visible">arxiv.org/pdf/2010.0958</span><span class="invisible">2.pdf</span><span class="ellipsis">< ;/span></a></p><p><b>論文概述</b></p><p>作者在第2 章首先回顧了以往3D 物體重建和分割方面的研究工作，包括單視圖和多視圖3D 物體重建、3D 點雲分割、對抗生成網絡（GAN）、注意力機制以及集合上的深度學習。此外，本章最後還介紹了在單視圖/ 多視圖3D 重建和3D 點雲分割方面，該研究相較於SOTA 方法的新穎之處。</p><p><b>基於單視圖的3D 物體重建</b></p><p>在第3 章，作者提出以一種基於GAN 的深度神經架構來從單一的深度視圖學習物體的密集3D 形狀。作者將這種簡單但有效的模型稱為3D-RecGAN++，它將殘差連接（skip-connected）的3D 編碼器- 解碼器和對抗學習結合，以生成單一2.5D 視圖條件下的完整細粒度3D結構。該模型網絡架構的訓練和測試流程如下圖所示：</p><figure data-size="normal" ><img src="https://pic4.zhimg.com/v2-9f3358702e5f449e10​​2375d89be40fef_r.jpg" data-caption="" data-size="normal" width="1080" referrerpolicy="no-referrer"> ;</figure><p>接著，作者利用條件對抗訓練來細化編碼器- 解碼器估計的3D 形狀，其中用於3D 形狀細化的判別器結構示意圖如下：</p>< ;figure data-size="normal"><img src="https://pic3.zhimg.com/v2-9721705e6c266cf67ca94fa00371feb6_r.jpg" data-caption="" data-size="normal" width="1080 " referrerpolicy="no-referrer"></figure><p>最後，作者將提出的3D-RecGAN++ 與SOTA 方法做了對比，並進行了控制變量研究。在合成和真實數據集上的大量實驗結果表明，該模型性能良好。</p><p><b>基於多視圖的3D 物體重建</b></p><p> 在第4 章，作者提出以一種新的基於注意力機制的神經模塊來從多視圖中推理出更好的3D 物體形狀。這種簡單但高效的注意力聚合模塊被稱為AttSets，其結構如下圖所示。與現有方法相比，這種方法可以學習從不同圖像中聚合有用信息。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-766c2ad3f954583f7b4cc1801c431079_r.jpg" data-caption="" data-size=" normal" width="1080" referrerpolicy="no-referrer"></figure><p>此外，研究者還引入了兩階段訓練算法，以確保在給出一定數量輸入圖像的情況下，預估的3D 形狀具有魯棒性。研究者在多個數據集上進行了實驗，證明該方法能夠精確地恢復物體的3D 形狀。</p><p><b>從點雲中學習分割3D 物體</b></p><p>在第五章中，研究者提出了一個新的框架來識別大規模3D 場景中的所有單個3D 物體。與現有的研究相比，該研究的框架能夠直接並且同時進行檢測、分割和識別所有的目標實例，而無需任何繁瑣的前/ 後處理步驟。研究者在多個大型實際數據集上展現了該方法相對於基線的性能提升。</p><p><b>作者介紹</b> </p><p>本文作者Bo Yang 現為香港理工大學計算機系助理教授。他本科和碩士分別畢業於北京郵電大學和香港大學，然後進入牛津大學計算機科學系攻讀博士學位，其導師為Niki Trigoni 和Andrew Markham 教授。</p><p>Bo Yang 作為一作以及合著的論文曾被《計算機視覺國際期刊》（IJCV）以及NeurIPS 和CVPR 等學術會議接收，谷歌學術主頁上顯示他共著有22 篇論文，被引用數超過400。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-19a7b3379b7db835a81e5fe6db37c06f_r.jpg" data-caption="" data-size=" normal" width="1080" referrerpolicy="no-referrer"></figure><p>論文目錄如下：</p><figure data-size="normal"><img src ="https://pic2.zhimg.com/v2-a553ca88dccdcf48ab751c80886b3e6d_r.jpg" data-caption="" data-size="normal" width="1080" 46:50 GMT</pubDate></item><item><title>瀏覽器中實現深度學習？有人分析了7個基於JS語言的DL框架，發現還有很長的路要走</title><link>https://zhuanlan.zhihu.com/p/310044954</link><description>< p><img src="https://pic3.zhimg.com/v2-bfc91fd629b2a7875099ae038fe38b7e_b.jpg"></p><div><blockquote>本文中，作者基於WWW'19 論文提供的線索，詳細解讀了在瀏覽器中實現深度學習的可能性、可行性和性能現狀。具體而言，作者重點分析了7 個最近出現的基於JavaScript 的DL 框架，並對比了具體框架支持哪些DL 任務。</blockquote><p><b>機器之心分析師網絡，作者：仵冀穎，編輯：H4O。</b></p><p> 深度學習（Deep Learning，DL）是一類利用多層非線性處理單元（稱為神經元）進行特徵提取和轉換的機器學習算法。每個連續層使用前一層的輸出作為輸入。近十年來，深度學習技術的進步極大地促進了人工智能的發展。大量的人工智能應用，如圖像處理、目標跟踪、語音識別和自然語言處理，都對採用DL 提出了迫切的要求。因此，各種DL 框架（Frameworks）和庫（Libraries），如TensorFlow、Caffe、CNTK 等，被提出並應用於實踐。目前，這些應用程序可以在諸如Windows、Linux、MacOS/iOS 和Android 等異構開發環境上運行，此外，可以使用各種命令式編程語言開發，例如Windows 上的C/C++、iOS 和MacOS 上的Objective-C，以及Android 上的Java。不過，受限於DL 框架和庫的特點，例如訓練數據量大、網絡結構複雜、網絡層級多、參數多等，通過本機程序調用運行DL 的AI 算法或模型的運算量非常大。</p><p>最近，關於DL 的一種應用趨勢是應用程序直接在客戶端中執行DL 任務，以實現更好的隱私保護和獲得及時的響應。其中，<b>在Web 瀏覽器中實現DL</b> ，成為了人工智能社區關於客戶端DL 支持的重要研究目標。瀏覽器中的DL 是用JavaScript 實現的，依靠瀏覽器引擎來執行。基於DL 的Web 應用程序可以部署在所有平台的瀏覽器中，而不管底層硬件設備類型（PC、智能手機和可穿戴設備）和操作系統（Windows、Mac、iOS 和Android）。這就使得在瀏覽器中實現DL 具有非常良好的適配性能和普適性能，不會對客戶端的選擇有諸多限制。另外，HTML5、CSS3，特別是JavaScript 語言的進步，使得支持創建DL 驅動的Web 應用程序具有良好的性能。得益於WebGL 的發展，目前主流瀏覽器如Google Chrome、Mozilla FireFox、Safari 等，都可以更好地利用顯卡來加速DL 任務。</p><p>不過，是不是我們現在就可以隨心所欲的在瀏覽器中運行DL 的模型或算法了？我們已經成功邁入在瀏覽器中實現深度學習的時代了麼？儘管上面介紹的內容似乎意味著使在瀏覽器中運行DL 任務成為可能，但是目前對於可以執行哪些DL 任務以及DL 在瀏覽器中的工作效果卻缺少深入的研究和分析。更重要的是，考慮到Web 應用程序與本機應用程序性能的長期爭論，開發基於DL 的Web 應用程序時也存在同樣的問題。在這篇文章中，我們以北京大學研究人員發表在WWW'19（The World Wide Web Conference 2019）中的文章《Moving Deep Learning intoWeb Browser: How Far CanWe Go?》[1]作為參考主線，具體分析和探討在瀏覽器中實現深度學習的問題。</p><p><b>1、瀏覽器中支持的深度學習功能</b></p>< p>在這一章節中，我們以文獻[1] 為參考主線，重點探討現有的框架提供了哪些特性來支持在瀏覽器中實現各種DL 任務。我們首先介紹了進行分析的幾個框架，然後從兩個方面比較了這些框架的特性：提供的功能和開發人員的支持。對於所提供的功能，主要檢查每個框架是否支持DL 應用程序開發中常用的一些基本功能。而對於開發人員支持，主要討論一些可能影響開發和部署DL 應用程序效率的因素。</p><p><b>1.1 選擇的框架</b></p><p>為了選擇最新的瀏覽器支持的DL 框架，作者在GitHub 上搜索關鍵字“ deep learning framework”，並用JavaScript 語言過濾結果。然後<b>選擇了GitHub 上星數超過1000 的前7 個框架[1]</b>。對每個框架的具體介紹如下：</p><ul><li>TensorFlow.js[2] ：2018 年3 月由Google 發布，是一個inbrowser 機器學習庫，支持使用JavaScript 在瀏覽器中定義、訓練和運行模型。TensorFlow.js 由WebGL 提供支持，並提供用於定義模型的高級API。TensorFlow.js 支持所有Keras 層（包括Dense、CNN、LSTM 等）。因此，很容易將原生TensorFlow 和Keras 預先訓練的模型導入到瀏覽器中並使用Tensorflow.js。</li><li> ConvNetJS[3] ：是一個Javascript 庫，最初由斯坦福大學的Andrej Karpathy 編寫。ConvNetJS 目前支持用於分類和回歸的常用神經網絡模型和代價函數。此外，它還支持卷積網絡和強化學習。然而遺憾的是，儘管ConvNetJS 可能是在TensorFlow.js 之前最著名的框架，但其在2016 年11 月後已經不再維護了。</li><li>Keras.js[4]：抽像出許多框架作為後端支撐，包括TensorFlow、CNTK 等。它支持導入Keras 預先訓練的模型進行推理。在GPU 模式下，Keras.js 的計算由WebGL 執行。然而，這個項目也已經不再活躍。</li><li>WebDNN[5]：由東京大學發布的WebDNN 號稱是瀏覽器中最快的DNN 執行框架。它只支持推理（訓練）任務。該框架支持4 個執行後端：WebGPU、WebGL、WebAssembly 和Fallback pure JavaScript 實現。WebDNN 通過壓縮模型數據來優化DNN 模型，以加快執行速度。</li><li>brain.js[6]：是一個用於神經網絡的JavaScript 庫，它取代了不推薦使用的“brain” 庫。它為訓練任務提供DNN、RNN、LSTM 和GRU。該庫支持將訓練好的DL 模型的狀態用JSON 序列化和加載。</li><li> synaptic[7]：這是一個不依賴於JavaScript 架構的神經網絡庫，基本上支持任何類型的一階甚至二階RNN。該庫還包括一些內置的DL 架構，包括多層感知器、LSTM、液態機（Liquid state machines）和Hopfield 網絡。</li><li>Mind[8]：這是一個靈活的神經網絡庫。核心框架只有247 行代碼，它使用矩陣實現來處理訓練數據。它支持自定義網絡拓撲和插件，以導入mind 社區創建的預訓練模型。然而，這個項目也已經不再活躍。</li></ul><p><b>1.2 所提供的功能</b></p><p>訓練支持</p><p>大多數框架都支持在瀏覽器中完成訓練和推理任務。然而，Keras.js 和WebDNN 不支持在瀏覽器中訓練DL 模型，它們只支持加載預訓練的模型來執行推理任務。</p><p>支持的網絡類型</p><p>有些框架並不是針對通用DL 任務的，所以它們支持的網絡類型有所不同。具體來說，TensorFlow.js、Keras.js 和WebDNN 支持三種網絡類型：DNN、CNN 和RNN。但ConvNetJS 主要支持CNN 任務，不支持RNN。brain.js 和synaptic 主要支持RNN 任務，不支持CNN 網絡中使用的捲積和池化操作。Mind 只支持基本的DNN。</p><p>支持的層類型</p> <p>所有框架都支持以層（Layer）為單位構建神經網絡。TensorFlow.js 的層API 支持49 種不同的層，包括密集層、卷積層、池化層、RNN、歸一化等。其他框架支持的層類型較少，這也與它們所支持的網絡類型有關。需要注意的是，TensorFlow.js 的核心API 是以類似於原生TensorFlow 的方式實現的，它結合了各種操作來構建計算圖。synaptic 是一個架構無關的框架，支持構建任何類型的一階甚至二階RNN 網絡。</p><p>支持的激活/ 優化類型</p><p>TensorFlow.js 為開發者提供了關於激活/ 優化器的最多種類的選擇。對於激活函數，其他框架只支持基本的sigmoid 或ReLU。對於優化器，其他框架主要支持基本的隨機梯度下降（SGD）。</p><p>支持GPU 加速</p><p>TensorFlow.js 是唯一支持GPU 加速訓練任務的框架。TensorFlow.js、Keras.js 和WebDNN 支持使用GPU 來加速推理任務。WebDNN 還支持更先進的技術—WebGPU，但WebGPU 只被Safari 的技術預覽版所支持。</p><p><b>1.3 開發者支持</b></p><ul><li><b>文檔。</b>TensorFlow. js、ConvNetJS、WebDNN 和synaptic 提供的文檔已經完成。Keras.js 的文檔並不完整，brain.js 只有幾篇教程。</li><li><b>演示。</b>所有的框架都為開發者提供了入門的demo。TensorFlow.js 提供了最豐富的demo，涵蓋了廣泛的用例。</li><li><b>從其他框架導入模型。</b>TensorFlow.js、Keras.js 和WebDNN 支持在Python 中從原生DL 框架中導入模型，並且它們都提供了用於轉換模型的Python 腳本。TensorFlow.js 支持由TensorFlow 和Keras 訓練的模型。Keras.js 支持Keras 模型。WebDNN 支持從TensorFlow、Keras、Caffe 和Pytorch 導入模型。在支持使用其他DL 框架的預訓練模型的情況下，可以大大減少開發工作量。</li><li><b>API 保存/ 加載模型。</b>所有支持瀏覽器中訓練任務的框架都有保存模型的API。所有框架都有用於加載模型的API。</li><li><b>支持服務器端（Node.js）。</b>所有框架都支持Node.js。這樣的功能使得瀏覽器內部的計算可以卸載到遠程服務器上。</li><li><b>庫大小。</b& gt;表1 中列出了需要加載到瀏覽器中的庫文件的大小。ConvNetJS 是最小的，只有33KB。TensorFlow.js 和brain.js 的文件大小非常大，分別為732KB 和819KB。小型庫更適合在瀏覽器中加載應用，因為所有的文件都要按需下載。</li></ul><p>表1. 基於JavaScript 的框架在瀏覽器中支持深度學習的情況</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-7fab69580d2d91f9cb3b01e208e5d82f_r.jpg" data-caption="" data-size="normal" width="1080" referrerpolicy="no-referrer"></figure> ;<p><b>2、瀏覽器中深度學習框架的性能</b></p><p>本節重點討論<b>現有框架的複雜度和後端處理器（CPU 或GPU）對瀏覽器運行訓練和推理任務時性能的影響。</b></p><p><b>2.1 實驗準備</b></p><p> 如前所述，不同框架支持的網絡類型是不一樣的。在實驗中採用最基本的全連接神經網絡作為模型。對於運行DL 任務的數據集，使用經典的MNIST 手寫數字識別數據庫。待訓練的模型有784 個輸入節點和10 個輸出節點。為了研究模型複雜度對性能的影響，對模型進行了不同的配置設置。參數包括：1）神經網絡的隱藏層數（深度），範圍在[1，2，4，8]；2）每個隱藏層的神經元數（寬度），範圍在[64，128，256] 。深度和寬度的範圍是基於“客戶端DL 模型應該是小尺寸，以便能夠在客戶端上運行” 的假設而設定的。在訓練過程中，批處理大小始終設置為64。</p><p>為了研究CPU 和GPU 的性能差異，使用一台Hasee T97E 筆記本電腦，它的獨立顯卡是Nvidia 1070 Max-Q（配備8GB GPU 內存）。CPU 為Intel i7-8750H，其中包含Intel HD Graphics 630，使得實驗中也能夠使用集成顯卡來驗證性能。在下文中，用nGPU 和iGPU 分別表示獨立Nvidia 顯卡和集成Intel 顯卡的GPU。所有的實驗都在Ubuntu 18.04.01 LTS（64 位）上的Chrome 瀏覽器（版本：71.0.3578.10 dev 64 位）上運行，且使用各個框架最新發布的版本。</p><p>對於每個DL 任務，實驗中構建了一個網頁，可以通過URL 中的參數來改變DL 模型的配置。作者在Chrome 瀏覽器上運行每個DL 任務，並記錄完成任務的時間。由於每個實驗通常需要運行不同配置下的幾十個任務，作者開發了一個Chrome 瀏覽器擴展版本：可以迭代運行所有頁面，並在一個任務完成之後更改配置。該擴展版本的瀏覽器還能夠監控網頁的系統資源使用情況。</p><p& gt;<b>2.2 訓練性能</b></p><p>作者選取了支持在瀏覽器中進行訓練的brain.js、ConvNetJS、synaptic 和TensorFlow.js 四個JavaScript 框架進行實驗，比較它們完成訓練任務的性能。除了Tensor-Flow.js 能通過WebGL 使用GPU 外，其他框架都是基於CPU 訓練模型的。作者使用每個框架對定義的模型進行訓練，獲得訓練一批模型的平均時間。圖1 給出了不同模型複雜度的結果。由於synaptic 的訓練時間大約是其他框架的幾十倍到幾百倍，為了更好的展示，作者在圖中省略了synaptic 的結果，但實際上它的結果與其他框架是相似的。</p><p>一般來說，訓練時間會隨著網絡規模的增大而增加，因為對於較大的網絡，需要更多的計算量來完成訓練過程。比較不同框架在CPU 後台的訓練時間，我們可以看到ConvNetJS 是所有框架中所有網絡配置下速度最快的。作者分析，可能的原因是ConvNetJS 的設計比較簡單，這也可以從它的庫文件大小反映出來。在速度方面Brain.js 緊隨其後，與ConvNetJS 的性能差距約為2 倍，而Tensorflow.js 與ConvNetJS 的性能差距約為兩到三倍。在對比ConvNetJS 與TensorFlow.js 的訓練時間時，作者發現當深度和寬度增加時，性能差距會逐漸縮小，這說明與ConvNetJS 相比Tensor-Flow.js 的計算之外的開銷相對較大。此外，隨著網絡寬度的增加而導致的性能差距要比隨著網絡深度增加而導致的性能差距要大，這意味著TensorFlow.js 比ConvNetJS 更適合於處理大規模矩陣計算。</p>< p>CPU 後端的訓練時間隨著網絡規模的增大而變長，但GPU（iGPU 和nGPU）後端的訓練結果卻不盡相同。對於計算能力較弱的iGPU 和能夠滿足較大規模矩陣計算的nGPU，訓練時間並沒有顯著增加。但在從4 個隱層、每層128 個神經元到8 個隱層、每層256 個神經元的過程中，iGPU 的訓練時間明顯增加。作者分析，其原因可能是在本實驗設定的網絡規模下，訓練過程沒有達到GPU 的能力瓶頸。雖然nGPU 的矩陣計算能力優於iGPU，但nGPU 的訓練時間比iGPU 長。作者分析這種結果可能是由於調用WebGL 訪問GPU 的時間開銷過大造成的，實際上單純GPU 的實時計算時間要短得多。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-4ab2f53e7945c3fae99a8f54d4401d06_r.jpg" data-size="normal" width="1080 " referrerpolicy="no-referrer"><figcaption>圖1. 在不同模型複雜度下，一個批次的平均訓練時間（ms），Y 軸為對數刻度。</figcaption></figure><p> 表2 中給出了在訓練過程中每個框架的CPU 利用率的統計數據。110% 是CPU 利用率的上限。由於JavaScript 引擎是單線程的，因此不能使用多核處理器的功能，它只能最大限度地提高單核的使用率。之所以CPU 利用率超過100%，是因為其他內核和用戶空間組件偶爾會在其他線程中同時運行。在CPU 後端，TensorFlow.js 有時無法最大化單個核心的利用率，其平均CPU 利用率僅為82.1%。同時，作者發現在GPU（iGPU 和nGPU）後端運行訓練任務時，由於大部分計算都在GPU 上，所以CPU 的利用率並不高。iGPU 上的訓練比nGPU 上訓練的CPU 利用率要高5-7% 左右。</p><p>表2. 訓練過程的CPU 利用率（%）</p><figure data-size="normal"><img src="https://pic3. zhimg.com/v2-f81a0170f5fa25f4c17916bca7f3b922_b.jpg" data-caption="" data-size="normal" referrerpolicy="no-referrer"></figure><p><b>2.3 推理性能< /b></p><p>作者選擇了6 個JavaScript 框架來比較它們運行推理任務的性能。TensorFlow.js、Keras.js 和WebDNN 支持使用GPU 進行加速，但brain. js、ConvNetJS 和synaptic 只支持使用CPU 進行推理。在模型使用方面，brain.js、ConvNetJS、synaptic 和TensorFlow.js 支持保存自己訓練的模型，而Keras.js 和WebDNN 只支持從其他深度學習框架導入預訓練的模型。因此，對於brain.js、ConvNetJS、synaptic 和TensorFlow.js 直接使用框架本身保存的模型。對於Keras.js 和WebDNN，則需要使用Keras 訓練的模型，然後將模型轉換為相應的格式。理論上，訓練得到的DL 模型的參數值應該是不同的，但絕對值不會影響推理時間。所以只需要給不同框架的所有模型分配相同的參數值即可。推理任務包括加載一個預先訓練好的模型，然後給定一個樣本輸入，模型輸出結果。此外，在GPU 後端，有一個預熱過程，推理的第一個樣本通常用於激活GPU 處理器。因此，作者將推理過程分解為模型加載、預熱和推理三個階段，並研究性能細節。</p><p>由於篇幅所限，作者在下面的分析中省略了模型深度為8 的結果，因為隨著深度的增加，趨勢是相似的。另外，由於synaptic 的模型加載時間和推理時間仍然比其他框架長很多，為了更好的展示，作者沒有在圖中給出synaptic 的結果。</p><p>首先研究不同框架使用的模型文件的大小。由於通常需要從遠程服務器下載用於推理的模型，模型文件的大小越小意味著下載時間越短。表3 給出了所有推理實驗中使用的模型文件的大小。ConvNetJS 和brain.js 使用類似的JSON 編碼，所以它們的模型文件大小幾乎相同。synaptic 的模型文件也使用JSON 編碼，但其大小是所有框架中最大的。由於TensorFlow.js、Keras. js 和WebDNN 使用的模型文件都是由Keras 模型轉換而來，所以它們的模型文件大小是一樣的，作者只在表3 中顯示TensorFlow.js。由於從Keras 轉換而來的模型被壓縮後保存為二進製文件，所以大小可以大大縮小，只有JSON 中模型文件的1/7 左右。</p><p>然後，我們比較加載不同框架的模型所花費的時間，如圖2 所示。對於CPU 後端，同一框架的不同模型的加載時間與表3 中描述的模型文件的大小成正比。但是，不同框架的模型加載時間有顯著差異。ConvNetJS 是最快的。Brain.js、TensorFlow.js 和Keras.js 的模型加載時間在幅度上是一致的。有趣的是，當寬度增加時，ConvNetJS、brain.js 和synaptic 的加載時間增加特別明顯。這個結果是由它們選擇使用JSON 來編碼模型所造成的。在所有框架中，synaptic 的模型加載時間最慢，比ConvNetJS 長100 多倍到1000 多倍。無論模型大小，TensorFlow.js 的模型加載時間幾乎沒有變化。</p><p>在不同的模型複雜度下，GPU（iGPU 和nGPU）後端的加載時間變化不大。但是，不同框架之間的差異還是很大的。TensorFlow.js 是最快的。與在CPU 後端加載模型相比，Keras.js 加載大型模型的速度較快，但WebDNN 的加載時間較長。此外，可以看到iGPU 和nGPU 的模型加載時間沒有區別。</p><p>表3. 模型文件的大小（MB）</p><figure data-size="normal"><img src="https: 68ms）。在作者設定的模型大小範圍內，GPU 強大的計算能力並沒有造成任何影響。在所有的模型大小中，ConvNetJS 佔據了所有的第一位，其次是CPU 後台的WebDNN。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-0cb4efcfb21d217bbbe8be2ac59809e9_r.jpg" data-size="normal" width="1080 " referrerpolicy="no-referrer"><figcaption>圖4. 在不同的模型複雜度下，一個樣本的平均推理時間(ms)。</figcaption></figure><p>WebDNN 在GPU（nGPU 和iGPU）後端的推理時間比CPU 後端的推理時間長。至於TensorFlow.js，在CPU 後端運行對於較小模型的推理速度更快，而GPU 後端對於較大模型的推理速度更快。Keras.js 在CPU 和GPU 後端的推理時間基本一致。我們可以觀察到，對於CPU 後端的所有框架，當模型變得複雜時，推理時間會增加。特別是當寬度增加時，時間會急劇增加（隨著模型寬度增加一倍，時間增加約2 倍）。訓練任務情況類似。這樣的結果也反映出這些框架在CPU 後端的前向傳播過程中沒有對大規模矩陣運算進行優化。GPU 後端的TensorFlow.js 和WebDNN 並沒有出現這個問題，但後端為GPU 的Keras.js 仍然存在這個問題。</p><p> <b>2.4 其它收穫</b></p><p>根據以上結果我們可以看到，在瀏覽器能夠勝任的小規模全連接神經網絡中，ConvNetJS 在訓練和推理方面的表現都是最好的。不過，由於ConvNetJS 已經不再維護，功能較少，開發者可能還是需要尋找替代品。Tensorflow.js 是唯一可以利用GPU（nGPU 和iGPU）加速訓練過程的框架。它功能豐富，性能與ConvNetJS 相當。所以TensorFlow.js 對於訓練和推理都是一個不錯的選擇。作者不建議在小規模的模型上使用GPU 作為後端，因為GPU 計算能力的優勢並沒有得到充分利用。</p><p>最後，為什麼ConvNetJS 在這些框架中的所有任務都有最好的性能。對於流程邏輯相同的同一模型，性能差異很可能由不同的實現細節來解釋。為此，作者在完成相同訓練任務時比較了ConvNetJS 和TensorFlow.js 的函數調用堆棧（Function Call Stack）。令人驚訝的是，ConvNetJS 的調用堆棧深度只有3，而TensorFlow.js 是48。這樣的結果表明，不同框架之間性能差異的一個可能原因是深度調用堆棧，這會消耗大量的計算資源。</p><p><b>3、瀏覽器DL 框架與原生DL 框架的比較</b></p><p>我們在上文已經分析了目前主要的瀏覽器DL 框架結構、特點，也通過實驗證明了不同框架的效果。那麼，在瀏覽器中運行DL 和原生平台上運行DL 的性能差距有多大？作者比較了TensorFlow. js 和Python 中的原生TensorFlow 的性能，兩者都是由Google 發布和維護的，並且有類似的API，因此所比較的結果是足夠公平的。</p><p><b>3.1 基於預訓練模型的推理</b></p><p>作者使用Keras 官方提供的預訓練模型來衡量TensorFlow.js 和原生TensorFlow 在這些經典模型上完成推理任務時的性能。</p><p>3.1.1 TensorFlow.js 的局限性和瀏覽器約束</p><p>Keras 官方提供了11 個預訓練模型。雖然在原生TensorFlow 上運行這些模型時是一切正常的，但當作者在瀏覽器中使用TensorFlow.js 運行它們時卻遇到了一系列錯誤。作者認為，這些錯誤是由於TensorFlow.js 本身的限制以及瀏覽器施加的限制所造成的。例如，對於NasNet Large 模型，瀏覽器會拋出以下錯誤信息“truncatedNormal is not a valid Distribution "。對於ResNet V3 模型，瀏覽器會拋出錯誤信息"Unknown layer: Lambda"。出現這兩個錯誤的原因是TensorFlow.js 仍在開發中，到目前為止只提供了對有限的轉換模型的支持。許多用戶定義的操作算子TensorFlow.js 是不支持的，例如，TensorFlow.js 不支持使用RNN 中控制流操作算子的模型。當嘗試使用VGG16 或VGG19 時，瀏覽器會拋出"GL OUT OF MEMORY " 的錯誤信息，這意味著GPU 內存溢出。VGG16 模式適用於超過1GB 的GPU 內存，不過實驗中使用筆記本的GPU 內存是8GB（Nvidia 1070 Max-Q），所以應該不是由內存所導致的問題。作者分析，這樣的錯誤是由於瀏覽器的限製造成的。</p><p>在嘗試了所有的模型後，作者得到了5 個可以正確轉換並在瀏覽器上運行的模型。這些模型的信息列在表4 中。可訓練參數的數量由tensorflow.keras 的build-in summary()方法獲得，計算複雜度(Floating Operations) 由tensorflow.profiler.profile()方法獲得。</p><p>表4. 部分Keras 預訓練模型</p><figure data-size="normal"><img src="https://pic3.zhimg.com/ v2-6a6ce0f908b44df9e29f460a808fd9be_r.jpg" data-caption="" data-size="normal" width="930" referrerpolicy="no-referrer"></figure><p>3.1.2 結論</p> ;<p>圖5 給出了每個模型的推理時間。可以看出，TensorFlow.js 在nGPU 上的推理時間與原生TensorFlow 的相當（慢1 倍- 2 倍）。iGPU 後端的TensorFlow. js 的性能比CPU 後端的原生TensorFlow 還要好。作者分析，考慮到集成顯卡和CPU 的計算能力，這一結果並不奇怪。然而，由於原生DL 框架不支持集成顯卡加速，通過瀏覽器執行的DL 框架可以在集成顯卡的應用中獲益良多，畢竟，現在的設備中集成顯卡已經是非常常見的了。</p><p>在客戶端DL 的實時性要求下，如果用戶想要達到10FPS（每秒幀數）的體驗，就需要考慮使用更強大的獨立顯卡。而通過iGPU 加速的移動網絡模型也能滿足要求。如果要求是達到1FPS，iGPU 也完全可以滿足。但如果只能使用CPU，那麼在瀏覽器中運行這些很常見的模型看起來就太過於沉重了。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-977586d8205d64d6888c261567cd56a0_r.jpg" data-size="normal" width="1080 " referrerpolicy="no-referrer"><figcaption>圖5. 預訓練的Keras 模型的推理時間，y 軸為對數刻度。</figcaption></figure><p><b>3.2 決策樹分析</b></p><p> 最後，為了深入探索不同因素是如何影響DL 在瀏覽器和原生框架上的性能差距的，作者建立了一個基於決策樹的預測模型，具體研究各種因素的重要性。</p><p>3.2.1 實驗設置</p><p>作者考慮4 個影響DL 在瀏覽器和原生平台上性能差距的因素，如表5 所示，包括後端（CPU 或GPU）、任務類型（訓練或推理）以及模型的深度和寬度。在DNN 和RNN 模型中，寬度是指每層神經元的數目。在CNN 模型中，寬度是指卷積層中使用的核數。對於DNN、CNN 和RNN，作者從Tensorflow.js 官方樣本示例中選取模型。DNN 和CNN 模型用於識別MNIST 數據集上的手寫數字，RNN 模型用於從尼采的著作中生成文本。根據Tensorflow.js 官方樣本的數值集合確定深度和寬度的範圍。</p><p>表5. 造成性能差距的主要因素</p><figure data-size="normal"><img src="https://pic2.zhimg.com /v2-80c6e77d21eed7bf72d873dcd90578a1_r.jpg" data-caption="" data-size="normal" width="800" referrerpolicy="no-referrer"></figure><p>在本文實驗中，作者使用不同配置的TensorFlow. js 和原生TensorFlow 運行DNN、CNN 和RNN 模型。作者將每個配置的執行時間衡量為兩個平台上訓練任務的每批平均時間和推理任務的每個樣本平均時間。作者使用TensorFlow.js 上的執行時間與原生TensorFlow 上的執行時間之比來量化性能差距。</p><p>3.2.2 方法</p><p>使用sklearn 運行決策樹算法來預測TensorFlow.js 和原生TensorFlow 之間的執行時間比。使用決策樹描述貢獻因素的相對重要性。直觀地講，靠近決策樹根部的因素比靠近葉子的因素對時間比的影響更大，這是因為決策樹是根據熵- 信息增益標準（the Entropy-Information Gain criterion）選擇對節點進行分割的。</p><p>作者首先為所有因素生成一棵完全生長的、未經修剪的決策樹。這樣一來，每個葉子只包含一個配置。然後，將樹的深度設置為因子的數量，以防止在一條路徑上多次使用一個因子。圖6 顯示了DNN、CNN 和RNN 的決策樹。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-31b1e033f95a52cdcf1e724e6f336a32_r.jpg" data-caption="" data-size=" normal" width="1080" referrerpolicy="no-referrer"></figure>< figure data-size="normal"><img src="https://pic4.zhimg.com/v2-3c01f040b0fda23a022da9eecf526973_r.jpg" data-caption="" data-size="normal" width="1080" referrerpolicy="no-referrer"></figure><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-cee32904940c480e8fe67620fecfbb61_r.jpg" data- size="normal" width="1080" referrerpolicy="no-referrer"><figcaption>圖6. 使用決策樹來分析TensorFlow.js 與原生TensorFlow 在DNN、CNN 和RNN 模型上的時間比。</figcaption></figure><p>3.2.3 結果</p><p>作者使用決策樹算法來預測的總的結論是：<b>在幾乎所有的配置中，TensorFlow.js 的執行時間都比原生TensorFlow 長</b>。</p>< p>首先，後端是造成不同的模型、平台性能差距的最重要因素。由圖6 中的實驗結果可以看出，總的來說CPU 後端的執行時間比GPU 後端要長得多。例如，當深度超過3、寬度超過192 的DNN 模型運行在GPU 後端而不是CPU 後端時，訓練任務的比率從44.7 下降到4.4。最極端的例子發生在CNN 上，在CPU 後端，比率範圍從低於5 到超過2200（當深度小於7.5，寬度超過600 時）。但是，當在GPU 後端執行深度超過12、寬度超過600 的推理任務時，TensorFlow.js 執行速度與原生TensorFlow 一樣快。這是因為當模型足夠大時，CNN 能夠有效利用GPU 強大的計算能力。</p><p>第二個重要因素是三個模型的任務類型。執行訓練任務的比率較高，而推理任務的表現差距較小。例如，對於CPU 後端的DNN 模型，訓練任務TensorFlow.js 平均比原生TensorFlow 慢33.9 倍，推理任務TensorFlow.js 執行速度比原生TensorFlow 平均慢5.8 倍。</p><p>最後，DNN 和RNN 的決策樹都表明，深度和寬度的重要性取決於任務在哪個後端執行。在CPU 後端，寬度的重要性大於深度的重要性，而深度在GPU 後端扮演更重要的角色。然而，在CNN 的情況下，對於訓練任務來說，寬度比深度對性能差距的影響更大。表6 總結了上述實驗的研究結果。</p><p>表6. 瀏覽器中DL 的主要影響因素</p><figure data-size="normal"><img src="https://pic3. js 提供了圖像分類、對象檢測、姿態估計、文本惡意檢測、語音指令識別、人臉識別、語義分割等豐富的開箱即用的模型。</p><p>TensorFlow.js 的API 主要是以TensorFlow 為藍本，並添加一些針對JS 環境的例外。和TensorFlow 一樣，其核心數據結構是Tensor。TensorFlow.js API 提供了從JS 數組創建張量的方法，以及對張量進行操作的數學函數。圖7 給出TensorFlow.js 的結構示意圖。TensorFlow.js 由兩組API 組成：提供了低層次的線性代數操作（如矩陣乘法、張量加法等）的Ops API 和被設計為在瀏覽器和服務器端運行的TensorFlow.js。在瀏覽器內部運行時，它通過WebGL 利用設備的GPU 來實現快速並行化浮點計算。在Node.js 中，TensorFlow.js 與TensorFlow C 庫綁定，可以完全訪問TensorFlow。TensorFlow.js 還提供了一個較慢的CPU 實現作為後備（為簡單起見圖7 中省略），用純JS 實現。這個後備設置可以在任何執行環境中運行，當環境無法訪問WebGL 或TensorFlow 二進制時，會自動使用。</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-5d57890729d2893249bdc542102d76f4_r.jpg" data-size="normal" width="1080 " referrerpolicy="no-referrer"><figcaption> 圖7. TensorFlow.js 的結構示意圖。</figcaption></figure><p>TensorFlow.js 提供了Layers API，它盡可能地鏡像了Keras API，包括序列化格式。這實現了Keras 和TensorFlow.js 之間的雙向門：用戶可以在TensorFlow.js 中加載一個預先訓練好的Keras 模型，對其進行修改、序列化，然後在Keras Python 中加載回來。</p><p>和TensorFlow 一樣，TensorFlow.js 的一個操作代表了一個抽象的計算(例如矩陣乘法)，它獨立於它所運行的物理設備。操作調用到內核，內核是數學函數的特定設備實現。為了支持特定設備的內核實現，Tensor-Flow.js 有一個Backend 的概念。Backend 實現了內核以及方法，如read() 和write()，這些方法用於存儲支持張量的TypedArray。張量與支持它們的數據是解耦的，因此像reshape 和clone 這樣的操作實際上是自由的。</p><p>此外，TensorFlow.js 還支持自動微分，提供一個API 來訓練模型和計算梯度。在討論TensorFlow. js 中的自動微分問題之前，讓我們先來回顧一下常見的自動微分方法。目前有兩種常見的自動微分方法：Graph-based 和eager。Graph-based 引擎提供了一個API 來構造一個計算圖，並在之後執行。在計算梯度時，引擎會靜態分析Graph 來創建一個額外的梯度計算Graph。這種方式的性能比較好，而且容易實現序列化。而eager 微分引擎則採取了不同的方式。在eager 模式下，當一個操作被調用時，立即進行計算，這可以很容易的通過Print 或使用debugger 來檢查結果。另一個好處是，當模型在執行時，主機語言的所有功能都是可用的。用戶可以使用原生的if 和while 循環，而不需要使用專門的控制流API。TensorFlow.js 的設計目標是優先考慮易用性而不是性能，所以TensorFlow.js 支持eager 自動微分方法。</p><p>TensorFlow.js 實現了平衡同步函數的簡單性和異步函數的優點。例如，像tf.matMul()這樣的操作是同步執行的並返回一個可能未計算的張量。這樣用戶就可以寫出常規的同步代碼，便於調試。當用戶需要檢索支持張量的數據時，TensorFlow.js 提供了一個異步的tensor.data()函數，該函數返回一個承諾（promise），當操作完成後，該承諾會被解析。因此，異步代碼的使用可以本地化到一個data()的調用。用戶還可以選擇調用tensor.dataSync()，這是一個阻塞調用（Blocking Call）。圖8 和圖9 分別說明了在瀏覽器中調用tensor.dataSync()和tensor.data()的時間線。</p><figure data-size="normal"><img src="https://pic2. js 的官網上提供了多個在瀏覽器中運行的在線演示和示例。比如，下面是一個通過網絡攝像頭玩Pacman 遊戲，使用一個預先訓練好的MobileNet 模型，並使用內部的MobileNet 激活訓練另一個模型，從用戶定義的網絡攝像頭中預測4 個不同的類。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-8f65e9e498d362c8a518452f3aba14a7_r.jpg" data-caption="" data-size=" normal" width="882" referrerpolicy="no-referrer"></figure><p>下面的示例在TensorFlow 中對鋼琴演奏的MIDI 進行了Performance RNN 訓練。然後，它被移植到瀏覽器中，在TensorFlow.js 環境中僅使用Javascript 運行。鋼琴樣本來自Salamander Grand Piano。TensorFlow.js 的中文網站上還有其它有趣的示例，大家可以直接上去嘗試一下。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-0d6e81bea1e2a70bc01f204b218c7291_r.jpg" data-caption="" data-size=" normal" width=" a href="http://cs.stanford.edu/people/karpathy/convnetjs/build/convnet-min.js" target="_blank"><span class="invisible">http://< ;/span><span class="visible">cs.stanford.edu/people/</span><span class="invisible">karpathy/convnetjs/build/convnet-min.js</ span><span class="ellipsis"></span></a>)，二選一。</p><p>ConvNetJS 的官網上提供了分類(Classifier)、回歸(Regression)、自編碼器(AutoEncoder) 等示例。</p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-8fbd787a3d84e18c9d89a2a0b999f395_r.jpg" data-caption="" data-size=" normal" width="713" referrerpolicy=" 目前，Paddle.js 支持在webGL2.0 和webGL1.0 的瀏覽器上運行。包括PC 端的chrome, firefox, safari 以及移動端的Baidu App, QQ 瀏覽器等。支持NCHW 與NHWC 格式的模型數據計算。Paddle.js 提供了豐富的模型資源庫，同時也提供了模型轉換工具可將Paddle 模型變成Web 可用模型（<a href="https://github.com/PaddlePaddle/Paddle.js/blob/ master/tools/ModelConverter/README_cn.md" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/ PaddlePaddle</span><span class="invisible">/Paddle.js/blob/master/tools/ModelConverter/README_cn.md</span><span class="ellipsis"></span> </a>）。Paddle.js 支持有限的算子，如果模型中使用了不支持的操作，那麼Paddle.js 將運行失敗並提示模型中有哪些算子目前還不支持。</p><figure data-size="normal">& lt;img src="https://pic3.zhimg.com/v2-32895fa369935daa13f44741a6cf1d22_r.jpg" data-caption="" data-size="normal" width="504" referrerpolicy="no-referrer">< ;/figure><p>Paddle.js 使用現成的JavaScript 模型或將其轉換為可在瀏覽器中運行的paddel 模型。目前，小型Yolo 模型可在30ms 內運行完畢，可以滿足一般的實時場景需求。Paddel.js 的部署示例如下，Github 上有完整的部署過程說明，大家也可以去試用一下。</p><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-2e941bd4b01cf1fc1c28e86ba7999ebf_r.jpg" data-caption="" data-size=" normal" width="480" referrerpolicy="no-referrer"></figure><p><b>5、文章小結</b></p><p> 本文我們基於WWW'19 論文提供的線索，詳細地了解了在瀏覽器中實現深度學習的可能性、可行性和性能現狀。具體地，重點分析了7 個最近出現的基於JavaScript 的DL 框架，並對比了具體框架支持哪些DL 任務。文中的實驗將不同任務中瀏覽器DL 和原生DL 的性能進行比較。雖然，目前關於瀏覽器中實現DL 的研究仍然處於早期階段，但是，從本文的研究中仍然可以看出瀏覽器DL 框架有很多很好的表現。例如，在完成某些任務時JavaScript 的DL 性能要優於原生DL，瀏覽器DL 框架能夠有效利用集成顯卡等等。</p><p>當然，目前瀏覽器中的DL 框架還有著很大的改進空間，距離大規模的推廣和應用還有很大差距。但是，隨著硬件設備的不斷改進、性能的不斷提升，瀏覽器本身的性能和智能化水平都在不斷的提高。相應的，瀏覽器中的DL 框架的性能也會越來越好。由上文的分析，我們認為，未來瀏覽器中DL 框架的發展可以沿著多個方向：一是，可以提升瀏覽器中DL 框架的執行性能，使其能夠滿足實時性的應用需求；二是，可以不斷完善和增加瀏覽器中DL 框架所適用的任務場景；三是，可以繼續完善上下游的工具鏈，提升瀏覽器中DL 框架的易用性，降低使用成本。</p><p>最後，無論對於AI 還是對於web 瀏覽器，瀏覽器中的DL 框架的發展都會對其產生深遠的影響。我們希望，關於瀏覽器中實現深度學習的探索工作可以為人工智能時代的Web 應用的未來提供一點啟示。</p><p><b>本文參考引用的文獻：</b></p><p>[1] Yun Ma, Dongwei Xiang, Shuyu Zheng, lt;/span><span class="ellipsis"></span></a>.</p><p>[9] 2020. Paddle.js. <a href=" https://github.com/paddlepaddle/paddle.js/releases" target="_blank"><span class="invisible">https://</span><span class="visible" >github.com/paddlepaddle</span><span class="invisible">/paddle.js/releases</span><span class="ellipsis"></span></a> ;.</p><p><b>分析師介紹：</b></p><p>仵冀穎，工學博士，畢業於北京交通大學，曾分別於香港中文大學和香港科技大學擔任助理研究員和研究助理，現從事電子政務領域信息化新技術研究工作。主要研究方向為模式識別、計算機視覺，愛好科研，希望能保持學習、不斷進步。</p>< /div></description><author>機器之心</author><guid>https://zhuanlan.zhihu.com/p/310044954</guid><pubDate>Tue, 24 Nov 2020 13:46:01 GMT</pubDate></item><item><title>谷歌搜索：幾乎所有的英文搜索都用上BERT了</title><link>https://zhuanlan.zhihu.com/p/310040995</ link><description><p><img src="https://pic2.zhimg.com/v2-3cf8dac1452b4f44034b4842e0425fb9_b.jpg"></p><div><blockquote>在前段時間舉辦的「Search On」活動中，谷歌宣布，BERT 現在幾乎為谷歌搜索引擎上的每一個基於英文的查詢提供支持。而在去年，這一比例僅為10%。</blockquote><p><b>機器之心報導，機器之心編輯部。</b></p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-f68b5bb9c03ff0edf2a8e72a3140dc92_r.jpg" data-caption="" data-size="normal" width="1080" referrerpolicy="no-referrer"></figure><p>BERT 是谷歌開源的一款自然語言處理預訓練模型，一經推出就刷新了11 項NLP 任務的SOTA 記錄，登頂GLUE 基準排行榜。</p><p>具體到搜索引擎來說，BERT 可以幫助搜索引擎更好地理解web 頁面上的內容，從而提高搜索結果的相關性。BERT 模型中創新性的<a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650727887&amp;idx=5&amp;sn=a5991e49a4b1df2ea70ddbb84631ff9c& ;amp;chksm=871b21b1b06ca8a71be9ec7ee1d7d5ef46eef1116c3d396f3cc467b95186d278bcdd8e6ac07c&amp;scene=21#wechat_redirect" target="_blank">Transformer</a> 架構是一大亮點。Transformer 處理一個句子中與所有其他單詞相關的單詞，而不是按順序逐個處理。基於此，BERT 模型就可以藉助某個單詞前後的詞來考慮其所處的完整語境，這對於理解查詢語句背後的意圖非常有用。</p><p> 2019 年9 月，谷歌宣布將BERT 用到搜索引擎中，但僅有10% 的英文搜索結果得到改善；2019 年12 月，谷歌將BERT 在搜索引擎中的使用擴展到70 多種語言。如今，這家搜索巨頭終於宣布：幾乎所有英文搜索都能用上BERT 了。</p><p><b>BERT 對於搜索引擎意味著什麼？</b></p><p>作為自然語言處理領域里程碑式的進展，BERT 為該領域帶來了以下創新：</p><ul><li>利用無標籤文本進行預訓練；</li><li>雙向上下文模型；</li><li>transformer 架構的應用；</li><li>masked 語言建模；</li> ;<li>注意力機制；</li><li>文本蘊涵（下一句預測）；</li><li>……</li></ul><p> ;這些特性使得BERT 對於搜索引擎的優化非常有幫助，尤其是在消除歧義方面。用上BERT 之後，對於比較長、會話性比較強的查詢，或者在「for」、「to」等介詞比較重要的語句中，谷歌搜索引擎將能夠理解查詢語句中詞的上下文。用戶可以用更加自然的方式進行搜索。</p><p> 此外，BERT 對於搜索中的指代消解、一詞多義、同形異義、命名實體確定、本文蘊涵等任務也有很大的幫助。其中，指代消解指的是追踪一個句子或短語在某個語境或廣泛的會話查詢中指代的是誰或什麼東西；一詞多義指同一個詞有多個義項，幾個義項之間有聯繫，搜索引擎需要處理模棱兩可的細微差別；同形異義是指形式相同但意義毫不相同的詞；命名實體確定是指從許多命名實體中了解文本與哪些相關；文本蘊含是指下一句預測。這些問題構成了搜索引擎面臨的常見挑戰。</p><p>在過去的一年，谷歌擴展了BERT 在搜索引擎中的應用範圍，「搜索引擎營銷之父」Danny Sullivan 和G-Squared Interactive 的SEO 顧問Glenn Gabe 等人在推特中介紹了谷歌搜索的最近亮點。</p><blockquote>在谷歌搜索中，有十分之一的搜索查詢拼寫錯誤。很快，一項新的變革將幫助我們在檢測和處理拼寫錯誤方面取得比過去五年更大的進步。</blockquote><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-9548199c74c1e793baec47bd068fe9a1_r.jpg" data-caption="" data-size=" normal" width="803" referrerpolicy="no-referrer"></figure><blockquote> 另一個即將到來的變化是，谷歌搜索將能夠識別網頁中的單個段落，並將它們處理為與搜索最相關的段落。我們預計這會改善7％的Google 搜索查詢。</blockquote><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-83a1c5a7367de6597312fcc0765e7c19_r.jpg" data-caption="" data-size=" normal" width="811" referrerpolicy="no-referrer"></figure><blockquote>Search On 2020：谷歌可以索引一個網頁的段落，而不僅僅是整個網頁。新算法可以放大一段回答問題的段落，而忽略頁面的其餘部分。從下個月開始。</blockquote><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-526c7bc0d3a7eae33818e4b74e2856b3_r.jpg" data-caption="" data-size=" normal" width="812" referrerpolicy="no-referrer"></figure><figure data-size="normal"> <img src="https://pic3.zhimg.com/v2-2aec6a6fe0b8a423a52b3a3e9a86deaa_r.jpg" data-caption="" data-size="normal" width="1000" referrerpolicy="no-referrer">< ;/figure><blockquote>使用人工智能，我們可以更好地檢測視頻的關鍵部分，並幫助人們直接跳到感興趣的內容，而不需要創作者手動標記。到今年年底，10% 的谷歌搜索將使用這項技術。</blockquote><figure data-size="normal"><img src="https://pic4.zhimg.com/v2-9b4c106852810f1ac4d27c4e6294b84f_r.jpg" data-caption="" data-size=" normal" width="810" referrerpolicy="no-referrer"></figure><p>此外，谷歌還表示，他們還應用神經網絡來理解搜索相關的子主題，當你搜索寬泛的內容時，這有助於提供更多樣化的內容。這項服務預計年底推出。</p><figure data-size="normal"><img src="https://pic1.zhimg. span class="visible">blog.google/products/se</span><span class="invisible">arch/search-on/</span><span class="ellipsis"> </span></a></p></div></description><author>機器之心</author><guid>https://zhuanlan.zhihu.com/p/310040995 </guid><pubDate>Tue, 24 Nov 2020 10:13:21 GMT</pubDate></item><item><title>每天調用達80億次的小米MiNLP平台，近期又開源了中文分詞功能</title><link>https://zhuanlan.zhihu.com/p/308191107</link><description><div><blockquote>近日，小米開源了其自然語言處理平台MiNLP 的中文分詞功能，具備分詞效果好、輕量級、詞典可定制、多粒度切分以及調用更便捷等特點。</blockquote><p><b>機器之心報導，作者：陳萍。</b></p>& lt;p>在自然語言處理任務中，除了模型之外，底層的數據處理也是非常重要的。在語料預處理階段，分詞作為其中一個環節，其作用也是非常重要，但也受不同語言的限制，因而採取的方法有所不同，如中文和英文。</p><p>中文分詞是將一個漢字序列進行切分，得到一個個單獨的詞，如「我住在朝陽區」，分詞後的結果為「我/ 住在/ 朝陽區」 。如果分詞效果不好的話，可能會出現其他情況，比較極端的結果是全部分錯「我住/ 在朝/ 陽區」。分詞效果好不好對信息檢索、實驗結果影響很大，同時分詞的背後涉及各種各樣的算法實現。</p><p>就中、英文而言，中文分詞與英文分詞有很大的不同，對英文而言，詞與詞之間都有空格隔開，一個單詞就是一個詞，而漢語是以字為基本的書寫單位，詞語之間沒有明顯的區分標記，需要人為切分。</p><p>現階段也出現了很多分詞算法：如基於規則的、基於統計的等，也有一些比較好的分詞工具如jieba、Hanlp、THULAC 等。此外還存在著一些在線分詞工具，其分詞效果也不錯。</p><p>國內一些科技大廠也早早投入到自然語言處理相關領域，如百度NLP、阿里雲NLP 等。就在近日，小米AI 實驗室NLP 團隊開發的小米自然語言處理平台MiNLP 現已開源了中文分詞功能。</p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-eb10153333b95ff9c2adcb1778c03976_r.jpg" data-caption="" data-size="normal" width="1080" referrerpolicy="no-referrer"></figure><p>項目地址：<a href="https://github.com/XiaoMi/MiNLP /tree/main/minlp-tokenizer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/XiaoMi /MiNLP</span><span class="invisible">/tree/main/minlp-tokenizer</span><span class="ellipsis"></span></a>< /p><p>目前，MiNLP 平台已經具備<b>詞法、句法、語義</b>等數十個功能模塊，後續將陸續開源<b>詞性標註、命名實體識別、句法分析</b>等功能。其致力於打造功能強大、效果領先的NLP 工具集。</p><p>MiNLP 從去年2. 0 版本的兩大功能模塊（基礎算法、語義理解）已經上升到3.0 版本四大功能模塊，在基礎算法、語義理解的基礎上增加了內容理解和輿情分析模塊。新的模塊可以幫助系統更好地結合上下文理解交互內容，提高識別的準確性</p><p>目前，小米自然語言處理平台MiNLP 每天輸出服務80 億次。</p><p><b>MiNLP-Tokenizer 介紹</b></p><p>MiNLP-Tokenizer 是小米AI 實驗室NLP 團隊自研的中文分詞工具，基於深度學習序列標註模型實現，在公開測試集上取得了SOTA 效果。其具備以下特點：</p><ul><li>分詞效果好：基於深度學習模型在大規模語料上進行訓練，粗、細粒度在SIGHAN 2005 PKU 測試集上的F1 分別達到95.7 % 和96.3%；</li><li>輕量級模型：精簡模型參數和結構，模型僅有20MB；</li><li>詞典可定制：靈活、方便的干預機制，根據用戶詞典對模型結果進行干預；</li><li>多粒度切分：提供粗、細粒度兩種分詞規範，滿足各種場景需要；</li><li>調用更便捷：一鍵快速安裝，API 簡單易用。</li></ul><p><b>安裝過程</b& gt;</p><p>安裝過程非常簡單，pip 全自動安裝：</p><div class="highlight"><pre><code class="language-text" >pip install minlp-tokenizer</code></pre></div><p>適用環境：Python 3.5~3.7，TensorFlow&gt;=1.15,&lt;2。</p><p><b>使用API​​</b></p><div class="highlight"><pre><code class="language-text"> from minlptokenizer.tokenizer import MiNLP lt;/pre></div><p>適用環境：Python 3.5~3.7，TensorFlow&gt;=1.15,&lt;2。</p><p><b>使用API​​</b></p><div class="highlight"><pre><code class="language-text"> from minlptokenizer.tokenizer import MiNLP lt;/pre></div><p>適用環境：Python 3.5~3.7，TensorFlow&gt;=1.15,&lt;2。</p><p><b>使用API​​</b></p><div class="highlight"><pre><code class="language-text"> from minlptokenizer.tokenizer import MiNLP
Tokenizertokenizer = MiNLPTokenizer(granularity='fine') # fine：細粒度，coarse：粗粒度，默認為細粒度
print(tokenizer.cut('今天天氣怎麼樣？'))</code></pre></div><p><b>自定義用戶詞典</b></ p><p>通過用戶詞典List 添加：</p><div class="highlight"><pre><code class="language-text">from minlptokenizer.tokenizer import MiNLP
Tokenizertokenizer = MiNLPTokenizer(['word1', 'word2'], granularity='fine') #用戶自定義干預詞典傳入</code></pre></div><p>通過文件路徑方式添加：</p><div class="highlight"><pre><code class="language-text">from minlptokenizer.tokenizer import MiNLP
195" referrerpolicy="no-referrer"></figure><p>在文件「default.txt」中，有150 個詞，基本上為公司名稱及網站名稱，示例如下所示：< /p><figure data-size="normal"><img src="https://pic3.zhimg.com/v2-28b07e5e895ce6692994527efa5ad90e_b.jpg" data-caption="" data-size="normal" width="213" referrerpolicy="no-referrer"></figure><p>其實早在11 月5 日2020 小米開發者大會上，小米集團副總裁、集團技術委員會主席崔寶秋就發布了小米自然語言處理平台MiNLP 平台開源計劃。現在，MiNLP 的分詞功能已經開源，接下來可以期待一下詞性標註、命名實體識別、句法分析等功能的開源。</p><p>據2020 小米開發者大會上的消息，詞性標註功能將於明年一季度開源，命名實體識別功能將於明年二季度開源，語義相關功能將於明年三季度開源。MiNLP 不同功能模塊，將按計劃逐漸開源。</p><p>參考鏈接：</p><p><a href="https://dy.163.